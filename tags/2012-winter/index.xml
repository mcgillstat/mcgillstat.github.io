<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2012 Winter on McGill Statistics Seminars</title>
    <link>/tags/2012-winter/</link>
    <description>Recent content in 2012 Winter on McGill Statistics Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Apr 2012 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/2012-winter/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Li: High-dimensional feature selection using hierarchical Bayesian logistic regression with heavy-tailed priors | Rao: Best predictive estimation for linear mixed models with applications to small area estimation</title>
      <link>/post/2012winter/2012-04-13/</link>
      <pubDate>Fri, 13 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-04-13/</guid>
      <description>Date: 2012-04-13 Time: 14:00-16:30 Location: MAASS 217 Abstract: Li: The problem of selecting the most useful features from a great many (eg, thousands) of candidates arises in many areas of modern sciences. An interesting problem from genomic research is that, from thousands of genes that are active (expressed) in certain tissue cells, we want to ﬁnd the genes that can be used to separate tissues of diﬀerent classes (eg. cancer and normal).</description>
    </item>
    
    <item>
      <title>Hypothesis testing in finite mixture models: from the likelihood ratio test to EM-test</title>
      <link>/post/2012winter/2012-04-05/</link>
      <pubDate>Thu, 05 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-04-05/</guid>
      <description>Date: 2012-04-05 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the presence of heterogeneity, a mixture model is most natural to characterize the random behavior of the samples taken from such populations. Such strategy has been widely employed in applications ranging from genetics, information technology, marketing, to finance. Studying the mixing structure behind a random sample from the population allows us to infer the degree of heterogeneity with important implications in applications such as the presence of disease subgroups in genetics.</description>
    </item>
    
    <item>
      <title>A matching-based approach to assessing the surrogate value of a biomarker</title>
      <link>/post/2012winter/2012-03-30/</link>
      <pubDate>Fri, 30 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-30/</guid>
      <description>Date: 2012-03-30 Time: 15:30-16:30 Location: BURN 1205 Abstract: Statisticians have developed a number of frameworks which can be used to assess the surrogate value of a biomarker, i.e. establish whether treatment effects on a biological quantity measured shortly after administration of treatment predict treatment effects on the clinical endpoint of interest. The most commonly applied of these frameworks is due to Prentice (1989), who proposed a set of criteria which a surrogate marker should satisfy.</description>
    </item>
    
    <item>
      <title>Model selection principles in misspecified models</title>
      <link>/post/2012winter/2012-03-23/</link>
      <pubDate>Fri, 23 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-23/</guid>
      <description>Date: 2012-03-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: Model selection is of fundamental importance to high-dimensional modeling featured in many contemporary applications. Classical principles of model selection include the Bayesian principle and the Kullback-Leibler divergence principle, which lead to the Bayesian information criterion and Akaike information criterion, respectively, when models are correctly specified. Yet model misspecification is unavoidable in practice. We derive novel asymptotic expansions of the two well-known principles in misspecified generalized linear models, which give the generalized BIC (GBIC) and generalized AIC.</description>
    </item>
    
    <item>
      <title>Variable selection in longitudinal data with a change-point</title>
      <link>/post/2012winter/2012-03-16/</link>
      <pubDate>Fri, 16 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-16/</guid>
      <description>Date: 2012-03-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: Follow-up studies are frequently carried out to investigate the evolution of measurements through time, taken on a set of subjects. These measurements (responses) are bound to be influenced by subject specific covariates and if a regression model is used the data analyst is faced with the problem of selecting those covariates that “best explain” the data. For example, in a clinical trial, subjects may be monitored for a response following the administration of a treatment with a view of selecting the covariates that are best predictive of a treatment response.</description>
    </item>
    
    <item>
      <title>Using tests of homoscedasticity to test missing completely at random | Hugh Chipman: Sequential optimization of a computer model and other Active Learning problems</title>
      <link>/post/2012winter/2012-03-09/</link>
      <pubDate>Fri, 09 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-09/</guid>
      <description>Date: 2012-03-09 Time: 14:00-16:30 Location: UQAM, 201 ave. du Président-Kennedy, salle 5115 Abstract: Li: The problem of selecting the most useful features from a great many (eg, thousands) of candidates arises in many areas of modern sciences. An interesting problem from genomic research is that, from thousands of genes that are active (expressed) in certain tissue cells, we want to ﬁnd the genes that can be used to separate tissues of diﬀerent classes (eg.</description>
    </item>
    
    <item>
      <title>Estimating a variance-covariance surface for functional and longitudinal data</title>
      <link>/post/2012winter/2012-03-02/</link>
      <pubDate>Fri, 02 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-02/</guid>
      <description>Date: 2012-03-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: In functional data analysis, as in its multivariate counterpart, estimates of the bivariate covariance kernel σ(s,t ) and its inverse are useful for many things, and we need the inverse of a covariance matrix or kernel especially often. However, the dimensionality of functional observations often exceeds the sample size available to estimate σ(s,t, and then the analogue S of the multivariate sample estimate is singular and non-invertible.</description>
    </item>
    
    <item>
      <title>McGillivray: A penalized quasi-likelihood approach for estimating the number of states in a hidden Markov model | Best: Risk-set sampling and left truncation in survival analysis</title>
      <link>/post/2012winter/2012-02-17/</link>
      <pubDate>Fri, 17 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-02-17/</guid>
      <description>Date: 2012-02-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: McGillivray: In statistical applications of hidden Markov models (HMMs), one may have no knowledge of the number of hidden states (or order) of the model needed to be able to accurately represent the underlying process of the data. The problem of estimating the number of hidden states of the HMM is thus brought to the forefront. In this talk, we present a penalized quasi-likelihood approach for order estimation in HMMs which makes use of the fact that the marginal distribution of the observations from a HMM is a finite mixture model.</description>
    </item>
    
    <item>
      <title>Stute: Principal component analysis of the Poisson Process | Blath: Longterm properties of the symbiotic branching model</title>
      <link>/post/2012winter/2012-02-10/</link>
      <pubDate>Fri, 10 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-02-10/</guid>
      <description>Date: 2012-02-10 Time: 14:00-16:30 Location: Concordia Abstract: Stute: The Poisson Process constitutes a well-known model for describing random events over time. It has many applications in marketing research, insurance mathematics and finance. Though it has been studied for decades not much is known how to check (in a non-asymptotic way) the validity of the Poisson Process. In this talk we present the principal component decomposition of the Poisson Process which enables us to derive finite sample properties of associated goodness-of-fit tests.</description>
    </item>
    
    <item>
      <title>Du: Simultaneous fixed and random effects selection in finite mixtures of linear mixed-effects models | Harel: Measuring fatigue in systemic sclerosis: a comparison of the SF-36 vitality subscale and FACIT fatigue scale using item response theory</title>
      <link>/post/2012winter/2012-02-03/</link>
      <pubDate>Fri, 03 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-02-03/</guid>
      <description>Date: 2012-02-03 Time: 15:30-16:30 Location: BURN 1205 Abstract: Du: Linear mixed-effects (LME) models are frequently used for modeling longitudinal data. One complicating factor in the analysis of such data is that samples are sometimes obtained from a population with significant underlying heterogeneity, which would be hard to capture by a single LME model. Such problems may be addressed by a finite mixture of linear mixed-effects (FMLME) models, which segments the population into subpopulations and models each subpopulation by a distinct LME model.</description>
    </item>
    
    <item>
      <title>Applying Kalman filtering to problems in causal inference</title>
      <link>/post/2012winter/2012-01-27/</link>
      <pubDate>Fri, 27 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-01-27/</guid>
      <description>Date: 2012-01-27 Time: 15:30-16:30 Location: BURN 1205 Abstract: A common problem in observational studies is estimating the causal effect of time-varying treatment in the presence of a time varying confounder. When random assignment of subjects to comparison groups is not possible, time-varying confounders can cause bias in estimating causal effects even after standard regression adjustment if past treatment history is a predictor of future confounders. To eliminate the bias of standard methods for estimating the causal effect of time varying treatment, Robins developed a number of innovative methods for discrete treatment levels, including G-computation, G-estimation, and marginal structural models (MSMs).</description>
    </item>
    
    <item>
      <title>A concave regularization technique for sparse mixture models</title>
      <link>/post/2012winter/2012-01-20/</link>
      <pubDate>Fri, 20 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-01-20/</guid>
      <description>Date: 2012-01-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Latent variable mixture models are a powerful tool for exploring the structure in large datasets. A common challenge for interpreting such models is a desire to impose sparsity, the natural assumption that each data point only contains few latent features. Since mixture distributions are constrained in their L1 norm, typical sparsity techniques based on L1 regularization become toothless, and concave regularization becomes necessary.</description>
    </item>
    
    <item>
      <title>Bayesian approaches to evidence synthesis in clinical practice guideline development</title>
      <link>/post/2012winter/2012-01-13/</link>
      <pubDate>Fri, 13 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-01-13/</guid>
      <description>Date: 2012-01-13 Time: 15:30-16:30 Location: Concordia, Library Building LB-921.04 Abstract: The American College of Cardiology Foundation (ACCF) and the American Heart Association (AHA) have jointly engaged in the production of guideline in the area of cardiovascular disease since 1980. The developed guidelines are intended to assist health care providers in clinical decision making by describing a range of generally acceptable approaches for the diagnosis, management, or prevention of specific diseases or conditions.</description>
    </item>
    
  </channel>
</rss>