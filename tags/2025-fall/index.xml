<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2025 Fall on McGill Statistics Seminars</title>
    <link>https://mcgillstat.github.io/tags/2025-fall/</link>
    <description>Recent content in 2025 Fall on McGill Statistics Seminars</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://mcgillstat.github.io/tags/2025-fall/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Unfolding Generalized Shannon’s Entropy</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-12-05/</link>
      <pubDate>Fri, 05 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-12-05/</guid>
      <description>&lt;h4 id=&#34;date-2025-12-05&#34;&gt;Date: 2025-12-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83026954715httpsmcgillzoomusj83026954715&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83026954715&#34;&gt;https://mcgill.zoom.us/j/83026954715&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-830-2695-4715&#34;&gt;Meeting ID: 830 2695 4715&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Shannon’s entropy is a cornerstone of information theory, quantifying uncertainty within a probability distribution. However, the classical definition may fail for distributions with heavy tails or infinite alphabets, leaving gaps in its theoretical foundation. This talk introduces a framework called Generalized Shannon’s Entropy (GSE), which extends the original concept to ensure well-definedness and robustness under broader conditions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep P-Spline: Theory, Fast Tuning, and Application</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-11-28/</link>
      <pubDate>Fri, 28 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-11-28/</guid>
      <description>&lt;h4 id=&#34;date-2025-11-28&#34;&gt;Date: 2025-11-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj86339405056httpsmcgillzoomusj86339405056&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/86339405056&#34;&gt;https://mcgill.zoom.us/j/86339405056&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-863-3940-5056&#34;&gt;Meeting ID: 863 3940 5056&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Deep neural networks (DNNs) have become a standard tool for tackling complex regression problems, yet identifying an optimal network architecture remains a fundamental challenge. In this work, we connect neuron selection in DNNs with knot placement in basis expansion methods. Building on this connection, we propose a difference-penalty approach that automates knot selection and, in turn, simplifies the process of choosing neurons. We call this method Deep P-Spline (DPS). This approach extends the class of models considered in conventional DNN modeling and forms the basis for a latent-variable modeling framework using the Expectation–Conditional Maximization (ECM) algorithm for efficient network structure tuning with theoretical guarantees. From the perspective of nonparametric regression, DPS alleviates the curse of dimensionality, allowing effective analysis of high-dimensional data where conventional methods often fail. These properties make DPS particularly well suited for applications such as computer experiments and image data analysis, where regression tasks routinely involve a large number of inputs. Numerical studies demonstrate the strong performance of DPS, underscoring its potential as a powerful tool for advanced nonlinear regression problems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Can uncertainty be quantified? On confident hallucinations in deep learning-based methods for inverse problems</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-11-14/</link>
      <pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-11-14/</guid>
      <description>&lt;h4 id=&#34;date-2025-11-14&#34;&gt;Date: 2025-11-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj82687773039httpsmcgillzoomusj82687773039&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/82687773039&#34;&gt;https://mcgill.zoom.us/j/82687773039&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-826-8777-3039&#34;&gt;Meeting ID: 826 8777 3039&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Deep learning is currently transforming how inverse problems arising in imaging reconstruction are solved. However, it is increasingly well-known that such deep learning-based methods are susceptible to hallucinations. In this talk, I will present a series of theoretical explanations for why hallucinations occur, in both deterministic and statistical estimators. I will conclude by observing that hallucinations can only be avoided by careful design of the forwards operator in tandem with the recovery algorithm, and then provide a theoretical framework for how this can be achieved when solving inverse problems using generative models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Towards Efficient and Reliable Generative and Sampling Models</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-11-07/</link>
      <pubDate>Fri, 07 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-11-07/</guid>
      <description>&lt;h4 id=&#34;date-2025-11-07&#34;&gt;Date: 2025-11-07&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj87181846336httpsmcgillzoomusj87181846336&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/87181846336&#34;&gt;https://mcgill.zoom.us/j/87181846336&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-871-8184-6336&#34;&gt;Meeting ID: 871 8184 6336&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This talk presents a unified framework for enhancing the reliability and geometric fidelity of generative models. We first develop a diffusion mechanism defined intrinsically on the SE(3) manifold, enabling the efficient sampling. To address the critical issue of mode collapse in energy-based samplers, we introduce a novel Importance Weighted Score Matching method that provably improves coverage of complex, multi-modal distributions. Finally, we extend these principles to infer underlying dynamical systems directly from incomplete and scattered training data. Collectively, this work bridges geometric consistency, statistical reliability, and learning from partial observations to advance the frontiers of generative and sampling models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Regularized Fine-Tuning for Representation Multi-Task Learning: Adaptivity, Minimaxity, and Robustness</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-10-24/</link>
      <pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-10-24/</guid>
      <description>&lt;h4 id=&#34;date-2025-10-24&#34;&gt;Date: 2025-10-24&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj81872329544httpsmcgillzoomusj81872329544&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81872329544&#34;&gt;https://mcgill.zoom.us/j/81872329544&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-818-7232-9544&#34;&gt;Meeting ID: 818 7232 9544&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We study multi-task linear regression for a collection of tasks that share a latent, low-dimensional structure. Each task’s regression vector belongs to a subspace whose dimension, denoted intrinsic dimension, is much smaller than the ambient dimension. Unlike classical analyses that assume an identical subspace for every task, we allow each task’s subspace to drift from a single reference subspace by a controllable similarity radius, and we permit an unknown fraction of tasks to be outliers that violate the shared-structure assumption altogether. Our contributions are threefold. First, adaptivity: we design a penalized empirical-risk algorithm and a spectral method.  Both algorithms automatically adjust to the unknown similarity radius and to the proportion of outliers. Second, minimaxity: we prove information-theoretic lower bounds on the best achievable prediction risk over this problem class and show that both algorithms attain these bounds up to constant factors; when no outliers are present, the spectral method is exactly minimax-optimal. Third, robustness: for every choice of similarity radius and outlier proportion, the proposed estimators never incur larger expected prediction error than independent single-task regression, while delivering strict improvements whenever tasks are even moderately similar and outliers are sparse. Additionally, we introduce a thresholding algorithm to adapt to an unknown intrinsic dimension. We conduct extensive numerical experiments to validate our theoretical findings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>K-contact Distance for Noisy Nonhomogeneous Spatial Point Data and Application to Repeating Fast Radio Burst Sources</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-10-10/</link>
      <pubDate>Fri, 10 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-10-10/</guid>
      <description>&lt;h4 id=&#34;date-2025-10-10&#34;&gt;Date: 2025-10-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj81986712072httpsmcgillzoomusj81986712072&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81986712072&#34;&gt;https://mcgill.zoom.us/j/81986712072&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-819-8671-2072&#34;&gt;Meeting ID: 819 8671 2072&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, I’ll introduce an approach to analyze nonhomogeneous Poisson processes (NHPP) observed with noise which focuses on previously unstudied second-order characteristics of the noisy process. Utilizing a hierarchical Bayesian model with noisy data, we first estimate hyperparameters governing a physically motivated NHPP intensity. Leveraging the posterior distribution, we then infer the probability of detecting a certain number of events within a given radius, the $k$-contact distance. This methodology is demonstrated by its motivating application: observations of fast radio bursts (FRBs) detected by the Canadian Hydrogen Intensity Mapping Experiment&amp;rsquo;s FRB Project (CHIME/FRB). The approach allows us to identify repeating FRB sources by computing the probability of observing $k$ physically independent sources within some radius in the detection domain, or the probability of coincidence ($P_C$). Applied, the new methodology improves the repeater detection $P_C$, in 86% of cases when applied to the largest sample of previously classified observations, with a median improvement factor (existing metric over $P_C$ from our methodology) of ~ 3000. Throughout the talk, I will provide the necessary astrophysical context to motivate the application and highlight some of the other active statistical problems in FRB science.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convergence Guarantees for Adversarially Robust Classifiers</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-10-03/</link>
      <pubDate>Fri, 03 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-10-03/</guid>
      <description>&lt;h4 id=&#34;date-2025-10-03&#34;&gt;Date: 2025-10-03&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj82469112499httpsmcgillzoomusj82469112499&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/82469112499&#34;&gt;https://mcgill.zoom.us/j/82469112499&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-824-6911-2499&#34;&gt;Meeting ID: 824 6911 2499&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Neural networks can be trained to classify images and achieve high levels of accuracy. However, researchers have discovered that well-targeted perturbations of an image can completely fool a trained classifier, even in cases where the modified image is visually indistinguishable from the original. This has sparked many new approaches to classification which include an adversary in the training process: such an adversary can improve robustness and generalization properties at the cost of decreased accuracy and increased training time. In this presentation, I will explore the connection between a certain class of adversarial training problems and the Bayes classification problem for binary classification. In particular, robustness can be encouraged by adding a regularizing nonlocal perimeter term, providing a strong connection to classical studies of perimeter. Borrowing tools from geometric measure theory, I will show the Hausdorff convergence of adversarially robust classifiers to Bayes classifiers as the strength of adversary decreases to 0. In this way, the theoretical results discussed in the presentation provide a rigorous comparison with the standard Bayes classification problem.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sparse Causal Learning: Challenges and Opportunities</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-09-26/</link>
      <pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-09-26/</guid>
      <description>&lt;h4 id=&#34;date-2025-09-26&#34;&gt;Date: 2025-09-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj81200178578httpsmcgillzoomusj81200178578&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81200178578&#34;&gt;https://mcgill.zoom.us/j/81200178578&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-812-0017-8578&#34;&gt;Meeting ID: 812 0017 8578&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In many observational studies, researchers are often interested in studying the effects of multiple exposures on a single outcome. Standard approaches for high-dimensional data such as the lasso assume the associations between the exposures and the outcome are sparse. These methods, however, do not estimate the causal effects in the presence of unmeasured confounding. In this paper, we consider an alternative approach that assumes the causal effects in view are sparse. We show that with sparse causation, the causal effects are identifiable even with unmeasured confounding. At the core of our proposal is a novel device, called the synthetic instrument, that in contrast to standard instrumental variables, can be constructed using the observed exposures directly. We show that under linear structural equation models, the problem of causal effect estimation can be formulated as an l0-penalization problem and hence can be solved efficiently using off-the-shelf software. Simulations show that our approach outperforms state-of-art methods in both low-dimensional and high-dimensional settings. We further illustrate our method using a mouse obesity dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optimal vintage factor analysis with deflation varimax</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-09-19/</link>
      <pubDate>Fri, 19 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-09-19/</guid>
      <description>&lt;h4 id=&#34;date-2025-09-19&#34;&gt;Date: 2025-09-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83914219181httpsmcgillzoomusj83914219181&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83914219181&#34;&gt;https://mcgill.zoom.us/j/83914219181&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-839-1421-9181&#34;&gt;Meeting ID: 839 1421 9181&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Vintage factor analysis is one important type of factor analysis that aims to first find a low-dimensional representation of the original data, and then to seek a rotation such that the rotated low-dimensional representation is scientifically meaningful. The most widely used vintage factor analysis is the Principal Component Analysis (PCA) followed by the varimax rotation. Despite its popularity, little theoretical guarantee can be provided to date mainly because varimax rotation requires to solve a non-convex optimization over the set of orthogonal matrices.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Proper Correlation Coefficients for Nominal Random Variables</title>
      <link>https://mcgillstat.github.io/post/2025fall/2025-09-12/</link>
      <pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2025fall/2025-09-12/</guid>
      <description>&lt;h4 id=&#34;date-2025-09-12&#34;&gt;Date: 2025-09-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj88021402798httpsmcgillzoomusj88021402798&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/88021402798&#34;&gt;https://mcgill.zoom.us/j/88021402798&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-880-2140-2798&#34;&gt;Meeting ID: 880 2140 2798&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This work develops an intuitive concept of perfect dependence between two variables of which at least one has a nominal scale that is attainable for all marginal distributions and proposes a set of dependence measures that are 1 if and only if this perfect dependence is satisfied. The advantages of these dependence measures relative to classical dependence measures like contingency coefficients, Goodman-Kruskal&amp;rsquo;s lambda and tau and the so-called uncertainty coefficient are twofold. Firstly, they are defined if one of the variables is real-valued and exhibits continuities. Secondly, they satisfy the property of attainability. That is, they can take all values in the interval [0,1] irrespective of the marginals involved. Both properties are not shared by the classical dependence measures which need two discrete marginal distributions and can in some situations yield values close to 0 even though the dependence is strong or even perfect.&#xA;Additionally, this work provide a consistent estimator for one of the new dependence measures together with its asymptotic distribution under independence as well as in the general case. This allows to construct confidence intervals and an independence test, whose finite sample performance is subsequently examine in a simulation study. Finally, we illustrate the use of the new dependence measure in two applications on the dependence between the variables country and income or country and religion, respectively.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
