<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2018 Winter on McGill Statistics Seminars</title>
    <link>http://localhost:4321/tags/2018-winter/</link>
    <description>Recent content in 2018 Winter on McGill Statistics Seminars</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Apr 2018 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:4321/tags/2018-winter/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Methodological challenges in using point-prevalence versus cohort data in risk factor analyses of hospital-acquired infections</title>
      <link>http://localhost:4321/post/2018winter/2018-04-27/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-04-27/</guid>
      <description>&lt;h4 id=&#34;date-2018-04-27&#34;&gt;Date: 2018-04-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;To explore the impact of length-biased sampling on the evaluation of risk&#xA;factors of nosocomial infections in point-prevalence studies.&#xA;We used cohort data with full information including the exact date of the&#xA;nosocomial infection and mimicked an artificial one-day prevalence study by&#xA;picking a sample from this cohort study. Based on the cohort data, we studied&#xA;the underlying multi-state model which accounts for nosocomial infection as an&#xA;intermediate and discharge/death as competing events. Simple formulas are&#xA;derived to display relationships between risk-, hazard- and prevalence odds&#xA;ratios.&#xA;Due to length-biased sampling, long-stay and thus sicker patients are more&#xA;likely to be sampled. In addition, patients with nosocomial infections usually stay longer in hospital. We explored mechanisms which are -due to the design-&#xA;hidden in prevalence data. In our example, we showed that prevalence odds&#xA;ratios were usually less pronounced than risk odds ratios but more pronounced&#xA;than hazard ratios.&#xA;Thus, to avoid misinterpretation, knowledge of the mechanisms from the&#xA;underlying multi-state model are essential for the interpretation of risk factors&#xA;derived from point-prevalence data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernel Nonparametric Overlap-based Syncytial Clustering</title>
      <link>http://localhost:4321/post/2018winter/2018-04-20/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-04-20/</guid>
      <description>&lt;h4 id=&#34;date-2018-04-20&#34;&gt;Date: 2018-04-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Standard clustering algorithms can find regular-structured clusters such as ellipsoidally- or spherically-dispersed groups, but are more challenged  with groups lacking formal structure or definition. Syncytial clustering is the name that we introduce for methods that merge groups obtained from standard clustering algorithms in order to reveal complex group structure in the data. Here, we develop a distribution-free fully-automated syncytial algorithm that can be used with the computationally efficient k-means or other algorithms. Our approach computes the cumulative distribution function of the normed residuals from an appropriately fit k-groups model and calculates  the nonparametric overlap between all pairs of groups. Groups with high pairwise overlap are merged as long as the generalized overlap decreases. Our methodology is always a top performer in identifying groups with regular and irregular structures in many datasets. We use our method to identify the distinct kinds of activation in a functional Magnetic Resonance Imaging study.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Empirical likelihood and robust regression in diffusion tensor imaging data analysis</title>
      <link>http://localhost:4321/post/2018winter/2018-04-06/</link>
      <pubDate>Fri, 06 Apr 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-04-06/</guid>
      <description>&lt;h4 id=&#34;date-2018-04-06&#34;&gt;Date: 2018-04-06&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;With modern technology development, functional responses are  observed frequently in various scientific fields including neuroimaging data analysis. Empirical likelihood as a nonparametric data-driven technique has become an important statistical inference methodology. In this paper, motivated by diffusion tensor imaging (DTI) data we propose three generalized empirical likelihood-based methods that accommodate within-curve dependence on the varying coefficient model with functional responses and embed a robust regression idea. To avoid the loss of efficiency in statistical inference, we take into consideration within-curve variance-covariance matrix in the subjectwise and elementwise empirical likelihood methods. We develop several statistical inference procedures for maximum empirical likelihood estimators (MELEs) and empirical log likelihood (ELL) ratio functions, and systematically study their asymptotic properties. We first establish the weak convergence of the MELEs and the ELL ratio processes, and derived a nonparametric version of the Wilks theorem for the limiting distributions of the ELLs at any designed point. We propose a global test for linear hypotheses of varying coefficient functions and construct simultaneous confidence bands for each individual effect curve based on MELEs, and construct simultaneous confidence regions for varying coefficient functions based on ELL ratios. A Monte Carlo simulation is conducted to examine the finite-sample performance of the proposed procedures. Finally, we illustrate the estimation and inference procedures on MELEs of varying coefficient model to a diffusion tensor imaging data from Alzheimer&amp;rsquo;s Disease Neuroimaging Initiative (ADNI) study. Joint work with Xingcai Zhou (Nanjing Audit University), Rohana Karunamuni and Adam Kashlak (University of Alberta).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some development on dynamic computer experiments</title>
      <link>http://localhost:4321/post/2018winter/2018-03-23/</link>
      <pubDate>Fri, 23 Mar 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-03-23/</guid>
      <description>&lt;h4 id=&#34;date-2018-03-23&#34;&gt;Date: 2018-03-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Computer experiments refer to the study of real systems using complex simulation models.&#xA;They have been widely used as efficient, economical alternatives to physical experiments.&#xA;Computer experiments with time series outputs are called dynamic computer experiments.&#xA;In this talk, we consider two problems of such experiments: emulation of large-scale dynamic&#xA;computer experiments and inverse problem. For the first problem, we proposed a&#xA;computationally efficient modelling approach which sequentially finds a set of local design&#xA;points based on a new criterion specifically designed for emulating dynamic computer&#xA;simulators. Singular value decomposition based Gaussian process models are built with the&#xA;sequentially chosen local data. To update the models efficiently, an empirical Bayesian&#xA;approach is introduced. The second problem aims to extract an optimal input of dynamic&#xA;computer simulator whose response matches a field observation as closely as possible. A&#xA;sequential design approach is employed and a novel expected improvement criterion is&#xA;proposed. A real application is discussed to support the efficiency of the proposed approaches.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical Genomics for Understanding Complex Traits</title>
      <link>http://localhost:4321/post/2018winter/2018-03-16/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-03-16/</guid>
      <description>&lt;h4 id=&#34;date-2018-03-16&#34;&gt;Date: 2018-03-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Over the last decade, advances in measurement technologies has enabled researchers to generate multiple types of high-dimensional &amp;ldquo;omics&amp;rdquo; datasets for large cohorts. These data provide an opportunity to derive a mechanistic understanding of human complex traits. However, inferring meaningful biological relationships from these data is challenging due to high-dimensionality , noise, and abundance of confounding factors. In this talk, I&amp;rsquo;ll describe statistical approaches for robust analysis of genomic data from large population studies, with a focus on 1) understanding the nature of confounding and approaches for addressing them and 2) understanding the genomic correlates of aging and dementia.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sparse Penalized Quantile Regression: Method, Theory, and Algorithm</title>
      <link>http://localhost:4321/post/2018winter/2018-02-23/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-02-23/</guid>
      <description>&lt;h4 id=&#34;date-2018-02-23&#34;&gt;Date: 2018-02-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Sparse penalized quantile regression is a useful tool for variable selection, robust estimation, and heteroscedasticity detection in high-dimensional data analysis. We discuss the variable selection and estimation properties of the lasso and folded concave penalized quantile regression via non-asymptotic arguments. We also consider consistent parameter tuning therein. The computational issue of the sparse penalized quantile regression has not yet been fully resolved in the literature, due to non-smoothness of the quantile regression loss function. We introduce fast alternating direction method of multipliers (ADMM) algorithms for computing the sparse penalized quantile regression. Numerical examples demonstrate the competitive performance of our algorithm: it significantly outperforms several other fast solvers for high-dimensional penalized quantile regression.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Law of Large Populations: The return of the long-ignored N and how it can affect our 2020 vision</title>
      <link>http://localhost:4321/post/2018winter/2018-02-16/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-02-16/</guid>
      <description>&lt;h4 id=&#34;date-2018-02-16&#34;&gt;Date: 2018-02-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-mcgill-university-otto-maass-217&#34;&gt;Location: McGill University, OTTO MAASS 217&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;For over a century now, we statisticians have successfully convinced ourselves and almost everyone else, that in statistical inference the size of the population N can be ignored, especially when it is large.  Instead, we focused on the size of the sample, n, the key driving force for both the Law of Large Numbers and the Central Limit Theorem. We were thus taught that the statistical error (standard error) goes down with n typically at the rate of 1/√n.   However, all these rely on the presumption that our data have perfect quality, in the sense of being equivalent to a probabilistic sample.  A largely overlooked statistical identity, a potential counterpart to the Euler identity in mathematics, reveals a Law of Large Populations (LLP), a law that we should be all afraid of. That is, once we lose control over data quality, the systematic error (bias) in the usual estimators, relative to the benchmarking standard error from simple random sampling, goes up with N at the rate of √N.   The coefficient in front of √N can be viewed as a data defect index, which is the simple Pearson correlation between the reporting/recording indicator and the value reported/recorded.  Because of the multiplier√N, a seemingly tiny correlation, say, 0.005, can have detrimental effect on the quality of inference.  Without understanding of this LLP,  “big data” can do more harm than good because of the drastically inflated precision assessment hence a gross overconfidence, setting us up to be caught by surprise when the reality unfolds, as we all experienced during the 2016 US presidential election. Data from Cooperative Congressional Election Study (CCES, conducted by Stephen Ansolabehere, Douglas River and others, and analyzed by Shiro Kuriwaki),   are used to estimate the data defect index for the 2016 US election, with the aim to gain a clearer vision for the 2020 election and beyond.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Methodological considerations for the analysis of relative treatment effects in multi-drug-resistant tuberculosis from fused observational studies</title>
      <link>http://localhost:4321/post/2018winter/2018-02-09/</link>
      <pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-02-09/</guid>
      <description>&lt;h4 id=&#34;date-2018-02-09&#34;&gt;Date: 2018-02-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Multi-drug-resistant tuberculosis (MDR-TB) is defined as strains of tuberculosis that do not respond to at least the two most used anti-TB drugs. After diagnosis, the intensive treatment phase for MDR-TB involves taking several alternative antibiotics concurrently. The Collaborative Group for Meta-analysis of Individual Patient Data in MDR-TB has assembled a large, fused dataset of over 30 observational studies comparing the effectiveness of 15 antibiotics. The particular challenges that we have considered in the analysis of this dataset are the large number of potential drug regimens, the resistance of MDR-TB strains to specific antibiotics, and the identifiability of a generalized parameter of interest though most drugs were not observed in all studies. In this talk, I describe causal inference theory and methodology that we have appropriated or developed for the estimation of treatment importance and relative effectiveness of different antibiotic regimens with a particular emphasis on targeted learning approaches&lt;/p&gt;</description>
    </item>
    <item>
      <title>A new approach to model financial data: The Factorial Hidden Markov Volatility Model</title>
      <link>http://localhost:4321/post/2018winter/2018-02-02/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-02-02/</guid>
      <description>&lt;h4 id=&#34;date-2018-02-02&#34;&gt;Date: 2018-02-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A new process, the factorial hidden Markov volatility (FHMV) model, is proposed to model financial returns or realized variances. This process is constructed based on a factorial hidden Markov model structure and corresponds to a parsimoniously parametrized hidden Markov model that includes thousands of volatility states. The transition probability matrix of the underlying Markov chain is structured so that the multiplicity of its second largest eigenvalue can be greater than one. This distinctive feature allows for a better representation of volatility persistence in financial data. Jumps and a leverage effect are also incorporated into the model and statistical properties are discussed. An empirical study on six financial time series shows that the FHMV process compares favorably to state-of-the-art volatility models in terms of in-sample fit and out-of-sample forecasting performance over time horizons ranging from one to one hundred days.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Back to the future: why I think REGRESSION is the new black in genetic association studies</title>
      <link>http://localhost:4321/post/2018winter/2018-01-26/</link>
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-01-26/</guid>
      <description>&lt;h4 id=&#34;date-2018-01-26&#34;&gt;Date: 2018-01-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-room-6254-pavillon-andre-aisenstadt-2920-udem&#34;&gt;Location: ROOM 6254 Pavillon Andre-Aisenstadt 2920, UdeM&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Linear regression remains an important framework in the era of big and complex data. In this talk I present some recent examples where we resort to the classical simple linear regression model and its celebrated extensions in novel settings. The Eureka moment came while reading Wu and Guan&amp;rsquo;s (2015) comments on our generalized Kruskal-Wallis (GKW) test (Elif Acar and Sun 2013, Biometrics). Wu and Guan presented an alternative “rank linear regression model and derived the proposed GKW statistic as a score test statistic&amp;quot;, and astutely pointed out that “the linear model approach makes the derivation more straightforward and transparent, and leads to a simplified and unified approach to the general rank based multi-group comparison problem.&amp;quot; More recently, we turned our attention to extending Levene&amp;rsquo;s variance test for data with group uncertainty and sample correlation. While a direct modification of the original statistic is indeed challenging, I will demonstrate that a two-stage regression framework makes the ensuing development quite straightforward, eventually leading to a generalized joint location-scale test (David Soave and Sun 2017, Biometrics). Finally, I will discuss on-going work, with graduate student Lin Zhang, on developing an allele-based association test that is robust to the assumption of Hardy-Weinberg equilibrium and is generalizable to complex data structure. The crux of this work is, again, reformulating the problem as a regression!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generalized Sparse Additive Models</title>
      <link>http://localhost:4321/post/2018winter/2018-01-19/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-01-19/</guid>
      <description>&lt;h4 id=&#34;date-2018-01-19&#34;&gt;Date: 2018-01-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;I will present a unified approach to the estimation of generalized sparse additive models in high dimensional regression problems. Our approach is based on combining structure-inducing and sparsity penalties in a single regression problem. It allows for the use of a large family of structure-inducing penalties: Those characterized by semi-norm constraints. This includes finite dimensional linear subspaces, sobolev and holder classes, classes with bounded total variation, among others. We give an efficient computational algorithm to fit this family of models that easily scales to thousands of observations and features. In addition we develop a framework for proving convergence bounds on these estimators; and show that our estimators converge at the minimax optimal rate under suitable conditions. We also compare the performance of existing methods in an empirical study and discuss directions for future work.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modelling RNA stability for decoding the regulatory programs that drive human diseases</title>
      <link>http://localhost:4321/post/2018winter/2018-01-12/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/post/2018winter/2018-01-12/</guid>
      <description>&lt;h4 id=&#34;date-2018-01-12&#34;&gt;Date: 2018-01-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The key determinant of the identity and behaviour of the cell is gene regulation, i.e. which genes are active and which genes are inactive in a particular cell. One of the least understood aspects of gene regulation is RNA stability: genes produce RNA molecules to carry their genetic information – the more stable these RNA molecules are, the longer they can function within the cell, and the less stable they are, the more rapidly they are removed from the pool of active molecules. The cell can effectively switch the genes on and off by regulating RNA stability. However, we do not know which genes are regulated at the RNA stability level, and what factors affect their stability. The focus of our research is development of novel computational methods that enables the measurement of RNA stability and decay rate from functional genomics data, and inference of models that explain how human cells regulate RNA stability. We are particularly interested in how defects in regulation of RNA stability can lead to development and progression of various human diseases, such as cancer.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
