<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2020 Fall on McGill Statistics Seminars</title>
    <link>https://mcgillstat.github.io/tags/2020-fall/</link>
    <description>Recent content in 2020 Fall on McGill Statistics Seminars</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Dec 2020 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://mcgillstat.github.io/tags/2020-fall/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Quasi-random sampling for multivariate distributions via generative neural networks</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-12-04/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-12-04/</guid>
      <description>&lt;h4 id=&#34;date-2020-12-04&#34;&gt;Date: 2020-12-04&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A novel approach based on generative neural networks is introduced for constructing quasi-random number generators for multivariate models with any underlying copula in order to estimate expectations with variance reduction. So far, quasi-random number generators for multivariate distributions required a careful design, exploiting specific properties (such as conditional distributions) of the implied copula or the underlying quasi-Monte Carlo point set, and were only tractable for a small number of models. Utilizing specific generative neural networks allows one to construct quasi-random number generators for a much larger variety of multivariate distributions without such restrictions. Once trained with a pseudo-random sample, these neural networks only require a multivariate standard uniform randomized quasi-Monte Carlo point set as input and are thus fast in estimating expectations under dependence with variance reduction. Reproducible numerical examples are considered to demonstrate the approach. Emphasis is put on ideas rather than mathematical proofs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Probabilistic Approaches to Machine Learning on Tensor Data</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-11-27/</link>
      <pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-11-27/</guid>
      <description>&lt;h4 id=&#34;date-2020-11-27&#34;&gt;Date: 2020-11-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In contemporary scientific research, it is often of great interest to predict a categorical response based on a high-dimensional tensor (i.e. multi-dimensional array). Motivated by applications in science and engineering, we propose two probabilistic methods for machine learning on tensor data in the supervised and the unsupervised context, respectively. For supervised problems, we develop a comprehensive discriminant analysis model, called the CATCH model. The CATCH model integrates the information from the tensor and additional covariates to predict the categorical outcome with high accuracy. We further consider unsupervised problems, where no categorical response is available even on the training data. A doubly-enhanced EM (DEEM) algorithm is proposed for model-based tensor clustering, in which both the E-step and the M-step are carefully tailored for tensor data. CATCH and DEEM are developed under explicit statistical models with clear interpretations. They aggressively take advantage of the tensor structure and sparsity to tackle the new computational and statistical challenges arising from the intimidating tensor dimensions. Efficient algorithms are developed to solve the related optimization problems. Under mild conditions, CATCH and DEEM are shown to be consistent even when the dimension of each mode grows at an exponential rate of the sample size. Numerical studies also strongly support the application of CATCH and DEEM.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modeling viral rebound trajectories after analytical antiretroviral treatment interruption</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-11-20/</link>
      <pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-11-20/</guid>
      <description>&lt;h4 id=&#34;date-2020-11-20&#34;&gt;Date: 2020-11-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Despite the success of combined antiretroviral therapy (ART) in achieving sustained control of viral replication, the concerns about side-effects, drug-drug interactions, drug resistance and cost call for a need to identify strategies for achieving HIV eradication or an ART-free remission. Following ART withdrawal, patients&amp;rsquo; viral load levels usually increase rapidly to a peak followed by a dip, and then stabilize at a viral load set point. Characterizing features of the viral rebound trajectories (e.g., time to viral rebound and viral set points) and identifying host, virological, and immunological factors that are predictive of these features requires addressing analytical challenges such as non-linear viral rebound trajectories, coarsened data due to the assay’s limit of quantification, and intermittent measurements of viral load values. We first introduce a parametric nonlinear mixed effects (NLME) model for the viral rebound trajectory and compare its performance to a mechanistic modeling approach. We then develop a smoothed simulated pseudo maximum likelihood method for fitting NLME models that permits flexible specification of random effects distributions. Finally, we investigate the association between the time to viral suppression after ART initiation and the time to viral rebound after ART interruption through a Cox proportional hazard regression model where both the outcome and the covariate are interval-censored observations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Approximate Cross-Validation for Large Data and High Dimensions</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-11-13/</link>
      <pubDate>Fri, 13 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-11-13/</guid>
      <description>&lt;h4 id=&#34;date-2020-11-13&#34;&gt;Date: 2020-11-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpcrmumontrealcacolloque-sciences-mathematiques-quebeccsmq&#34;&gt;&lt;a href=&#34;http://crm.umontreal.ca/colloque-sciences-mathematiques-quebec/#csmq&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The error or variability of statistical and machine learning algorithms&#xA;is often assessed by repeatedly re-fitting a model with different&#xA;weighted versions of the observed data.  The ubiquitous tools of&#xA;cross-validation (CV) and the bootstrap are examples of this technique.&#xA;These methods are powerful in large part due to their model agnosticism&#xA;but can be slow to run on modern, large data sets due to the need to&#xA;repeatedly re-fit the model.  We use a linear approximation to the&#xA;dependence of the fitting procedure on the weights, producing results&#xA;that can be faster than repeated re-fitting by orders of magnitude.&#xA;This linear approximation is sometimes known as the &amp;ldquo;infinitesimal&#xA;jackknife&amp;rdquo; (IJ) in the statistics literature, where it has mostly been&#xA;used as a theoretical tool to prove asymptotic results.  We provide&#xA;explicit finite-sample error bounds for the infinitesimal jackknife in&#xA;terms of a small number of simple, verifiable assumptions.  Without&#xA;further modification, though, we note that the IJ deteriorates in&#xA;accuracy in high dimensions and incurs a running time roughly cubic in&#xA;dimension.  We additionally show, then, how dimensionality reduction can&#xA;be used to successfully run the IJ in high dimensions when data is&#xA;sparse or low rank.  Simulated and real-data experiments support our&#xA;theory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generalized Energy-Based Models</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-11-06/</link>
      <pubDate>Fri, 06 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-11-06/</guid>
      <description>&lt;h4 id=&#34;date-2020-11-06&#34;&gt;Date: 2020-11-06&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;I will introduce Generalized Energy Based Models (GEBM) for generative modelling. These models combine two trained components: a base distribution (generally an implicit model), which can learn the support of data with low intrinsic dimension in a high dimensional space; and an energy function, to refine the probability mass on the learned support. Both the energy function and base jointly constitute the final model, unlike GANs, which retain only the base distribution (the &amp;ldquo;generator&amp;rdquo;). In particular, while the energy function is analogous to the GAN critic function, it is not discarded after training.&#xA;GEBMs are trained by alternating between learning the energy and the base, much like a GAN. Both training stages are well-defined: the energy is learned by maximising a generalized likelihood, and the resulting energy-based loss provides informative gradients for learning the base. Samples from the posterior on the latent space of the trained model can be obtained via MCMC, thus finding regions in this space that produce better quality samples. Empirically, the GEBM samples on image-generation tasks are of better quality than those from the learned generator alone, indicating that all else being equal, the GEBM will outperform a GAN of the same complexity. GEBMs also return state-of-the-art performance on density modelling tasks, and when using base measures with an explicit form.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test-based integrative analysis of randomized trial and real-world data for treatment heterogeneity estimation</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-30/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-30/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-30&#34;&gt;Date: 2020-10-30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Parallel randomized clinical trial (RCT) and real-world data (RWD) are becoming increasingly available for treatment evaluation. Given the complementary features of the RCT and RWD, we propose a test-based integrative analysis of the RCT and RWD for accurate and robust estimation of the heterogeneity of treatment effect (HTE), which lies at the heart of precision medicine. When the RWD are not subject to bias, e.g., due to unmeasured confounding, our approach combines the RCT and RWD for optimal estimation by exploiting semiparametric efficiency theory. Utilizing the design advantage of RTs, we construct a built-in test procedure to gauge the reliability of the RWD and decide whether or not to use RWD in an integrative analysis. We characterize the asymptotic distribution of the test-based integrative estimator under local alternatives, which provides a better approximation of the finite-sample behaviors of the test and estimator when the idealistic assumption required for the RWD is weakly violated. We provide a data-adaptive procedure to select the threshold of the test statistic that promises the smallest mean square error of the proposed estimator of the HTE. Lastly, we construct an adaptive confidence interval that has a good finite-sample coverage property. We apply the proposed method to characterize who can benefit from adjuvant chemotherapy in patients with stage IB non-small cell lung cancer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Regression and its Inference on Noisy Network-linked Data</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-23/</link>
      <pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-23/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-23&#34;&gt;Date: 2020-10-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Linear regression on a set of observations linked by a network has been an essential tool in modeling the relationship between response and covariates with additional network data. Despite its wide range of applications in many areas, such as social sciences and health-related research, the problem has not been well-studied in statistics so far. Previous methods either lack of inference tools or rely on restrictive assumptions on social effects, and usually treat the network structure as precisely observed, which is too good to be true in many problems. We propose a linear regression model with nonparametric social effects. Our model does not assume the relational data or network structure to be accurately observed; thus, our method can be provably robust to a certain level of perturbation of the network structure. We establish a full set of computationally efficient asymptotic inference tools under a general requirement of the perturbation and then study the robustness of our method in the specific setting when the perturbation is from random network models. We discover a phase-transition phenomenon of inference validity concerning the network density when no prior knowledge about the network model is available, while also show the significant improvement achieved by knowing the network model. A by-product of our analysis is a rate-optimal concentration bound about subspace projection that may be of independent interest. We conduct extensive simulation studies to verify our theoretical observations and demonstrate the advantage of our method compared to a few benchmarks under different data-generating models. The method is then applied to adolescent network data to study the gender and racial differences in social activities.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adaptive MCMC For Everyone</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-16/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-16/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-16&#34;&gt;Date: 2020-10-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Markov chain Monte Carlo (MCMC) algorithms, such as the Metropolis&#xA;Algorithm and the Gibbs Sampler, are an extremely useful and popular&#xA;method of approximately sampling from complicated probability&#xA;distributions.  Adaptive MCMC attempts to automatically modify the&#xA;algorithm while it runs, to improve its performance on the fly.  However,&#xA;such adaptation often destroys the ergodicity properties necessary for the&#xA;algorithm to be valid.  In this talk, we first illustrate MCMC algorithms&#xA;using simple graphical examples.  We then discuss adaptive MCMC, and&#xA;present examples and theorems concerning its ergodicity and efficiency.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning and Neural Networks: Foundations and Some Fundamental Questions</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-09/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-09/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-09&#34;&gt;Date: 2020-10-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statistical learning theory is by now a mature branch of data science that hosts a vast variety of practical techniques for tackling data-related problems. In this talk we present some fundamental concepts upon which statistical learning theory has been based. Different approaches to statistical inference will be discussed and the main problem of learning from Vapnik&amp;rsquo;s point of view will be explained. Further we discuss the topic of function estimation as the heart of Vapnik-Chervonenkis theory. There exist several state-of-the-art methods for estimating functional dependencies, such as maximum margin estimator and artificial neural networks. While for some of these methods, e.g., the support vector machines, there has already been developed a profound theory, others require more investigation. Accordingly, we pay a closer attention to the so-called mapping neural networks and try to shed some light on certain theoretical aspects of them. We highlight some of the fundamental challenges that have attracted the attention of researcher and they are yet to be fully resolved. One of these challenges is estimation of the intrinsic dimension of data that will be discussed in detail. Another challenge is inferring causal direction when the training data set is not representative of the target population.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Science, Classification, Clustering and Three-Way Data</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-02/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-02/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-02&#34;&gt;Date: 2020-10-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsumontrealzoomusj93983313215pwdclb6cunssjavrmfmme1pblhktutsqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-8331-3215&#34;&gt;Meeting ID: 939 8331 3215&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-096952&#34;&gt;Passcode: 096952&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Data science is discussed along with some historical perspective.  Selected problems in classification are considered, either via specific datasets or general problem types.  In each case, the problem is introduced before one or more potential solutions are discussed and applied.  The problems discussed include data with outliers, longitudinal data, and three-way data.  The proposed approaches are generally mixture model-based.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Large-scale Network Inference</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-09-25/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-09-25/</guid>
      <description>&lt;h4 id=&#34;date-2020-09-25&#34;&gt;Date: 2020-09-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1400-1500&#34;&gt;Time: 14:00-15:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj93947077997&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/93947077997&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-4707-7997&#34;&gt;Meeting ID: 939 4707 7997&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-no-password&#34;&gt;Passcode: no password&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Network data is prevalent in many contemporary big data applications in which a common interest is to unveil important latent links between different pairs of nodes. Yet a simple fundamental question of how to precisely quantify the statistical uncertainty associated with the identification of latent links still remains largely unexplored. In this paper, we propose the method of statistical inference on membership profiles in large networks (SIMPLE) in the setting of degree-corrected mixed membership model, where the null hypothesis assumes that the pair of nodes share the same profile of community memberships. In the simpler case of no degree heterogeneity, the model reduces to the mixed membership model for which an alternative more robust test is also proposed. Both tests are of the Hotelling-type statistics based on the rows of empirical eigenvectors or their ratios, whose asymptotic covariance matrices are very challenging to derive and estimate. Nevertheless, their analytical expressions are unveiled and the unknown covariance matrices are consistently estimated. Under some mild regularity conditions, we establish the exact limiting distributions of the two forms of SIMPLE test statistics under the null hypothesis and contiguous alternative hypothesis. They are the chi-square distributions and the noncentral chi-square distributions, respectively, with degrees of freedom depending on whether the degrees are corrected or not. We also address the important issue of estimating the unknown number of communities and establish the asymptotic properties of the associated test statistics. The advantages and practical utility of our new procedures in terms of both size and power are demonstrated through several simulation examples and real network applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>BdryGP: a boundary-integrated Gaussian process model for computer code emulation</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-09-18/</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-09-18/</guid>
      <description>&lt;h4 id=&#34;date-2020-09-18&#34;&gt;Date: 2020-09-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;With advances in mathematical modeling and computational methods, complex phenomena (e.g., universe formations, rocket propulsion) can now be reliably simulated via computer code. This code solves a complicated system of equations representing the underlying science of the problem. Such simulations can be very time-intensive, requiring months of computation for a single run. Gaussian processes (GPs) are widely used as predictive models for “emulating” this expensive computer code. Yet with limited training data on a high-dimensional parameter space, such models can suffer from poor predictive performance and physical interpretability.&#xA;Fortunately, in many physical applications, there is additional boundary information on the code beforehand, either from governing physics or scientific knowledge. We propose a new BdryGP model which incorporates such boundary information for prediction. We show that BdryGP not only enjoys improved convergence rates over standard GP models which do not incorporate boundaries, but is also more resistant to the ``curse-of-dimensionality&amp;rsquo;&amp;rsquo; in nonparametric regression. We then demonstrate the improved predictive performance and posterior contraction of the BdryGP model on several test problems in the literature.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning for Causal Inference</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-09-11/</link>
      <pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-09-11/</guid>
      <description>&lt;h4 id=&#34;date-2020-09-11&#34;&gt;Date: 2020-09-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1600-1700&#34;&gt;Time: 16:00-17:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsumontrealzoomusj96525367383pwddzburjbvc2fwtgpyruh4aurbz0rvqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/96525367383?pwd=dzBURjBvc2FWTGpyRUh4aURBZ0RvQT09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-965-2536-7383&#34;&gt;Meeting ID: 965 2536 7383&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-421254&#34;&gt;Passcode: 421254&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Given advances in machine learning over the past decades, it is now possible to accurately solve difficult non-parametric prediction problems in a way that is routine and reproducible. In this talk, I’ll discuss how machine learning tools can be rigorously integrated into observational study analyses, and how they interact with classical statistical ideas around randomization, semiparametric modeling, double robustness, etc. I’ll also survey some recent advances in methods for treatment heterogeneity. When deployed carefully, machine learning enables us to develop causal estimators that reflect an observational study design more closely than basic linear regression based methods.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
