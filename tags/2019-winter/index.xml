<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2019 Winter on McGill Statistics Seminars</title>
    <link>/tags/2019-winter/</link>
    <description>Recent content in 2019 Winter on McGill Statistics Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/2019-winter/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>New Perspectives on Generative Adversarial Networks</title>
      <link>/post/2019winter/2019-01-25/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019winter/2019-01-25/</guid>
      <description>Date: 2019-01-25 Time: 15:30-16:30 Location: BURN 1104 Abstract: Generative Adversarial Networks (GANs) are a popular generative modeling approach known for producing appealing samples, but their theoretical properties are not yet fully understood, and they are notably difficult to train. In the first part of this talk, I will provide some insights on why GANs are a more meaningful framework to model high dimensional data like images than the more traditional maximum likelihood approach, interpreting them as &amp;ldquo;parametric adversarial divergences&amp;rdquo; and rooting the analysis with statistical decision theory.</description>
    </item>
    
    <item>
      <title>Magic Cross-Validation Theory for Large-Margin Classification</title>
      <link>/post/2019winter/2019-01-11/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019winter/2019-01-11/</guid>
      <description>Date: 2019-01-11 Time: 15:30-16:30 Location: BURN 1104 Abstract: Cross-validation (CV) is perhaps the most widely used tool for tuning supervised machine learning algorithms in order to achieve better generalization error rate. In this paper, we focus on leave-one-out cross-validation (LOOCV) for the support vector machine (SVM) and related algorithms. We first address two wide-spreading misconceptions on LOOCV. We show that LOOCV, ten-fold, and five-fold CV are actually well-matched in estimating the generalization error, and the computation speed of LOOCV is not necessarily slower than that of ten-fold and five-fold CV.</description>
    </item>
    
  </channel>
</rss>