<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2019 Winter on McGill Statistics Seminars</title>
    <link>/tags/2019-winter/</link>
    <description>Recent content in 2019 Winter on McGill Statistics Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/2019-winter/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>New Perspectives on Generative Adversarial Networks</title>
      <link>/post/2019winter/2019-01-25/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019winter/2019-01-25/</guid>
      <description>Date: 2019-01-25 Time: 15:30-16:30 Location: BURN 1104 Abstract: Generative Adversarial Networks (GANs) are a popular generative modeling approach known for producing appealing samples, but their theoretical properties are not yet fully understood, and they are notably difficult to train. In the first part of this talk, I will provide some insights on why GANs are a more meaningful framework to model high dimensional data like images than the more traditional maximum likelihood approach, interpreting them as &amp;ldquo;parametric adversarial divergences&amp;rdquo; and rooting the analysis with statistical decision theory.</description>
    </item>
    
    <item>
      <title>Singularities of the information matrix and longitudinal data with change points</title>
      <link>/post/2019winter/2019-01-18/</link>
      <pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019winter/2019-01-18/</guid>
      <description>Date: 2019-01-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: Non-singularity of the information matrix plays a key role in model identification and the asymptotic theory of statistics. For many statistical models, however, this condition seems virtually impossible to verify. An example of such models is a class of mixture models associated with multi-path change-point problems (MCP) which can model longitudinal data with change points. The MCP models are similar in nature to mixture-of-experts models in machine learning.</description>
    </item>
    
    <item>
      <title>Magic Cross-Validation Theory for Large-Margin Classification</title>
      <link>/post/2019winter/2019-01-11/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019winter/2019-01-11/</guid>
      <description>Date: 2019-01-11 Time: 15:30-16:30 Location: BURN 1205 Abstract: Cross-validation (CV) is perhaps the most widely used tool for tuning supervised machine learning algorithms in order to achieve better generalization error rate. In this paper, we focus on leave-one-out cross-validation (LOOCV) for the support vector machine (SVM) and related algorithms. We first address two wide-spreading misconceptions on LOOCV. We show that LOOCV, ten-fold, and five-fold CV are actually well-matched in estimating the generalization error, and the computation speed of LOOCV is not necessarily slower than that of ten-fold and five-fold CV.</description>
    </item>
    
  </channel>
</rss>