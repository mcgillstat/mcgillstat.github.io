<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>McGill Statistics Seminar on McGill Statistics Seminars</title>
    <link>https://mcgillstat.github.io/categories/mcgill-statistics-seminar/</link>
    <description>Recent content in McGill Statistics Seminar on McGill Statistics Seminars</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Conditional nonparametric variable screening via neural network factor regression</title>
      <link>https://mcgillstat.github.io/post/2024fall/2024-12-06/</link>
      <pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024fall/2024-12-06/</guid>
      <description>&lt;h4 id=&#34;date-2024-12-06&#34;&gt;Date: 2024-12-06&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83785721810httpsmcgillzoomusj83785721810&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83785721810&#34;&gt;https://mcgill.zoom.us/j/83785721810&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-837-8572-1810&#34;&gt;Meeting ID: 837 8572 1810&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;High-dimensional covariates often admit linear factor structure. To effectively screen correlated covariates in high-dimension, we propose a conditional variable screening test based on non-parametric regression using neural networks due to their representation power. We ask the question whether individual covariates have additional contributions given the latent factors. Our test statistics are based on the estimated partial derivative of the regression function of the candidate variable for screening and an observable proxy for the latent factors. Hence, our test reveals how much predictors contribute additionally to the non-parametric regression after accounting for the latent factors. Our derivative estimator is the convolution of a deep neural network regression estimator and a smoothing kernel. We demonstrate that when the neural network size diverges with the sample size, unlike estimating the regression function itself, it is necessary to smooth the partial derivative of the neural network estimator&#xA;to recover the desired convergence rate for the derivative. Moreover, our screening test achieves asymptotic normality under the null after finely centering our test statistics that makes the biases negligible, as well as consistency for local alternatives under mild conditions. We demonstrate the performance of our test in a simulation study and a real world application.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Asymptotic behavior of data driven empirical measures for testing multivariate regular variation</title>
      <link>https://mcgillstat.github.io/post/2024fall/2024-11-22/</link>
      <pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024fall/2024-11-22/</guid>
      <description>&lt;h4 id=&#34;date-2024-11-22&#34;&gt;Date: 2024-11-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj82125361063httpsmcgillzoomusj82125361063&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/82125361063&#34;&gt;https://mcgill.zoom.us/j/82125361063&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-821-2536-1063&#34;&gt;Meeting ID: 821 2536 1063&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Nowadays, empirical processes are well known objects. A reason that push forward theirs studies is that, in many models, we can write the estimators as images of empirical measures. In this work, the interest is touched upon the case of local empirical measures built over a sub-sample of data conditioned to be in a certain area, itself depending on the data. In the present work we present a general framework which allows to derive asymptotic results for these empirical measures. This approach is specified for the framework of extreme values theory. As an application, an asymptotic result allowing to derive a test procedure for Multivariate Regular Variation is detailed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Practical existence theorems for deep learning approximation in high dimensions</title>
      <link>https://mcgillstat.github.io/post/2024fall/2024-11-15/</link>
      <pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024fall/2024-11-15/</guid>
      <description>&lt;h4 id=&#34;date-2024-11-15&#34;&gt;Date: 2024-11-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj89043936588httpsmcgillzoomusj89043936588&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/89043936588&#34;&gt;https://mcgill.zoom.us/j/89043936588&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-890-4393-6588&#34;&gt;Meeting ID: 890 4393 6588&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Deep learning is having a profound impact on industry and scientific research. Yet, while this paradigm continues to show impressive performance in a wide variety of applications, its mathematical foundations are far from being well understood. Motivated by deep learning methods for scientific computing, I will present new practical existence theorems that aim at bridging the gap between theory and practice in this area. Combining universal approximation results for deep neural networks with sparse high-dimensional polynomial approximation theory, these theorems identify sufficient conditions on the network architecture, the training strategy, and the size of the training set able to guarantee a target accuracy. I will illustrate practical existence theorems in the contexts of high-dimensional function approximation via feedforward networks, reduced order modeling based on convolutional autoencoders, and physics-informed neural networks for high-dimensional PDEs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A latent-vine factor-copula time series model for extreme flood insurance losses</title>
      <link>https://mcgillstat.github.io/post/2024fall/2024-11-08/</link>
      <pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024fall/2024-11-08/</guid>
      <description>&lt;h4 id=&#34;date-2024-11-08&#34;&gt;Date: 2024-11-08&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj89121567327httpsmcgillzoomusj89121567327&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/89121567327&#34;&gt;https://mcgill.zoom.us/j/89121567327&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-891-2156-7327&#34;&gt;Meeting ID: 891 2156 7327&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Vines and factor copula models are handy tools for statistical inference in high dimension. However, their use for assessing and predicting the co-occurrence of rare events is subject to caution when multivariate extreme data are sparse. Motivated by the need to assess the risk of concurrent large insurance claims in the American National Flood Insurance Program (NFIP), I will describe a novel class of copula models that can account for spatio-temporal dependence within clustered sets of time series. This new class, which combines the advantages of vines and factor copula models, provides great flexibility in capturing tail dependence while maintaining interpretability through a parsimonious latent structure. Using NFIP data, I will show the value of this approach in evaluating the risks associated with extreme weather events.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On Mixture of Experts in Large-Scale Statistical Machine Learning Applications</title>
      <link>https://mcgillstat.github.io/post/2024fall/2024-11-01/</link>
      <pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024fall/2024-11-01/</guid>
      <description>&lt;h4 id=&#34;date-2024-11-01&#34;&gt;Date: 2024-11-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj81284191962httpsmcgillzoomusj81284191962&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81284191962&#34;&gt;https://mcgill.zoom.us/j/81284191962&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-812-8419-1962&#34;&gt;Meeting ID: 812 8419 1962&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Mixtures of experts (MoEs), a class of statistical machine learning models that combine multiple models, known as experts, to form more complex and accurate models, have been combined&#xA;into deep learning architectures to improve the ability of these architectures and AI models to capture the heterogeneity of the data and to scale up these architectures without increasing the computational&#xA;cost. In mixtures of experts, each expert specializes in a different aspect of the data, which is then combined with a gating function to produce the final output. Therefore, parameter and expert estimates&#xA;play a crucial role by enabling statisticians and data scientists to articulate and make sense of the diverse patterns present in the data. However, the statistical behaviors of parameters and experts in a mixture&#xA;of experts have remained unsolved, which is due to the complex interaction between gating function and expert parameters.&lt;/p&gt;</description>
    </item>
    <item>
      <title>New Statistical Methods and Quality Control for Industry 4.0</title>
      <link>https://mcgillstat.github.io/post/2024fall/2024-10-25/</link>
      <pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024fall/2024-10-25/</guid>
      <description>&lt;h4 id=&#34;date-2024-10-25&#34;&gt;Date: 2024-10-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-online-retransmitted-in-burnside-1104&#34;&gt;Location: Online, retransmitted in Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj81908885431httpsmcgillzoomusj81908885431&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81908885431&#34;&gt;https://mcgill.zoom.us/j/81908885431&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-819-0888-5431&#34;&gt;Meeting ID: 819 0888 5431&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statistical and machine learning techniques have emerged as powerful tools in Industry 4.0 to benefit from the increasing availability of sensors and data and enable informed decision-making and process optimization. This seminar will provide an overview of several industrial applications in high-dimensional settings developed by the Statistics for Engineering Research (SFERe) group (&lt;a href=&#34;https://www.sfere.unina.it&#34;&gt;www.sfere.unina.it&lt;/a&gt;), affiliated with the Department of Industrial Engineering at the University of Naples FEDERICO II. In these applications, the definition and use of novel statistical methods have been a competitive advantage for naval, automotive, and rail companies. The open-source R software packages implementing these methods will also be mentioned to highlight their accessibility and potential applicability in different industrial contexts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A functional data approach for statistical shapes analysis</title>
      <link>https://mcgillstat.github.io/post/2024fall/2024-10-11/</link>
      <pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024fall/2024-10-11/</guid>
      <description>&lt;h4 id=&#34;date-2024-10-11&#34;&gt;Date: 2024-10-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj87824357176httpsmcgillzoomusj87824357176&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/87824357176&#34;&gt;https://mcgill.zoom.us/j/87824357176&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-878-2435-7176&#34;&gt;Meeting ID: 878 2435 7176&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The shape $\tilde{\mathbf{X}}$ of a random planar curve, $\mathbf{X}$, is what remains when the deformation variables (scaling, rotation, translation, and reparametrization) are removed. Previous studies in statistical shape analysis have focused on analyzing $\tilde{\bf X}$ through discrete observations of ${\bf X}$. While this approach has some computational advantages, it overlooks the continuous nature of variables: $\tilde{\bf X}$, ${\bf X}$, and it ignores the potential dependence of deformation variables on each other and $\tilde{ \bf X}$, which results in a loss of information in the data structure. I will introduce a new framework for studying $\bf X$ based on functional data analysis in this presentation. Basis expansion techniques are employed to find analytic solutions for deformation variables such as rotation and parametrization deformations. Then, the generative model of $\bf X$ is investigated using a joint-principal component analysis approach. Numerical experiments on synthetic and real datasets demonstrate how this new approach performs better at analyzing random planar curves than traditional functional data methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>VCBART: Bayesian trees for varying coefficients</title>
      <link>https://mcgillstat.github.io/post/2024fall/2024-09-27/</link>
      <pubDate>Fri, 27 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024fall/2024-09-27/</guid>
      <description>&lt;h4 id=&#34;date-2024-09-27&#34;&gt;Date: 2024-09-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-online-retransmitted-in-burnside-1104&#34;&gt;Location: Online, retransmitted in Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj88350756970httpsmcgillzoomusj88350756970&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/88350756970&#34;&gt;https://mcgill.zoom.us/j/88350756970&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-883-5075-6970&#34;&gt;Meeting ID: 883 5075 6970&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The linear varying coefficient models posits a linear relationship between an outcome and covariates in which the covariate effects are modeled as functions of additional effect modifiers. Despite a long history of study and use in statistics and econometrics, state-of-the-art varying coefficient modeling methods cannot accommodate multivariate effect modifiers without imposing restrictive functional form assumptions or involving computationally intensive hyperparameter tuning. In response, we introduce VCBART which flexibly estimates the covariate effect in a varying coefficient model using Bayesian Additive Regression Trees. With simple default settings, VCBART outperforms existing varying coefficient methods in terms of covariate effect estimation, uncertainty quantification, and outcome prediction. Theoretically, we show that the VCBART posterior contracts at the near-minimax optimal rate. Finally, we illustrate the utility of VCBART through simulation studies and a real data application examining how the association between later-life cognition and measures of socioeconomic position vary with respect to age and sociodemographics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variance reduction by occluding a Markov chain</title>
      <link>https://mcgillstat.github.io/post/2024fall/2024-09-20/</link>
      <pubDate>Fri, 20 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024fall/2024-09-20/</guid>
      <description>&lt;h4 id=&#34;date-2024-09-20&#34;&gt;Date: 2024-09-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj88265323185httpsmcgillzoomusj88265323185&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/88265323185&#34;&gt;https://mcgill.zoom.us/j/88265323185&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-882-6532-3185&#34;&gt;Meeting ID: 882 6532 3185&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Stochastic algorithms which simulate random variables/processes on a computer to estimate intractable quantities are ubiquitous in Statistics and elsewhere. One such method is Markov chain Monte Carlo which, under mild conditions, offer asymptotical (in time) guarantees. In this talk, we define infinitely many stopping times at which an ergodic Markov chain is occluded by a (conditionally) independent process. The resulting process, called the occluded process, is not Markov, but provided that the stopping times/independent process are cleverly defined, we show that it is ergodic. One particularly powerful way to define the stopping times/independent process leverages the recent advances in ML regarding approximations of probability distributions (divergence minimization, normalizing flows, etc.). We discuss the variance reduction effect of the occluded process through some illustrations and (weak) theoretical results in some limiting regime.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Free energy fluctuations of spherical spin glasses near the critical temperature threshold</title>
      <link>https://mcgillstat.github.io/post/2024winter/2024-04-12/</link>
      <pubDate>Fri, 12 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024winter/2024-04-12/</guid>
      <description>&lt;h4 id=&#34;date-2024-04-12&#34;&gt;Date: 2024-04-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location--in-person-burnside-1104&#34;&gt;Location:  In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj86957985232httpsmcgillzoomusj86957985232&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/86957985232&#34;&gt;https://mcgill.zoom.us/j/86957985232&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-869-5798-5232&#34;&gt;Meeting ID: 869 5798 5232&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;One of the fascinating phenomena of spin glasses is the dramatic change in behavior that occurs between the high and low temperature regimes.  In addition to its physical meaning, this phase transition corresponds to a detection threshold with respect to the signal-to-noise ratio in a spiked matrix model.  The free energy of the spherical Sherrington-Kirkpatrick (SSK) model has Gaussian fluctuations at high temperature, but Tracy-Widom fluctuations at low temperature.  A similar phenomenon holds for the bipartite SSK model, and we show that, when the temperature is within a small window around the critical temperature, the free energy fluctuations converge to an independent sum of Gaussian and Tracy-Widom random variables (joint work with Han Le).  Our work follows two recent papers that proved similar results for the SSK model (by Landon and by Johnstone, Klochkov, Onatski, Pavlyshyn).  From a statistical perspective, the free energy of SSK and bipartite SSK correspond to log-likelihood ratios for spiked Wigner and spiked Wishart matrices respectively.  Analyzing bipartite SSK at critical temperature requires a variety of tools including classical random matrix results, contour integral techniques, and a CLT for the log-characteristic polynomial of Wishart random matrices evaluated near the spectral edge.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Minimum Covariance Determinant: Spectral Embedding and Subset Size Determination</title>
      <link>https://mcgillstat.github.io/post/2024winter/2024-03-22/</link>
      <pubDate>Fri, 22 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024winter/2024-03-22/</guid>
      <description>&lt;h4 id=&#34;date-2024-03-22&#34;&gt;Date: 2024-03-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-online-retransmitted-in-burnside-1104&#34;&gt;Location: Online, retransmitted in Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj81895414756httpsmcgillzoomusj81895414756&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81895414756&#34;&gt;https://mcgill.zoom.us/j/81895414756&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-818-9541-4756&#34;&gt;Meeting ID: 818 9541 4756&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This paper introduces several enhancements to the minimum covariance determinant method of outlier detection and robust estimation of means and covariances. We leverage the principal component transform to achieve dimension reduction and ultimately better analyses. Our best subset selection algorithm strategically combines statistical depth and concentration steps. To ascertain the appropriate subset size and number of principal components, we introduce a bootstrap procedure that estimates the instability of the best subset algorithm. The parameter combination exhibiting minimal instability proves ideal for the purposes of outlier detection and robust estimation. Rigorous benchmarking against prominent MCD variants showcases our approach&amp;rsquo;s superior statistical performance and computational speed in high dimensions. Application to a fruit spectra data set and a cancer genomics data set illustrates our claims.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Recent advances in causal inference under irregular and informative observation times for the outcome</title>
      <link>https://mcgillstat.github.io/post/2024winter/2024-03-01/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024winter/2024-03-01/</guid>
      <description>&lt;h4 id=&#34;date-2024-03-01&#34;&gt;Date: 2024-03-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj89811237909httpsmcgillzoomusj89811237909&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/89811237909&#34;&gt;https://mcgill.zoom.us/j/89811237909&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-898-1123-7909&#34;&gt;Meeting ID: 898 1123 7909&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Electronic health records (EHR) data contain rich information about patients’ health condition, comorbidities, clinical outcomes, and drug prescriptions. They are often used to draw causal inferences and compare different treatments’ effectiveness. However, these data are not experimental. They present with special features that should be addressed or that may affect the inference. One of these features is the irregular observation of the longitudinal processes used in the inference. In longitudinal studies in which we seek the causal effect of a treatment on a repeated outcome, for instance, covariate-dependent observation of the outcome has been shown to bias standard causal estimators. In this presentation, I will review recent work and present some of the most interesting findings in this area of research. Themes will include identifiability, efficiency, and alternatives to weighting methods to address irregular observation times.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Matrix completion in genetic methylation studies: LMCC, a Linear Model of Coregionalization with informative Covariates</title>
      <link>https://mcgillstat.github.io/post/2024winter/2024-02-16/</link>
      <pubDate>Fri, 16 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024winter/2024-02-16/</guid>
      <description>&lt;h4 id=&#34;date-2024-02-16&#34;&gt;Date: 2024-02-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj82678428848httpsmcgillzoomusj82678428848&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/82678428848&#34;&gt;https://mcgill.zoom.us/j/82678428848&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-826-7842-8848&#34;&gt;Meeting ID: 826 7842 8848&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;DNA methylation is an important epigenetic mark that modulates gene expression through the inhibition of transcriptional proteins binding to DNA. As in many other omics experiments, missing values is an issue and appropriate imputation techniques are important to avoid an unnecessary sample size reduction as well as to optimally leverage the information collected. We consider the case where a relatively small number of samples are processed via an expensive high-density Whole Genome Bisulfite Sequencing (WGBS) strategy and a larger number of samples are processed using more affordable low-density array-based technologies. In such cases, one can impute/complete the data matrix of the low coverage (array-based) methylation data using the high-density information provided by the WGBS samples. In this work, we propose an efficient Linear Model of Coregionalization with informative Covariates (LMCC) to predict missing values based on observed values and informative covariates. Our model assumes that at each genomics position, the methylation vector of all samples is linked to the set of fixed factors (covariates) and a set of latent factors. Furthermore, we exploit the functional nature of the data and the spatial correlation across positions/sites by assuming Gaussian processes on the fixed and latent coefficient vectors, respectively. Our simulations show that the use of covariates can significantly improve the accuracy of imputed values, especially in cases where missing data contain some relevant information about the explanatory variable. We also show that the proposed model is efficient when the number of columns is much greater than the number of rows in the data matrix-which is usually the case in methylation data analysis. Finally, we apply and compare the proposed method with alternative approaches to complete a matrix of DNA methylation containing 15 rows (methylation samples) and 1 million columns (sites).&#xA;Joint work with Melina Ribaud and Aurelie Labbe (HEC, Montreal).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mesoscale two-sample testing for networks</title>
      <link>https://mcgillstat.github.io/post/2024winter/2024-02-09/</link>
      <pubDate>Fri, 09 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024winter/2024-02-09/</guid>
      <description>&lt;h4 id=&#34;date-2024-02-09&#34;&gt;Date: 2024-02-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj87465663442httpsmcgillzoomusj87465663442&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/87465663442&#34;&gt;https://mcgill.zoom.us/j/87465663442&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-874-6566-3442&#34;&gt;Meeting ID: 874 6566 3442&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Networks arise naturally in many scientific fields as a representation of pairwise connections. Statistical network analysis has most often considered a single large network, but it is common in a number of applications, for example, neuroimaging, to observe multiple networks on a shared node set. When these networks are grouped by case-control status or another categorical covariate, the classical statistical question of two-sample comparison arises. In this work, we address the problem of testing for statistically significant differences in a prespecified subset of the connections. This general framework allows an analyst to focus on a single node, a specific region of interest, or compare whole networks. In this &amp;ldquo;mesoscale&amp;rdquo; setting, we develop statistically sound projection-based tests for two-sample comparison in both weighted and binary edge networks. Our approach can leverage all available network information, and learn informative projections which improve testing power when low-dimensional network structure is present.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fast calibration of FARIMA models with dependent errors</title>
      <link>https://mcgillstat.github.io/post/2024winter/2024-02-02/</link>
      <pubDate>Fri, 02 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024winter/2024-02-02/</guid>
      <description>&lt;h4 id=&#34;date-2024-02-02&#34;&gt;Date: 2024-02-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-online-retransmitted-in-burnside-1104&#34;&gt;Location: Online, retransmitted in Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj89669635642httpsmcgillzoomusj89669635642&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/89669635642&#34;&gt;https://mcgill.zoom.us/j/89669635642&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-896-6963-5642&#34;&gt;Meeting ID: 896 6963 5642&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this work, we investigate the asymptotic properties of Le Cam’s one-step estimator for weak Fractionally AutoRegressive Integrated Moving-Average (FARIMA) models. For these models, noises are uncorrelated but neither necessarily independent nor martingale differences errors. We show under some regularity assumptions that the one-step estimator is strongly consistent and asymptotically normal with the same asymptotic variance as the least squares estimator. We show through simulations that the proposed estimator reduces computational time compared with the least squares estimator.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Imaging and Clinical Biomarker Estimation in Alzheimer’s Disease</title>
      <link>https://mcgillstat.github.io/post/2024winter/2024-01-19/</link>
      <pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024winter/2024-01-19/</guid>
      <description>&lt;h4 id=&#34;date-2024-01-19&#34;&gt;Date: 2024-01-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-online-retransmitted-in-burnside-1104&#34;&gt;Location: Online, retransmitted in Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj85422946487httpsmcgillzoomusj85422946487&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/85422946487&#34;&gt;https://mcgill.zoom.us/j/85422946487&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-854-2294-6487&#34;&gt;Meeting ID: 854 2294 6487&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Estimation of biomarkers related to disease classification and modeling of its progression is essential for treatment development for Alzheimer’s Disease (AD). The task is more daunting for characterizing relatively rare AD subtypes such as the early-onset AD. In this talk, I will describe the Longitudinal Alzheimer’s Disease Study (LEADS) intending to collect and publicly distribute clinical, imaging, genetic, and other types of data from people with EOAD, as well as cognitively normal (CN) controls and people with early-onset non-amyloid positive (EOnonAD) dementias. I will discuss manifold estimation methods for estimation of surfaces of shapes in the brain using data clouds, longitudinal manifold learning methods for modeling trajectories of shape changes in the brain over time. Finally, I will discuss our work in leveraging magnetic resonance imaging and positron emission tomography data to characterize distributions of white matter hyperintensities in people with EOAD and to obtain imaging-based biomarkers of disease trajectories of AD subtypes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>New Advances in High-Dimensional DNA Methylation Analysis in Cancer Epigenetic Using Trans-dimensional Hidden Markov Models</title>
      <link>https://mcgillstat.github.io/post/2024winter/2024-01-12/</link>
      <pubDate>Fri, 12 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2024winter/2024-01-12/</guid>
      <description>&lt;h4 id=&#34;date-2024-01-12&#34;&gt;Date: 2024-01-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83008174313httpsmcgillzoomusj83008174313&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83008174313&#34;&gt;https://mcgill.zoom.us/j/83008174313&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-830-0817-4313&#34;&gt;Meeting ID: 830 0817 4313&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Epigenetic alterations are key drivers in the development and progression of cancer. Identifying differentially methylated cytosines (DMCs) in cancer samples is a crucial step toward understanding these changes. In this talk, we propose a trans-dimensional Markov chain Monte Carlo (TMCMC) approach that uses hidden Markov models (HMMs) with binomial emission, and bisulfite sequencing (BS-Seq) data, called DMCTHM, to identify DMCs in cancer epigenetic studies. We introduce the Expander-Collider penalty to tackle under and over-estimation in TMCMC-HMMs. We address all known challenges inherent in BS-Seq data by introducing novel approaches for capturing functional patterns and autocorrelation structure of the data, as well as for handling missing values, multiple covariates, multiple comparisons, and family-wise errors. We demonstrate the effectiveness of DMCTHM through comprehensive simulation studies. The results show that our proposed method outperforms other competing methods in identifying DMCs. Notably, with DMCTHM, we uncovered new DMCs and genes in Colorectal cancer that were significantly enriched in the Tp53 pathway.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Robust and Tuning-Free Sparse Linear Regression via Square-Root Slope</title>
      <link>https://mcgillstat.github.io/post/2023fall/2023-11-17/</link>
      <pubDate>Fri, 17 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023fall/2023-11-17/</guid>
      <description>&lt;h4 id=&#34;date-2023-11-17&#34;&gt;Date: 2023-11-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-online-retransmitted-in-burnside-1104&#34;&gt;Location: Online, retransmitted in Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj81865630475httpsmcgillzoomusj81865630475&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81865630475&#34;&gt;https://mcgill.zoom.us/j/81865630475&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-818-6563-0475&#34;&gt;Meeting ID: 818 6563 0475&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider the high-dimensional linear regression model and assume that a fraction of the responses are contaminated by an adversary with complete knowledge of the data and the underlying distribution. We are interested in the situation when the dense additive noise can be heavy-tailed but the predictors have sub-Gaussian distribution. We establish minimax lower bounds that depend on the fraction of the contaminated data and the tails of the additive noise. Moreover, we design a modification of the square root Slope estimator with several desirable features: (a) it is provably robust to adversarial contamination, with the performance guarantees that take the form of sub-Gaussian deviation inequalities and match the lower error bounds up to log-factors; (b) it is fully adaptive with respect to the unknown sparsity level and the variance of the noise, and (c) it is computationally tractable as a solution of a convex optimization problem. To analyze the performance of the proposed estimator, we prove several properties of matrices with sub-Gaussian rows that could be of independent interest.&#xA;This is joint work with Stanislav Minsker and Lang Wang.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Copula-based estimation of health inequality measures</title>
      <link>https://mcgillstat.github.io/post/2023fall/2023-11-10/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023fall/2023-11-10/</guid>
      <description>&lt;h4 id=&#34;date-2023-11-10&#34;&gt;Date: 2023-11-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj89337793218httpsmcgillzoomusj89337793218&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/89337793218&#34;&gt;https://mcgill.zoom.us/j/89337793218&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-893-3779-3218&#34;&gt;Meeting ID: 893 3779 3218&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This paper aims to use copulas to derive estimators of the health concentration curve and Gini coefficient for health distribution. We highlight the importance of expressing health inequality measures in terms of a copula, which we in turn use to build copula-based semi and nonparametric estimators of the above measures. Thereafter, we study the asymptotic properties of these estimators. In particular, we establish their consistency and asymptotic normality. We provide expressions for their variances, which can be used to construct confidence intervals and build tests for the health concentration curve and Gini health coefficient. A Monte-Carlo simulation exercise shows that the semiparametric estimator outperforms the smoothed nonparametric estimator, and the latter does better than the empirical estimator in terms of Mean Squared Error. We also run an extensive empirical study where we apply our estimators to show that the inequalities across U.S. states&amp;rsquo;s socioeconomic variables like income/poverty and race/ethnicity explain the observed inequalities in COVID-19 infections and deaths in the U.S.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reduced-Rank Envelope Vector Autoregressive Models</title>
      <link>https://mcgillstat.github.io/post/2023fall/2023-11-03/</link>
      <pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023fall/2023-11-03/</guid>
      <description>&lt;h4 id=&#34;date-2023-11-03&#34;&gt;Date: 2023-11-03&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj2571023554httpsmcgillzoomusj2571023554&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/2571023554&#34;&gt;https://mcgill.zoom.us/j/2571023554&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-257-102-3554&#34;&gt;Meeting ID: 257 102 3554&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Classical vector autoregressive (VAR) models have long been a popular choice for modeling multivariate time series data due to their flexibility and ease of use. However,  the VAR model suffers from overparameterization which is a serious issue for high-dimensional time series data as it restricts the number of variables and lags that can be incorporated into the model. Several statistical methods have been proposed to achieve dimension reduction in the parameter space of VAR models. Yet, these methods prove inefficient in extracting relevant information from complex datasets, as they fail to distinguish between information aligned with scientific objectives and are also inefficient in addressing rank deficiency problems. Envelope methods, founded on novel parameterizations that employ reduced subspaces to establish connections between the mean function and covariance matrix, offer a solution by efficiently identifying and eliminating irrelevant information. In this presentation, we introduce a new, parsimonious VAR model that incorporates the concept of envelope models into the reduced-rank VAR framework that can achieve substantial dimension reduction and efficient parameter estimation. We will present the results of simulation studies and real data analysis comparing the performance of our proposed model with that of existing models in the literature.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Doubly Robust Estimation under Covariate-induced Dependent Left Truncation</title>
      <link>https://mcgillstat.github.io/post/2023fall/2023-10-27/</link>
      <pubDate>Fri, 27 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023fall/2023-10-27/</guid>
      <description>&lt;h4 id=&#34;date-2023-10-27&#34;&gt;Date: 2023-10-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-online-retransmitted-in-burnside-1104&#34;&gt;Location: Online, retransmitted in Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj84195498572httpsmcgillzoomusj84195498572&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/84195498572&#34;&gt;https://mcgill.zoom.us/j/84195498572&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-841-9549-8572&#34;&gt;Meeting ID: 841 9549 8572&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In prevalent cohort studies with follow-up, the time-to-event outcome is subject to left truncation leading to selection bias. For estimation of the distribution of time-to-event, conventional methods adjusting for left truncation tend to rely on the (quasi-)independence assumption that the truncation time and the event time are “independent&amp;quot; on the observed region. This assumption is violated when there is dependence between the truncation time and the event time possibly induced by measured covariates. Inverse probability of truncation weighting leveraging covariate information can be used in this case, but it is sensitive to misspecification of the truncation model. In this work, we apply the semiparametric theory to find the efficient influence curve of an expected  (arbitrarily transformed) survival time in the presence of covariate-induced dependent left truncation. We then use it to construct estimators that are shown to enjoy double-robustness properties. Our work represents the first attempt to construct doubly robust estimators in the presence of left truncation, which does not fall under the established framework of coarsened data where doubly robust approaches are developed. We provide technical conditions for the asymptotic properties that appear to not have been carefully examined in the literature for time-to-event data, and study the estimators via extensive simulation. We apply the estimators to two data sets from practice, with different right-censoring patterns.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neural network architectures for functional data analysis</title>
      <link>https://mcgillstat.github.io/post/2023fall/2023-10-20/</link>
      <pubDate>Fri, 20 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023fall/2023-10-20/</guid>
      <description>&lt;h4 id=&#34;date-2023-10-20&#34;&gt;Date: 2023-10-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj89761165882httpsmcgillzoomusj89761165882&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/89761165882&#34;&gt;https://mcgill.zoom.us/j/89761165882&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-897-6116-5882&#34;&gt;Meeting ID: 897 6116 5882&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Functional data is defined as any random variables that assume values in an infinite precision domain, such as time or space. In applications, this data is usually discretely observed at some regularly or irregularly-spaced points over the domain. In this talk, we discuss ways to adapt modern neural network architectures for the analysis of functional data. To do so, we design new neural network layers in order to process functional data either as input, output or both. First, we propose the functional output layer, which can be used to solve a multitude of function-on-scalar regression problems in a non-linear way. The proposed layer provides a smooth representation of the output and we demonstrate how to regularize such a layer during the network training phase. Second, we propose a concept for functional weights that project functional data to a scalar representation, leading to a novel formulation for a functional input layer. We demonstrate how to combine both of these proposed functional layers to create a functional autoencoder. This model takes as input the data in the form it is usually collected, as discrete points over the domain, and can be used for feature extraction and functional data smoothing. We demonstrate the benefits of the proposed architectures with various experiments on simulated data and real data applications. We conclude with a brief discussion of ongoing work in the design of a functional convolution layer that bridges the gap between the discrete convolution operation and its continuous counterpart.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Distances on and between complex networks</title>
      <link>https://mcgillstat.github.io/post/2023fall/2023-10-13/</link>
      <pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023fall/2023-10-13/</guid>
      <description>&lt;h4 id=&#34;date-2023-10-13&#34;&gt;Date: 2023-10-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-online-retransmitted-in-burnside-1104&#34;&gt;Location: Online, retransmitted in Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83477865796httpsmcgillzoomusj83477865796&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83477865796&#34;&gt;https://mcgill.zoom.us/j/83477865796&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-7786-5796&#34;&gt;Meeting ID: 834 7786 5796&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Distance plays a pivotal role in statistics. Meanwhile, recent technologies and social networks have yielded large complex network data sets, which require customized statistical tools. From a mathematical viewpoint, these complex networks are graphs with non-trivial structures (in contrast to Erdös-Rényi graphs, for example). These networks are models of systemic phenomena and cases where individual-level analyses are insufficient. Such models are not only used in the study of social networks, but are also widely employed in neurology, biology, telecommunication and finance, among many areas of application. Unfortunately, however, distances on graphs are not clearly defined.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Doubly robust inference under possibly misspecified marginal structural Cox model</title>
      <link>https://mcgillstat.github.io/post/2023fall/2023-09-29/</link>
      <pubDate>Fri, 29 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023fall/2023-09-29/</guid>
      <description>&lt;h4 id=&#34;date-2023-09-29&#34;&gt;Date: 2023-09-29&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-online-retransmitted-in-burnside-1104&#34;&gt;Location: Online, retransmitted in Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj82440807026httpsmcgillzoomusj82440807026&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/82440807026&#34;&gt;https://mcgill.zoom.us/j/82440807026&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-824-4080-7026&#34;&gt;Meeting ID: 824 4080 7026&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Doubly robust estimation under the marginal structural Cox model has been a challenge until recently due to the non-collapsibility of the Cox regression model. This is because the estimand of causal hazard ratio assumes that the marginal structural Cox model holds, while the doubly robust estimating function requires the specification of an additional model for the conditional distribution of the time-to-event given treatment and covariates, both models unlikely to hold simultaneously. It became possible recently to resolve this issue with the understanding of rate double robustness and machine learning or nonparametric approaches, although technical details are still to be spelt out to ensure root-n inference for the estimand. We describe our work considering both observational studies setting and in the presence of covariate-induced informative censoring. An added benefit of our approach is the interpretation of the estimand when the assumed marginal structural Cox model does not hold, as a time-averaged treatment effect. This allows meaningful estimation of treatment effects for general two-group comparison without the Cox model, or under alternative models such as the semiparametric proportional odds or transformation models for the potential time-to-event outcomes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Detection of Multiple Influential Observations on Variable Selection for High-dimensional Data: New Perspective with an Application to Neurologic Signature of Physical Pain.</title>
      <link>https://mcgillstat.github.io/post/2023fall/2023-09-22/</link>
      <pubDate>Fri, 22 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023fall/2023-09-22/</guid>
      <description>&lt;h4 id=&#34;date-2023-09-22&#34;&gt;Date: 2023-09-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj89374813252httpsmcgillzoomusj89374813252&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/89374813252&#34;&gt;https://mcgill.zoom.us/j/89374813252&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-893-7481-3252&#34;&gt;Meeting ID: 893 7481 3252&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Influential diagnosis is an integral part of data analysis, of which most existing methodological frameworks presume a deterministic submodel and are designed for low-dimensional data (i.e., the number of predictors $p$ smaller than the sample size $n$). However, the stochastic selection of a submodel from high-dimensional data where $p$ exceeds $n$ has become ubiquitous. Thus, methods for identifying observations that could exert undue influence on the choice of a submodel can play an important role in this setting. To date, discussion of this topic has been limited, falling short in two domains: (1) constrained ability to detect multiple influential points, and (2) applicability only in restrictive settings. In this talk, building on a recently proposed measure, we introduce a generalized version accommodating different model selectors, the asymptotic property of which is subsequently examined for large $p$. The $K$-means clustering is incorporated into our scheme to detect multiple influential points. Simulation is then conducted to assess the performances of various diagnostic approaches. The proposed procedure further demonstrates its value in improving predictive power when analyzing thermal-stimulated pain based on fMRI data. In addition, the latest development revolving around this newly proposed measure is also presented. This work is conducted under the joint supervision of Professors Masoud Asgharian and Martin Lindquist.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Three Myths About Causal Mediation</title>
      <link>https://mcgillstat.github.io/post/2023fall/2023-09-15/</link>
      <pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023fall/2023-09-15/</guid>
      <description>&lt;h4 id=&#34;date-2023-09-15&#34;&gt;Date: 2023-09-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1104&#34;&gt;Location: Burnside 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj86404798712httpsmcgillzoomusj86404798712&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/86404798712&#34;&gt;https://mcgill.zoom.us/j/86404798712&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-864-0479-8712&#34;&gt;Meeting ID: 864 0479 8712&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Causal mediation techniques are a means for identifying the degree to which a cause influences its effect along particular causal paths. For example, in a model where a cause influences its effect both indirectly via a mediator and directly via factors not included in the model, mediation techniques enable one to measure both direct and indirect effects. Although mediation techniques are widely employed, they are often misunderstood. This is in part due to the long-term influence of Baron and Kenny’s (1986) treatment of mediation, which applies only to linear models without interaction, and which leads one to develop intuitions about direct and indirect effects that do not generalize to non-parametric causal models. In my talk, I identify and reject three persistent myths about mediation. I argue that such methods: 1. Should not be understood as decomposing the total effect into additive components corresponding to the contributions of the paths; 2. Are not a means for eliminating latent heterogeneity; and 3. Do not require one to appeal to causal concepts other than the counterfactual causal ones built into structural causal models. These points are crucial for understanding mediation effects in any contexts in which they are studied, and have particular applications for studies of fairness and discrimination, in which such effects play an increasingly central role (Plečko and Bareinboim, 2022).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Empirical Bayes Control of the False Discovery Exceedance</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-08-17/</link>
      <pubDate>Thu, 17 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-08-17/</guid>
      <description>&lt;h4 id=&#34;date-2023-08-17&#34;&gt;Date: 2023-08-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;hybrid-in-person--zoom&#34;&gt;Hybrid: In person / Zoom&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-hall-1104&#34;&gt;Location: Burnside Hall 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj89623344755pwds1e0qwvjsm8wrhdiyu5izzllsxnjut09httpsmcgillzoomusj89623344755pwds1e0qwvjsm8wrhdiyu5izzllsxnjut09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/89623344755?pwd=S1E0QWVjSm8wRHdIYU5IZzllSXNjUT09&#34;&gt;https://mcgill.zoom.us/j/89623344755?pwd=S1E0QWVjSm8wRHdIYU5IZzllSXNjUT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-896-2334-4755&#34;&gt;Meeting ID: 896 2334 4755&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-287381&#34;&gt;Passcode: 287381&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In sparse large-scale testing problems where the false discovery proportion (FDP) is highly&#xA;variable, the false discovery exceedance (FDX) provides a valuable alternative to the&#xA;widely used false discovery rate (FDR). We develop an empirical Bayes approach to&#xA;controlling the FDX. We show that for independent hypotheses from a two-group model&#xA;and dependent hypotheses from a Gaussian model fulfilling the exchangeability condition,&#xA;an oracle decision rule based on ranking and thresholding the local false discovery rate&#xA;(lfdr) is optimal in the sense that the power is maximized subject to FDX constraint. We&#xA;propose a data-driven FDX procedure that emulates the oracle via carefully designed&#xA;computational shortcuts. We investigate the empirical performance of the proposed&#xA;method using simulations and illustrate the merits of FDX control through an application&#xA;for identifying abnormal stock trading strategies.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Residual-based estimation of parametric copulas under regression</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-08-14/</link>
      <pubDate>Mon, 14 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-08-14/</guid>
      <description>&lt;h4 id=&#34;date-2023-08-14&#34;&gt;Date: 2023-08-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;hybrid-in-person--zoom&#34;&gt;Hybrid: In person / Zoom&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-hall-1104&#34;&gt;Location: Burnside Hall 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We study a multivariate response regression model where each coordinate is described by a location-scale regression, and where the dependence structure of the &amp;ldquo;noise&amp;rdquo; terms in the regression is described by a parametric copula.  Our goal is to estimate the associated Euclidean copula parameter given a sample of the response and the covariate.  In the absence of the copula sample, the oracle ranks in the usual pseudo-likelihood estimation procedure are no longer computable.  Instead, we base our estimation on the residual ranks calculated from some preliminary estimators of the regression functions.  We show that the residual-based estimators are asymptotically equivalent to their oracle counterparts, even when the dimension of the covariate in the regression is moderately diverging.  Partially to serve this objective, we also study the weighted convergence of the residual empirical processes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Confidence sets for Causal Discovery</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-03-24/</link>
      <pubDate>Fri, 24 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-03-24/</guid>
      <description>&lt;h4 id=&#34;date-2023-03-24&#34;&gt;Date: 2023-03-24&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;on-zoom-only&#34;&gt;On Zoom only&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Causal discovery procedures are popular methods for discovering causal&#xA;structure across the physical, biological, and social sciences.&#xA;However, most procedures for causal discovery only output a single&#xA;estimated causal model or single equivalence class of models. We&#xA;propose a procedure for quantifying uncertainty in causal discovery.&#xA;Specifically, we consider linear structural equation models with&#xA;non-Gaussian errors and propose a procedure which returns a confidence&#xA;sets of causal orderings which are not ruled out by the data. We show&#xA;that asymptotically, the true causal ordering will be contained in the&#xA;returned set with some user specified probability.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Excursions in Statistical History: Highlights</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-03-17/</link>
      <pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-03-17/</guid>
      <description>&lt;h4 id=&#34;date-2023-03-17&#34;&gt;Date: 2023-03-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;hybrid-in-person--zoom&#34;&gt;Hybrid: In person / Zoom&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-hall-1104&#34;&gt;Location: Burnside Hall 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Over the last 20 years, the speaker has delved into  the origins of &amp;lsquo;regression&amp;rsquo;;  the development of the &amp;rsquo;t&amp;rsquo; and &amp;lsquo;Poisson&amp;rsquo; distributions; forerunners of the &amp;lsquo;hazard&amp;rsquo; function; and the statistical design and conduct of US Selective Service lotteries from 1917 onwards.  This talk will recount the stories, data and simulations behind some of these, and provide some modern-day re-enactments.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Heteroskedastic Sparse PCA in High Dimensions</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-03-10-2/</link>
      <pubDate>Fri, 10 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-03-10-2/</guid>
      <description>&lt;h4 id=&#34;date-2023-03-10&#34;&gt;Date: 2023-03-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;hybrid-in-person--zoom&#34;&gt;Hybrid: In person / Zoom&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-hall-1104&#34;&gt;Location: Burnside Hall 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Principal component analysis (PCA) is one of the most commonly used techniques for dimension reduction and feature extraction. Though it has been well-studied for high-dimensional sparse PCA, little is known when the noise is heteroskedastic, which turns out to be ubiquitous in many scenarios, like biological sequencing data and information network data. We propose an iterative algorithm for sparse PCA in the presence of heteroskedastic noise, which alternatively updates the estimates of the sparse eigenvectors using the power method with adaptive thresholding in one step, and imputes the diagonal values of the sample covariance matrix to reduce the estimation bias due to heteroskedasticity in the other step. Our procedure is computationally fast and provably optimal under the generalized spiked covariance model, assuming the leading eigenvectors are sparse. A comprehensive simulation study demonstrates its robustness and effectiveness in various settings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>High Dimensional Logistic Regression Under Network Dependence</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-03-10-1/</link>
      <pubDate>Fri, 10 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-03-10-1/</guid>
      <description>&lt;h4 id=&#34;date-2023-03-10&#34;&gt;Date: 2023-03-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1415-1515-montreal-time&#34;&gt;Time: 14:15-15:15 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;hybrid-in-person--zoom&#34;&gt;Hybrid: In person / Zoom&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-hall-1104&#34;&gt;Location: Burnside Hall 1104&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The classical formulation of logistic regression relies on the independent sampling assumption, which is often violated when the outcomes interact through an underlying network structure, such as over a temporal/spatial domain or on a social network. This necessitates the development of models that can simultaneously handle both the network peer-effect (arising from neighborhood interactions) and the effect of (possibly) high-dimensional covariates. In this talk, I will describe a framework for incorporating such dependencies in a high-dimensional logistic regression model by introducing a quadratic interaction term, as in the Ising model, designed to capture the pairwise interactions from the underlying network. The resulting model can also be viewed as an Ising model, where the node-dependent external fields linearly encode the high-dimensional covariates. We use a penalized maximum pseudo-likelihood method for estimating the network peer-effect and the effect of the covariates (the regression coefficients), which, in addition to handling the high-dimensionality of the parameters, conveniently avoids the computational intractability of the maximum likelihood approach. Our results imply that even under network dependence it is possible to consistently estimate the model parameters at the same rate as in classical (independent) logistic regression, when the true parameter is sparse and the underlying network is not too dense. Towards the end, I will talk about the rates of consistency of our proposed estimator for various natural graph ensembles, such as bounded degree graphs, sparse Erdos-Renyi random graphs, and stochastic block models, which follow as a consequence of our general results. This is a joint work with Ziang Niu, Sagnik Halder, Bhaswar Bhattacharya and George&#xA;Michailidis.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Epidemic Forecasting using Delayed Time Embedding</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-02-17/</link>
      <pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-02-17/</guid>
      <description>&lt;h4 id=&#34;date-2023-02-17&#34;&gt;Date: 2023-02-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Forecasting the future trajectory of an outbreak plays a crucial role in the mission of managing emerging infectious disease epidemics. Compartmental models, such as the Susceptible-Exposed-Infectious-Recovered (SEIR), are the most popular tools for this task. They have been used extensively to combat many infectious disease outbreaks including the current COVID-19 pandemic. One downside of these models is that they assume that the dynamics of an epidemic follow a pre-defined dynamical system which may not capture the true trajectories of an outbreak. Consequently, the users need to make several modifications throughout an epidemic to ensure their models fit well with the data. However, there is no guarantee that these modifications can also help increase the precision of forecasting. In this talk, I will introduce a new method for predicting epidemics that does not make any assumption on the underlying dynamical system. Our method combines sparse random feature expansion and delay embedding to learn the trajectory of an epidemic.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Efficient Label Shift Adaptation through the Lens of Semiparametric Models</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-02-10/</link>
      <pubDate>Fri, 10 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-02-10/</guid>
      <description>&lt;h4 id=&#34;date-2023-02-10&#34;&gt;Date: 2023-02-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1500-1600-montreal-time&#34;&gt;Time: 15:00-16:00 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;hybrid-in-person--zoom&#34;&gt;Hybrid: In person / Zoom&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-hall-1205&#34;&gt;Location: Burnside Hall 1205&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We study the domain adaptation problem with label shift in this work. Under the label shift context, the marginal distribution of the label varies across the training and testing datasets, while the conditional distribution of features given the label is the same. Traditional label shift adaptation methods either suffer from large estimation errors or require cumbersome post-prediction calibrations. To address these issues, we first propose a moment-matching framework for adapting the label shift based on the geometry of the influence function. Under such a framework, we propose a novel method named efficient label shift adaptation (ELSA), in which the adaptation weights can be estimated by solving linear systems. Theoretically, the ELSA estimator is root-n consistent (n is the sample size of the source data) and asymptotically normal. Empirically, we show that ELSA can achieve state-of-the-art estimation performances without post-prediction calibrations, thus, gaining computational efficiency.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning from a Biased Sample</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-02-03/</link>
      <pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-02-03/</guid>
      <description>&lt;h4 id=&#34;date-2023-02-03&#34;&gt;Date: 2023-02-03&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The empirical risk minimization approach to data-driven decision making assumes that we can learn a decision rule from training data drawn under the same conditions as the ones we want to deploy it under. However, in a number of settings, we may be concerned that our training sample is biased, and that some groups (characterized by either observable or unobservable attributes) may be under- or over-represented relative to the general population; and in this setting empirical risk minimization over the training set may fail to yield rules that perform well at deployment. Building on concepts from distributionally robust optimization and sensitivity analysis, we propose a method for learning a decision rule that minimizes the worst-case risk incurred under a family of test distributions whose conditional distributions of outcomes  given covariates  differ from the conditional training distribution by at most a constant factor, and whose covariate distributions are absolutely continuous with respect to the covariate distribution of the training data. We apply a result of Rockafellar and Uryasev to show that this problem is equivalent to an augmented convex risk minimization problem. We give statistical guarantees for learning a robust model using the method of sieves and propose a deep learning algorithm whose loss function captures our robustness target. We empirically validate our proposed method in simulations and a case study with the MIMIC-III dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is TWAS and how do we use it in integrating gene expression data</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-01-20/</link>
      <pubDate>Fri, 20 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-01-20/</guid>
      <description>&lt;h4 id=&#34;date-2023-01-20&#34;&gt;Date: 2023-01-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The transcriptome-wide association studies (TWAS) is a pioneering approach utilizing gene expression data to identify genetic basis of complex diseases. Its core component is called &amp;ldquo;genetically regulated expression (GReX)&amp;rdquo;. GReX links gene expression information with phenotype by serving as both the outcome of genotype-based expression models and the predictor for downstream association testing. Although it is popular and has been used in many high-profile projects, its mathematical nature and interpretation haven’t been rigorously verified. As such, we have first conducted power analysis using NCP-based closed forms (Cao et al, PLoS Genet 2021), based on which we realized that the common interpretation of TWAS that looks biologically sensible is actually mathematically questionable. Following this insight, by real data analysis and simulations, we demonstrated that current linear models of GReX inadvertently combine two separable steps of machine learning - feature selection and aggregation - which can be independently replaced to improve overall power (Cao et al, Genetics 2021). Based on this new interpretation, we have developed novel protocols disentangling feature selections and aggregations, leading to improved power and novel biological discoveries (Cao et al, BiB 2021; Genetics 2021). To promote this new understanding, we moved forward to develop two statistical tools utilizing gene expressions in identifying genetic basis of gene-gene interactions (Kossinna et al, in preparation) and low-effect genetic variants (Li et al, in review), respectively. Looking forward, our mathematical characterization of TWAS opens a door for a new way to integrate gene expressions in genetic studies towards the realization of precision medicine.&lt;/p&gt;</description>
    </item>
    <item>
      <title>To split or not to split that is the question: From cross validation to debiased machine learning</title>
      <link>https://mcgillstat.github.io/post/2023winter/2023-01-13/</link>
      <pubDate>Fri, 13 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2023winter/2023-01-13/</guid>
      <description>&lt;h4 id=&#34;date-2023-01-13&#34;&gt;Date: 2023-01-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Data splitting is an ubiquitous method in statistics with examples ranging from cross validation to cross-fitting. However, despite its prevalence, theoretical guidance regarding its use is still lacking. In this talk we will explore two examples and establish an asymptotic theory for it. In the first part of this talk, we study the cross-validation method, a ubiquitous method for risk estimation, and establish its asymptotic properties for a large class of models and with an arbitrary number of folds. Under stability conditions, we establish a central limit theorem and Berry-Esseen bounds for the cross-validated risk, which enable us to compute asymptotically accurate confidence intervals. Using our results, we study the statistical speed-up offered by cross validation compared to a train-test split procedure. We reveal some surprising behavior of the cross-validated risk and establish the statistically optimal choice for the number of folds. In the second part of this talk, we study the role of cross fitting in the generalized method of moments with moments that also depend on some auxiliary functions. Recent lines of work show how one can use generic machine learning estimators for these auxiliary problems, while maintaining asymptotic normality and root-n consistency of the target parameter of interest. The literature typically requires that these auxiliary problems are fitted on a separate sample or in a cross-fitting manner. We show that when these auxiliary estimation algorithms satisfy natural leave-one-out stability properties, then sample splitting is not required. This allows for sample re-use, which can be beneficial in moderately sized sample regimes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optimal One-pass Nonparametric Estimation Under Memory Constraint</title>
      <link>https://mcgillstat.github.io/post/2022fall/2022-11-18/</link>
      <pubDate>Fri, 18 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022fall/2022-11-18/</guid>
      <description>&lt;h4 id=&#34;date-2022-11-18&#34;&gt;Date: 2022-11-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;For nonparametric regression in the streaming setting, where data constantly flow in and require real-time analysis, a main challenge is that data are cleared from the computer system once processed due to limited computer memory and storage. We tackle the challenge by proposing a novel one-pass estimator based on penalized orthogonal basis expansions and developing a general framework to study the interplay between statistical efficiency and memory consumption of estimators. We show that, the proposed estimator is statistically optimal under memory constraint, and has asymptotically minimal memory footprints among all one-pass estimators of the same estimation quality. Numerical studies demonstrate that the proposed one-pass estimator is nearly as efficient as its non-streaming counterpart that has access to all historical data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automated Inference on Sharp Bounds</title>
      <link>https://mcgillstat.github.io/post/2022fall/2022-11-11/</link>
      <pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022fall/2022-11-11/</guid>
      <description>&lt;h4 id=&#34;date-2022-11-11&#34;&gt;Date: 2022-11-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Many causal parameters involving the joint distribution of potential outcomes in treated and control states cannot be point-identified, but only be bounded from above and below. The bounds can be further tightened by conditioning on pre-treatment covariates, and the sharp version of the bounds corresponds to using a full covariate vector. This paper gives a method for estimation and inference on sharp bounds determined by a linear system of under-identified equalities (e.g., as in Heckman et al (ReSTUD, 1997)).  In the sharp bounds’ case, the RHS of this system involves a nuisance function of (many) covariates (e.g., the conditional probability of employment in treated or control state). Combining Neyman-orthogonality and sample splitting, I provide an asymptotically Gaussian estimator of sharp bound that does not require solving the linear system in closed form. I demonstrate the method in an empirical application to Connecticut’s Jobs First welfare reform experiment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Max-linear Graphical Models for Extreme Risk Modelling</title>
      <link>https://mcgillstat.github.io/post/2022fall/2022-11-04/</link>
      <pubDate>Fri, 04 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022fall/2022-11-04/</guid>
      <description>&lt;h4 id=&#34;date-2022-11-04&#34;&gt;Date: 2022-11-04&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Graphical models can represent multivariate distributions in an intuitive way and, hence, facilitate statistical analysis of high-dimensional data. Such models are usually modular so that high-dimensional distributions can be described and handled by careful combination of lower dimensional factors. Furthermore, graphs are natural data structures for algorithmic treatment. Moreover, graphical models can allow for causal interpretation, often provided through a recursive system on a directed acyclic graph (DAG) and the max-linear Bayesian network we introduced in [1] is a specific example. This talk contributes to the recently emerged topic of graphical models for extremes, in particular to max-linear Bayesian networks, which are max-linear graphical models on DAGs. Generalized MLEs are derived in [2].&#xA;In this context, the Latent River Problem has emerged as a flagship problem for causal discovery in extreme value statistics. In [3] we provide a simple and efficient algorithm QTree to solve the Latent River Problem. QTree returns a directed graph and achieves almost perfect recovery on the Upper Danube, the existing benchmark dataset, as well as on new data from the Lower Colorado River in Texas. It can handle missing data, and has an automated parameter tuning procedure. In our paper, we also show that, under a max-linear Bayesian network model for extreme values with propagating noise, the QTree algorithm returns asymptotically a.s. the correct tree. Here we use the fact that the non-noisy model has a left-sided atom for every bivariate marginal distribution, when there is a directed edge between the the nodes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Conformal-Based Two-Sample Conditional Distribution Test</title>
      <link>https://mcgillstat.github.io/post/2022fall/2022-10-21/</link>
      <pubDate>Fri, 21 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022fall/2022-10-21/</guid>
      <description>&lt;h4 id=&#34;date-2022-10-21&#34;&gt;Date: 2022-10-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider the problem of testing the equality of the conditional distribution of a response variable given a set of covariates between two populations.  Such a testing problem is related to transfer learning and causal inference.  We develop a nonparametric procedure by combining recent advances in conformal prediction with some new ingredients such as a novel choice of conformity score and data-driven choices of weight and score functions. To our knowledge, this is the first successful attempt of using conformal prediction for testing statistical hypotheses beyond exchangeability. The final test statistic reveals a natural connection between conformal inference and the classical rank-sum test.  Our method is suitable for modern machine learning scenarios where the data has high dimensionality and the sample size is large, and can be effectively combined with existing classification algorithms to find good weight and score functions.  The performance of the proposed method is demonstrated in synthetic and real data examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some steps towards causal representation learning</title>
      <link>https://mcgillstat.github.io/post/2022fall/2022-10-07/</link>
      <pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022fall/2022-10-07/</guid>
      <description>&lt;h4 id=&#34;date-2022-10-07&#34;&gt;Date: 2022-10-07&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;High-dimensional unstructured data such images or sensor data can often be collected cheaply in experiments, but is challenging to use in a causal inference pipeline without extensive engineering and domain knowledge to extract underlying latent factors. The long term goal of causal representation learning is to find appropriate assumptions and methods to disentangle latent variables and learn the causal mechanisms that explain a system&amp;rsquo;s behaviour. In this talk, I&amp;rsquo;ll present results from a series of recent papers that describe how we can leverage assumptions about a system&amp;rsquo;s causal mechanisms to provably disentangle latent factors. I will also talk about the limitations of a commonly used injectivity assumption, and discuss a hierarchy of settings that relax this assumption.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical Inference for Functional Linear Quantile Regression</title>
      <link>https://mcgillstat.github.io/post/2022fall/2022-09-16/</link>
      <pubDate>Fri, 16 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022fall/2022-09-16/</guid>
      <description>&lt;h4 id=&#34;date-2022-09-16&#34;&gt;Date: 2022-09-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1520-1620-montreal-time&#34;&gt;Time: 15:20-16:20 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We propose inferential tools for functional linear quantile regression where the conditional quantile of a scalar response is assumed to be a linear functional of a functional covariate. In contrast to conventional approaches, we employ kernel convolution to smooth the original loss function. The coefficient function is estimated under a reproducing kernel Hilbert space framework. A gradient descent algorithm is designed to minimize the smoothed loss function with a roughness penalty. With the aid of the Banach fixed-point theorem, we show the existence and uniqueness of our proposed estimator as the minimizer of the regularized loss function in an appropriate Hilbert space. Furthermore, we establish the convergence rate as well as the weak convergence of our estimator. As far as we know, this is the first weak convergence result for a functional quantile regression model. Pointwise confidence intervals and a simultaneous confidence band for the true coefficient function are then developed based on these theoretical properties. Numerical studies including both simulations and a data application are conducted to investigate the performance of our estimator and inference tools in finite sample.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Markov-Switching State Space Models For Uncovering Musical Interpretation</title>
      <link>https://mcgillstat.github.io/post/2022fall/2022-09-09/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022fall/2022-09-09/</guid>
      <description>&lt;h4 id=&#34;date-2022-09-09&#34;&gt;Date: 2022-09-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;For concertgoers, musical interpretation is the most important factor in determining whether or not we enjoy a classical performance. Every performance includes mistakes—intonation issues, a lost note, an unpleasant sound—but these are all easily forgotten (or unnoticed) when a performer engages her audience, imbuing a piece with novel emotional content beyond the vague instructions inscribed on the printed page. In this research, we use data from the CHARM Mazurka Project—forty-six professional recordings of Chopin’s Mazurka Op. 68 No. 3 by consummate artists—with the goal of elucidating musically interpretable performance decisions. We focus specifically on each performer’s use of musical tempo by examining the inter-onset intervals of the note attacks in the recording. To explain these tempo decisions, we develop a switching state space model and estimate it by maximum likelihood combined with prior information gained from music theory and performance practice. We use the estimated parameters to quantitatively describe individual performance decisions and compare recordings. These comparisons suggest methods for informing music instruction, discovering listening preferences, and analyzing performances.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Enriched post-selection models for high dimensional data</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-04-08/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022winter/2022-04-08/</guid>
      <description>&lt;h4 id=&#34;date-2022-04-08&#34;&gt;Date: 2022-04-08&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1535-1635-montreal-time&#34;&gt;Time: 15:35-16:35 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;High dimensional data are rapidly growing in many domains, for example, in microarray gene expression studies, fMRI data analysis, large-scale healthcare analytics, text/image analysis, natural language processing and astronomy, to name but a few. In the last two decades regularisation approaches have become the methods of choice for analysing high dimensional data. However, obtaining accurate estimates and predictions as well as valid statistical inference remains a major challenge in high dimensional situations. In this talk, we present enriched post-selection models that aim to improve parameter estimation and prediction, and to facilitate statistical inferences in high dimensional regression models. The enriched post-selection method enables us to construct valid post-selection inference for regression parameters in high dimensions. We discuss the empirical and asymptotic properties of the enriched post-selection method.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-04-01/</link>
      <pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022winter/2022-04-01/</guid>
      <description>&lt;h4 id=&#34;date-2022-04-01&#34;&gt;Date: 2022-04-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1535-1635-montreal-time&#34;&gt;Time: 15:35-16:35 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We introduce Learn then Test, a framework for calibrating machine learning models so that their predictions satisfy explicit, finite-sample statistical guarantees regardless of the underlying model and (unknown) data-generating distribution. The framework addresses, among other examples, false discovery rate control in multi-label classification, intersection-over-union control in instance segmentation, and the simultaneous control of the type-1 error of outlier detection and confidence set coverage in classification or regression. To accomplish this, we solve a key technical challenge: the control of arbitrary risks that are not necessarily monotonic. Our main insight is to reframe the risk-control problem as multiple hypothesis testing, enabling techniques and mathematical arguments different from those in the previous literature. We use our framework to provide new calibration methods for several core machine learning tasks with detailed worked examples in computer vision.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Distribution-​free inference for regression: discrete, continuous, and in between</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-03-25/</link>
      <pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022winter/2022-03-25/</guid>
      <description>&lt;h4 id=&#34;date-2022-03-25&#34;&gt;Date: 2022-03-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1535-1635-montreal-time&#34;&gt;Time: 15:35-16:35 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In data analysis problems where we are not able to rely on distributional assumptions, what types of inference guarantees can still be obtained?  Many popular methods, such as holdout methods, cross-validation methods, and conformal prediction, are able to provide distribution-free guarantees for predictive inference, but the problem of providing inference for the underlying regression function (for example, inference on the conditional mean E[Y|X]) is more challenging. If X takes only a small number of possible values, then inference on E[Y|X] is trivial to achieve. At the other extreme, if the features X are continuously distributed, we show that any confidence interval for E[Y|X] must have non-vanishing width, even as sample size tends to infinity - this is true regardless of smoothness properties or other desirable features of the underlying distribution. In between these two extremes, we find several distinct regimes - in particular, it is possible for distribution-free confidence intervals to have vanishing width if and only if the effective support size of the distribution ofXis smaller than the square of the sample size.&lt;/p&gt;</description>
    </item>
    <item>
      <title>New Approaches for Inference on Optimal Treatment Regimes</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-03-11/</link>
      <pubDate>Fri, 11 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022winter/2022-03-11/</guid>
      <description>&lt;h4 id=&#34;date-2022-03-11&#34;&gt;Date: 2022-03-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Finding the optimal treatment regime (or a series of sequential treatment regimes) based on individual characteristics has important applications in precision medicine. We propose two new approaches to quantify uncertainty in optimal treatment regime estimation. First, we consider inference in the model-free setting, which does not require specifying an outcome regression model. Existing model-free estimators for optimal treatment regimes are usually not suitable for the purpose of inference, because they either have nonstandard asymptotic distributions or do not necessarily guarantee consistent estimation of the parameter indexing the Bayes rule due to the use of surrogate loss. We study a smoothed robust estimator that directly targets the parameter corresponding to the Bayes decision rule for optimal treatment regimes estimation. We verify that a resampling procedure provides asymptotically accurate inference for both the parameter indexing the optimal treatment regime and the optimal value function. Next, we consider the high-dimensional setting and propose a semiparametric model-assisted approach for simultaneous inference. Simulation results and real data examples are used for illustration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integration of multi-omics data for the discovery of novel regulators that modulate biological processes</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-02-11/</link>
      <pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022winter/2022-02-11/</guid>
      <description>&lt;h4 id=&#34;date-2022-02-11&#34;&gt;Date: 2022-02-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The cellular states in various biological processes such as cell differentiation, disease progression, and treatment response are often enormously complex and thus hard to be profiled with unimodal profiling (e.g., transcriptome). Although those unimodal measurements had brought success for studies in a large variety of studies,  the incomplete (and often misleading) unimodal cellular profiling could lead to     &lt;br&gt;&#xA;biased and inaccurate conclusions. With the development of biotechnologies,  the availability of multi-omics data (bulk or single-cell) is ever-increasing. The rapid-accumulating multi-omics data offers unprecedented opportunities to accurately decode the cellular states in biological process and thus could derive a deep understanding of the change of the cellular states, crucial for finding biomarkers and therapeutic intervention strategies. In this talk, we will discuss a few multimodal methods that we developed to integrate multi-omics data for the discovery of novel regulators for multiple biological processes. Many of the novel predictions from the multimodal methods were experimentally validated and had brought new understandings of the underlying mechanisms for several diseases. I will also discuss how a potential novel COVID19 drug is discovered from such a multi-omics data integration analysis.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Off-Policy Confidence Interval Estimation with Confounded Markov Decision Process</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-02-04/</link>
      <pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022winter/2022-02-04/</guid>
      <description>&lt;h4 id=&#34;date-2022-02-04&#34;&gt;Date: 2022-02-04&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, we consider constructing a confidence interval for a&#xA;target policy’s value offline based on pre-collected observational&#xA;data in infinite horizon settings. Most of the existing works assume&#xA;no unmeasured variables exist that confound the observed actions. This&#xA;assumption, however, is likely to be violated in real applications&#xA;such as healthcare and technological industries. We show that with&#xA;some auxiliary variables that mediate the effect of actions on the&#xA;system dynamics, the target policy’s value is identifiable in a&#xA;confounded Markov decision process. Based on this result, we develop&#xA;an efficient off-policy value estimator that is robust to potential&#xA;model misspecification and provides rigorous uncertainty&#xA;quantification.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Change-point analysis for complex data structures</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-01-21/</link>
      <pubDate>Fri, 21 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022winter/2022-01-21/</guid>
      <description>&lt;h4 id=&#34;date-2022-01-21&#34;&gt;Date: 2022-01-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The change-point analysis is more than sixty years old. Over this long period, it has been an important subject of interest in many scientific disciplines such as finance and econometrics, bioinformatics and genomics, climatology, engineering, and technology.&lt;/p&gt;&#xA;&lt;p&gt;In this talk, I will provide a general overview of the topic alongside some historical notes. I will then review the most recent and transformative advancements on the subject. Finally, I will discuss the change-point methodologies that my research team has developed over the past several years, covering various complex data structures.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prediction of Bundled Insurance Risks with Dependence-aware Prediction using Pair Copula Construction</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-11-19/</link>
      <pubDate>Fri, 19 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-11-19/</guid>
      <description>&lt;h4 id=&#34;date-2021-11-19&#34;&gt;Date: 2021-11-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We propose a dependence-aware predictive modeling framework for multivariate risks stemmed from an insurance contract with bundling features &amp;ndash; an important type of policy increasingly offered by major insurance companies. The bundling feature naturally leads to longitudinal measurements of multiple insurance risks. We build a novel predictive model that actively exploits the dependence among the evolution of multivariate repeated risk measurements. Specifically, the longitudinal measurement of each individual risk is first modeled using pair copula construction with a D-vine structure, and the multiple D-vines are then integrated by a flexible copula. While our analysis mainly focuses on the claim count as the measurement of insurance risk, the proposed model indeed provides a unified modeling framework that can accommodate different scales of measurements, including continuous, discrete, and mixed observations. A computationally efficient sequential method is proposed for model estimation and inference, and its performance is investigated both theoretically and via simulation studies. In the application, we examine multivariate bundled risks in multi-peril property insurance using the proprietary data obtained from a commercial property insurance provider. The proposed predictive model is found to provide improved decision making for several key insurance operations, including risk segmentation and risk management. In the underwriting operation, we show that the experience rate priced by the proposed model leads to a 9% lift in the insurer&amp;rsquo;s profit. In the reinsurance operation, we show that the insurer underestimates the risk of the retained insurance portfolio by 10% when ignoring the dependence among bundled insurance risks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Bayes for high-dimensional linear regression with sparse priors</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-11-12/</link>
      <pubDate>Fri, 12 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-11-12/</guid>
      <description>&lt;h4 id=&#34;date-2021-11-12&#34;&gt;Date: 2021-11-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A core problem in Bayesian statistics is approximating difficult to compute posterior distributions. In variational Bayes (VB), a method from machine learning, one approximates the posterior through optimization, which is typically faster than Markov chain Monte Carlo. We study a mean-field (i.e. factorizable) VB approximation to Bayesian model selection priors, including the popular spike-and-slab prior, in sparse high-dimensional linear regression. We establish convergence rates for this VB approach, studying conditions under which it provides good estimation. We also discuss some computational issues and study the empirical performance of the algorithm.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model-assisted analyses of cluster-randomized experiments</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-10-22/</link>
      <pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-10-22/</guid>
      <description>&lt;h4 id=&#34;date-2021-10-22&#34;&gt;Date: 2021-10-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Cluster-randomized experiments are widely used due to their logistical convenience and policy relevance. To analyze them properly, we must address the fact that the treatment is assigned at the cluster level instead of the individual level. Standard analytic strategies are regressions based on individual data, cluster averages, and cluster totals, which differ when the cluster sizes vary. These methods are often motivated by models with strong and unverifiable assumptions, and the choice among them can be subjective. Without any outcome modeling assumption, we evaluate these regression estimators and the associated robust standard errors from a design-based perspective where only the treatment assignment itself is random and controlled by the experimenter. We demonstrate that regression based on cluster averages targets a weighted average treatment effect, regression based on individual data is suboptimal in terms of efficiency, and regression based on cluster totals is consistent and more efficient with a large number of clusters. We highlight the critical role of covariates in improving estimation efficiency, and illustrate the efficiency gain via both simulation studies and data analysis. Moreover, we show that the robust standard errors are convenient approximations to the true asymptotic standard errors under the design-based perspective. Our theory holds even when the outcome models are misspecified, so it is model-assisted rather than model-based. We also extend the theory to a wider class of weighted average treatment effects.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Imbalanced learning using actuarial modified loss function in tree-based models</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-10-08/</link>
      <pubDate>Fri, 08 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-10-08/</guid>
      <description>&lt;h4 id=&#34;date-2021-10-08&#34;&gt;Date: 2021-10-08&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Tree-based models have gained momentum in insurance claim loss modeling; however, the point mass at zero and the heavy tail of insurance loss distribution pose the challenge to apply conventional methods directly to claim loss modeling. With a simple illustrative dataset, we first demonstrate how the traditional tree-based algorithm&amp;rsquo;s splitting function fails to cope with a large proportion of data with zero responses. To address the imbalance issue presented in such loss modeling, this paper aims to modify the traditional splitting function of Classification and Regression Tree (CART). In particular, we propose two novel actuarial modified loss functions, namely, the weighted sum of squared error and the sum of squared Canberra error. These modified loss functions impose a significant penalty on grouping observations of non-zero response with those of zero response at the splitting procedure, and thus significantly enhance their separation. Finally, we examine and compare the predictive performance of such actuarial modified tree-based models to the traditional model on synthetic datasets that imitate insurance loss. The results show that such modification leads to substantially different tree structures and improved prediction performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The HulC: Hull based Confidence Regions</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-10-01/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-10-01/</guid>
      <description>&lt;h4 id=&#34;date-2021-10-01&#34;&gt;Date: 2021-10-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We develop and analyze the HulC, an intuitive and general method for constructing confidence sets using the convex hull of estimates constructed from subsets of the data. Unlike classical methods which are based on estimating the (limiting) distribution of an estimator, the HulC is often simpler to use and effectively bypasses this step. In comparison to the bootstrap, the HulC requires fewer regularity conditions and succeeds in many examples where the bootstrap provably fails. Unlike subsampling, the HulC does not require knowledge of the rate of convergence of the estimators on which it is based. The validity of the HulC requires knowledge of the (asymptotic) median-bias of the estimators. We further analyze a variant of our basic method, called the Adaptive HulC, which is fully data-driven and estimates the median-bias using subsampling. We show that the Adaptive HulC retains the aforementioned strengths of the HulC. In certain cases where the underlying estimators are pathologically asymmetric, the HulC and Adaptive HulC can fail to provide useful confidence sets. We discuss these methods in the context of several challenging inferential problems which arise in parametric, semi-parametric, and non-parametric inference. Although our focus is on validity under weak regularity conditions, we also provide some general results on the width of the HulC confidence sets, showing that in many cases the HulC confidence sets have near-optimal width.&#xA;Please let me know if you need anything else.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On the Minimal Error of Empirical Risk Minimization</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-09-17/</link>
      <pubDate>Fri, 17 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-09-17/</guid>
      <description>&lt;h4 id=&#34;date-2021-09-17&#34;&gt;Date: 2021-09-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In recent years, highly expressive machine learning models, i.e. models that can express rich classes of functions, are becoming more and&#xA;more commonly used due their success both in regression and classification tasks, such models are deep neural nets, kernel machines and more.&#xA;From the classical theory statistics point of view (the minimax theory),&#xA;rich models tend to have a higher minimax rate, i.e. any estimator must&#xA;have a high risk (a “worst case scenario” error). Therefore, it seems that&#xA;for modern models the classical theory may be too conservative and strict.&#xA;In this talk, we consider the most popular procedure for regression&#xA;task, that is Empirical Risk Minimization with squared loss (ERM) and&#xA;we shall analyze its minimal squared error both in the random and the&#xA;fixed design settings, under the assumption of a convex family of functions.&#xA;Namely, the minimal squared error that the ERM attains on estimating&#xA;any function in our class in both settings.&#xA;In the fixed design setting, we show that the error is governed by&#xA;the global complexity of the entire class. In contrast, in random design,&#xA;the ERM may only adapt to simpler models if the local neighborhoods&#xA;around the regression function are nearly as complex as the class itself,&#xA;a somewhat counter-intuitive conclusion. We provide sharp lower bounds&#xA;for performance of ERM for both Donsker and non-Donsker classes. This&#xA;talk is based on joint work with Alexander Rakhlin.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Weighted empirical processes</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-09-10/</link>
      <pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-09-10/</guid>
      <description>&lt;h4 id=&#34;date-2021-09-10&#34;&gt;Date: 2021-09-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Empirical processes concern the uniform behavior of averaged sums over a sample of observations where the sums are indexed by a class of functions.  Classical empirical processes typically study the empirical distribution function over the real line, while more modern empirical processes study much more general indexing function classes (e.g., Vapnik-Chervonenkis class, smoothness class); typical results include moment bounds and deviation inequalities.  In this talk we will survey some of these results, but for the weighted empirical process that is obtained by weighing the original process by a factor related to the standard deviation of the process, which will make the resulting process more difficult to bound.  Applications to multivaraite rank order statistics and residual empirical processes will be discussed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dependence Modeling of Mixed Insurance Claim Data</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-04-09/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-04-09/</guid>
      <description>&lt;h4 id=&#34;date-2021-04-09&#34;&gt;Date: 2021-04-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj84308655572pwdoulcn2fuckfmetrrsgnjmzvzuzkrzz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/84308655572?pwd=OUlCN2FUckFmeTRRSGNjMzVzUzkrZz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-843-0865-5572&#34;&gt;Meeting ID: 843 0865 5572&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Multivariate claim data are common in insurance applications, e.g. claims of each policyholder for different types of insurance coverages. Understanding the dependencies among such multivariate risks is essential for the solvency and profitability of insurers. Effectively modeling insurance claim data is challenging due to their special complexities. At the policyholder level, claims data usually follow a two-part mixed distribution: a probability mass at zero corresponding to no claim and an otherwise positive claim from a skewed and long-tailed distribution. Copula models are often employed in order to simultaneously model the relationship between outcomes and covariates while flexibly quantifying the dependencies among the different outcomes. However, due to the mixed data feature, specification of copula models has been a problem. We fill this gap by developing a consistent nonparametric copula estimator for mixed data. Under our framework, both the models for the i) marginal relationship between covariates and claims and ii) dependence structure between claims can be chosen in a principled way. We show the uniform convergence of the proposed nonparametric copula estimator. Using the claim data from the Wisconsin Local Government Property Insurance Fund, we illustrate that our nonparametric copula estimator can assist analysts in identifying important features of the underlying dependence structure, revealing how different claims or risks are related to one another.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning Causal Structures via Continuous Optimization</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-03-26/</link>
      <pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-03-26/</guid>
      <description>&lt;h4 id=&#34;date-2021-03-26&#34;&gt;Date: 2021-03-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj84308655572pwdoulcn2fuckfmetrrsgnjmzvzuzkrzz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/84308655572?pwd=OUlCN2FUckFmeTRRSGNjMzVzUzkrZz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-843-0865-5572&#34;&gt;Meeting ID: 843 0865 5572&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;There has been a recent surge of interest in the machine learning community in developing &lt;em&gt;causal models&lt;/em&gt; that handle the effect of &lt;em&gt;interventions&lt;/em&gt; in a system. In this talk, I will consider the problem of learning (estimating) a causal graphical model from data. The search over possible directed acyclic graphs modeling the causal structure is inherently combinatorial, but I&amp;rsquo;ll describe our recent work which use gradient-based continuous optimization for learning both the parameters of the distribution and the causal graph jointly, and can be combined naturally with flexible parametric families that use neural networks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Measuring timeliness of annual reports filing by jump additive models</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-03-19/</link>
      <pubDate>Fri, 19 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-03-19/</guid>
      <description>&lt;h4 id=&#34;date-2021-03-19&#34;&gt;Date: 2021-03-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj84308655572pwdoulcn2fuckfmetrrsgnjmzvzuzkrzz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/84308655572?pwd=OUlCN2FUckFmeTRRSGNjMzVzUzkrZz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-843-0865-5572&#34;&gt;Meeting ID: 843 0865 5572&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Foreign public issuers (FPIs) are required by the Securities and Exchanges Commission (SEC) to file Form 20-F as comprehensive annual reports. In an effort to increase the usefulness of 20-Fs, the SEC recently enacted a regulation to accelerate the deadline of 20-F filing from six months to four months after the fiscal year-end. The rationale is that the shortened reporting lag would improve the informational relevance of 20-Fs. In this work we propose a jump additive model to evaluate the SEC’s rationale by investigating the relationship between the timeliness of 20-F filing and its decision usefulness using the market data. The proposed model extends the conventional additive models to allow possible discontinuities in the regression functions. We suggest a two-step jump-preserving estimation procedure and show that it is statistically consistent. By applying the procedure to the 20-F study, we find a moderate positive association between the magnitude of the market reaction and the filing timeliness when the acceleration is less than 17 days. We also find that the market considers the filings significantly more informative when the acceleration is more than 18 days and such reaction tapers off when the acceleration exceeds 40 days.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CoinPress: Practical Private Point Estimation and Confidence Intervals</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-02-26/</link>
      <pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-02-26/</guid>
      <description>&lt;h4 id=&#34;date-2021-02-26&#34;&gt;Date: 2021-02-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj84308655572pwdoulcn2fuckfmetrrsgnjmzvzuzkrzz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/84308655572?pwd=OUlCN2FUckFmeTRRSGNjMzVzUzkrZz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-843-0865-5572&#34;&gt;Meeting ID: 843 0865 5572&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider point estimation and generation of confidence intervals under the constraint of differential privacy. We provide a simple and practical framework for these tasks in relatively general settings. Our investigation addresses a novel challenge that arises in the differentially private setting, which involves the cost of weak a priori bounds on the parameters of interest. This framework is applied to the problems of Gaussian mean and covariance estimation. Despite the simplicity of our method, we are able to achieve minimax near-optimal rates for these problems. Empirical evaluations, on the problems of mean estimation, covariance estimation, and principal component analysis, demonstrate significant improvements in comparison to previous work.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Joint integrative analysis of multiple data sources with correlated vector outcomes</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-02-19/</link>
      <pubDate>Fri, 19 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-02-19/</guid>
      <description>&lt;h4 id=&#34;date-2021-02-19&#34;&gt;Date: 2021-02-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj84308655572pwdoulcn2fuckfmetrrsgnjmzvzuzkrzz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/84308655572?pwd=OUlCN2FUckFmeTRRSGNjMzVzUzkrZz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-843-0865-5572&#34;&gt;Meeting ID: 843 0865 5572&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider the joint estimation of regression parameters from&#xA;multiple potentially heterogeneous data sources with correlated vector&#xA;outcomes. The primary goal of this joint integrative analysis is to&#xA;estimate covariate effects on all vector outcomes through a marginal&#xA;regression model in a statistically and computationally efficient way.&#xA;We present a general class of distributed estimators that can be&#xA;implemented in a parallelized computational scheme. Modelling,&#xA;computational and theoretical challenges are overcome by first fitting a&#xA;local model within each data source and then combining local results&#xA;while accounting for correlation between data sources. This approach to&#xA;distributed estimation and inference is formulated using Hansen’s&#xA;generalized method of moments but implemented via an asymptotically&#xA;equivalent and communication-efficient meta-estimator. We show both&#xA;theoretically and numerically that the proposed method yields efficiency&#xA;improvements and is computationally fast. We illustrate the proposed&#xA;methodology with the joint integrative analysis of metabolic pathways in&#xA;a large multi-cohort study.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Adaptive Algorithm to Multi-armed Bandit Problem with High-dimensional Covariates</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-02-05/</link>
      <pubDate>Fri, 05 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-02-05/</guid>
      <description>&lt;h4 id=&#34;date-2021-02-05&#34;&gt;Date: 2021-02-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj84308655572pwdoulcn2fuckfmetrrsgnjmzvzuzkrzz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/84308655572?pwd=OUlCN2FUckFmeTRRSGNjMzVzUzkrZz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-843-0865-5572&#34;&gt;Meeting ID: 843 0865 5572&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This work studies an important sequential decision making problem known as the multi-armed bandit problem with covariates. Under a linear bandit framework with high-dimensional covariates, we propose a general arm allocation algorithm that integrates both arm elimination and randomized assignment strategies. By employing a class of high-dimensional regression methods for coefficient estimation, the proposed algorithm is shown to have near optimal finite-time regret performance under a new study scope that requires neither a margin condition nor a reward gap condition for competitive arms. Based on synergistically verified benefit of the margin, our algorithm exhibits an adaptive performance that automatically adapts to the margin and gap conditions, and attains the optimal regret rates under both study scopes, without or with the margin, up to a logarithmic factor. The proposed algorithm also simultaneously generates useful coefficient estimation output for competitive arms and is shown to achieve both estimation consistency and variable selection consistency. Promising empirical performance is demonstrated through two real data evaluation examples in drug dose assignment and news article recommendation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Large-scale Machine Learning Algorithms for Biomedical Data Science</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-01-15/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-01-15/</guid>
      <description>&lt;h4 id=&#34;date-2021-01-15&#34;&gt;Date: 2021-01-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj84308655572pwdoulcn2fuckfmetrrsgnjmzvzuzkrzz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/84308655572?pwd=OUlCN2FUckFmeTRRSGNjMzVzUzkrZz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-843-0865-5572&#34;&gt;Meeting ID: 843 0865 5572&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;During the last decade, hundreds of machine learning methods have been developed&#xA;for disease outcome prediction based on high-throughput genomics data. However, the quality&#xA;of the input genomics features and the output clinical variables has been ignored in these&#xA;algorithms. In this talk, I will introduce two studies that develop methods to learn more&#xA;accurate molecular signatures and drug response values for cancer research. These studies are&#xA;supported by NSF, NIH, and Moffitt Cancer Center.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quasi-random sampling for multivariate distributions via generative neural networks</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-12-04/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-12-04/</guid>
      <description>&lt;h4 id=&#34;date-2020-12-04&#34;&gt;Date: 2020-12-04&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A novel approach based on generative neural networks is introduced for constructing quasi-random number generators for multivariate models with any underlying copula in order to estimate expectations with variance reduction. So far, quasi-random number generators for multivariate distributions required a careful design, exploiting specific properties (such as conditional distributions) of the implied copula or the underlying quasi-Monte Carlo point set, and were only tractable for a small number of models. Utilizing specific generative neural networks allows one to construct quasi-random number generators for a much larger variety of multivariate distributions without such restrictions. Once trained with a pseudo-random sample, these neural networks only require a multivariate standard uniform randomized quasi-Monte Carlo point set as input and are thus fast in estimating expectations under dependence with variance reduction. Reproducible numerical examples are considered to demonstrate the approach. Emphasis is put on ideas rather than mathematical proofs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Probabilistic Approaches to Machine Learning on Tensor Data</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-11-27/</link>
      <pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-11-27/</guid>
      <description>&lt;h4 id=&#34;date-2020-11-27&#34;&gt;Date: 2020-11-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In contemporary scientific research, it is often of great interest to predict a categorical response based on a high-dimensional tensor (i.e. multi-dimensional array). Motivated by applications in science and engineering, we propose two probabilistic methods for machine learning on tensor data in the supervised and the unsupervised context, respectively. For supervised problems, we develop a comprehensive discriminant analysis model, called the CATCH model. The CATCH model integrates the information from the tensor and additional covariates to predict the categorical outcome with high accuracy. We further consider unsupervised problems, where no categorical response is available even on the training data. A doubly-enhanced EM (DEEM) algorithm is proposed for model-based tensor clustering, in which both the E-step and the M-step are carefully tailored for tensor data. CATCH and DEEM are developed under explicit statistical models with clear interpretations. They aggressively take advantage of the tensor structure and sparsity to tackle the new computational and statistical challenges arising from the intimidating tensor dimensions. Efficient algorithms are developed to solve the related optimization problems. Under mild conditions, CATCH and DEEM are shown to be consistent even when the dimension of each mode grows at an exponential rate of the sample size. Numerical studies also strongly support the application of CATCH and DEEM.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modeling viral rebound trajectories after analytical antiretroviral treatment interruption</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-11-20/</link>
      <pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-11-20/</guid>
      <description>&lt;h4 id=&#34;date-2020-11-20&#34;&gt;Date: 2020-11-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Despite the success of combined antiretroviral therapy (ART) in achieving sustained control of viral replication, the concerns about side-effects, drug-drug interactions, drug resistance and cost call for a need to identify strategies for achieving HIV eradication or an ART-free remission. Following ART withdrawal, patients&amp;rsquo; viral load levels usually increase rapidly to a peak followed by a dip, and then stabilize at a viral load set point. Characterizing features of the viral rebound trajectories (e.g., time to viral rebound and viral set points) and identifying host, virological, and immunological factors that are predictive of these features requires addressing analytical challenges such as non-linear viral rebound trajectories, coarsened data due to the assay’s limit of quantification, and intermittent measurements of viral load values. We first introduce a parametric nonlinear mixed effects (NLME) model for the viral rebound trajectory and compare its performance to a mechanistic modeling approach. We then develop a smoothed simulated pseudo maximum likelihood method for fitting NLME models that permits flexible specification of random effects distributions. Finally, we investigate the association between the time to viral suppression after ART initiation and the time to viral rebound after ART interruption through a Cox proportional hazard regression model where both the outcome and the covariate are interval-censored observations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generalized Energy-Based Models</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-11-06/</link>
      <pubDate>Fri, 06 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-11-06/</guid>
      <description>&lt;h4 id=&#34;date-2020-11-06&#34;&gt;Date: 2020-11-06&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;I will introduce Generalized Energy Based Models (GEBM) for generative modelling. These models combine two trained components: a base distribution (generally an implicit model), which can learn the support of data with low intrinsic dimension in a high dimensional space; and an energy function, to refine the probability mass on the learned support. Both the energy function and base jointly constitute the final model, unlike GANs, which retain only the base distribution (the &amp;ldquo;generator&amp;rdquo;). In particular, while the energy function is analogous to the GAN critic function, it is not discarded after training.&#xA;GEBMs are trained by alternating between learning the energy and the base, much like a GAN. Both training stages are well-defined: the energy is learned by maximising a generalized likelihood, and the resulting energy-based loss provides informative gradients for learning the base. Samples from the posterior on the latent space of the trained model can be obtained via MCMC, thus finding regions in this space that produce better quality samples. Empirically, the GEBM samples on image-generation tasks are of better quality than those from the learned generator alone, indicating that all else being equal, the GEBM will outperform a GAN of the same complexity. GEBMs also return state-of-the-art performance on density modelling tasks, and when using base measures with an explicit form.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Test-based integrative analysis of randomized trial and real-world data for treatment heterogeneity estimation</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-30/</link>
      <pubDate>Fri, 30 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-30/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-30&#34;&gt;Date: 2020-10-30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Parallel randomized clinical trial (RCT) and real-world data (RWD) are becoming increasingly available for treatment evaluation. Given the complementary features of the RCT and RWD, we propose a test-based integrative analysis of the RCT and RWD for accurate and robust estimation of the heterogeneity of treatment effect (HTE), which lies at the heart of precision medicine. When the RWD are not subject to bias, e.g., due to unmeasured confounding, our approach combines the RCT and RWD for optimal estimation by exploiting semiparametric efficiency theory. Utilizing the design advantage of RTs, we construct a built-in test procedure to gauge the reliability of the RWD and decide whether or not to use RWD in an integrative analysis. We characterize the asymptotic distribution of the test-based integrative estimator under local alternatives, which provides a better approximation of the finite-sample behaviors of the test and estimator when the idealistic assumption required for the RWD is weakly violated. We provide a data-adaptive procedure to select the threshold of the test statistic that promises the smallest mean square error of the proposed estimator of the HTE. Lastly, we construct an adaptive confidence interval that has a good finite-sample coverage property. We apply the proposed method to characterize who can benefit from adjuvant chemotherapy in patients with stage IB non-small cell lung cancer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear Regression and its Inference on Noisy Network-linked Data</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-23/</link>
      <pubDate>Fri, 23 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-23/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-23&#34;&gt;Date: 2020-10-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Linear regression on a set of observations linked by a network has been an essential tool in modeling the relationship between response and covariates with additional network data. Despite its wide range of applications in many areas, such as social sciences and health-related research, the problem has not been well-studied in statistics so far. Previous methods either lack of inference tools or rely on restrictive assumptions on social effects, and usually treat the network structure as precisely observed, which is too good to be true in many problems. We propose a linear regression model with nonparametric social effects. Our model does not assume the relational data or network structure to be accurately observed; thus, our method can be provably robust to a certain level of perturbation of the network structure. We establish a full set of computationally efficient asymptotic inference tools under a general requirement of the perturbation and then study the robustness of our method in the specific setting when the perturbation is from random network models. We discover a phase-transition phenomenon of inference validity concerning the network density when no prior knowledge about the network model is available, while also show the significant improvement achieved by knowing the network model. A by-product of our analysis is a rate-optimal concentration bound about subspace projection that may be of independent interest. We conduct extensive simulation studies to verify our theoretical observations and demonstrate the advantage of our method compared to a few benchmarks under different data-generating models. The method is then applied to adolescent network data to study the gender and racial differences in social activities.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adaptive MCMC For Everyone</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-16/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-16/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-16&#34;&gt;Date: 2020-10-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Markov chain Monte Carlo (MCMC) algorithms, such as the Metropolis&#xA;Algorithm and the Gibbs Sampler, are an extremely useful and popular&#xA;method of approximately sampling from complicated probability&#xA;distributions.  Adaptive MCMC attempts to automatically modify the&#xA;algorithm while it runs, to improve its performance on the fly.  However,&#xA;such adaptation often destroys the ergodicity properties necessary for the&#xA;algorithm to be valid.  In this talk, we first illustrate MCMC algorithms&#xA;using simple graphical examples.  We then discuss adaptive MCMC, and&#xA;present examples and theorems concerning its ergodicity and efficiency.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning and Neural Networks: Foundations and Some Fundamental Questions</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-09/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-09/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-09&#34;&gt;Date: 2020-10-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statistical learning theory is by now a mature branch of data science that hosts a vast variety of practical techniques for tackling data-related problems. In this talk we present some fundamental concepts upon which statistical learning theory has been based. Different approaches to statistical inference will be discussed and the main problem of learning from Vapnik&amp;rsquo;s point of view will be explained. Further we discuss the topic of function estimation as the heart of Vapnik-Chervonenkis theory. There exist several state-of-the-art methods for estimating functional dependencies, such as maximum margin estimator and artificial neural networks. While for some of these methods, e.g., the support vector machines, there has already been developed a profound theory, others require more investigation. Accordingly, we pay a closer attention to the so-called mapping neural networks and try to shed some light on certain theoretical aspects of them. We highlight some of the fundamental challenges that have attracted the attention of researcher and they are yet to be fully resolved. One of these challenges is estimation of the intrinsic dimension of data that will be discussed in detail. Another challenge is inferring causal direction when the training data set is not representative of the target population.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Large-scale Network Inference</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-09-25/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-09-25/</guid>
      <description>&lt;h4 id=&#34;date-2020-09-25&#34;&gt;Date: 2020-09-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1400-1500&#34;&gt;Time: 14:00-15:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj93947077997&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/93947077997&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-4707-7997&#34;&gt;Meeting ID: 939 4707 7997&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-no-password&#34;&gt;Passcode: no password&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Network data is prevalent in many contemporary big data applications in which a common interest is to unveil important latent links between different pairs of nodes. Yet a simple fundamental question of how to precisely quantify the statistical uncertainty associated with the identification of latent links still remains largely unexplored. In this paper, we propose the method of statistical inference on membership profiles in large networks (SIMPLE) in the setting of degree-corrected mixed membership model, where the null hypothesis assumes that the pair of nodes share the same profile of community memberships. In the simpler case of no degree heterogeneity, the model reduces to the mixed membership model for which an alternative more robust test is also proposed. Both tests are of the Hotelling-type statistics based on the rows of empirical eigenvectors or their ratios, whose asymptotic covariance matrices are very challenging to derive and estimate. Nevertheless, their analytical expressions are unveiled and the unknown covariance matrices are consistently estimated. Under some mild regularity conditions, we establish the exact limiting distributions of the two forms of SIMPLE test statistics under the null hypothesis and contiguous alternative hypothesis. They are the chi-square distributions and the noncentral chi-square distributions, respectively, with degrees of freedom depending on whether the degrees are corrected or not. We also address the important issue of estimating the unknown number of communities and establish the asymptotic properties of the associated test statistics. The advantages and practical utility of our new procedures in terms of both size and power are demonstrated through several simulation examples and real network applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>BdryGP: a boundary-integrated Gaussian process model for computer code emulation</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-09-18/</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-09-18/</guid>
      <description>&lt;h4 id=&#34;date-2020-09-18&#34;&gt;Date: 2020-09-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsmcgillzoomusj92453904989pwdzdr6rumxtznyk0zime9obwtomgjqdz09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/92453904989?pwd=ZDR6RUMxTzNYK0ZiME9ObWtoMGJqdz09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-924-5390-4989&#34;&gt;Meeting ID: 924 5390 4989&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-690084&#34;&gt;Passcode: 690084&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;With advances in mathematical modeling and computational methods, complex phenomena (e.g., universe formations, rocket propulsion) can now be reliably simulated via computer code. This code solves a complicated system of equations representing the underlying science of the problem. Such simulations can be very time-intensive, requiring months of computation for a single run. Gaussian processes (GPs) are widely used as predictive models for “emulating” this expensive computer code. Yet with limited training data on a high-dimensional parameter space, such models can suffer from poor predictive performance and physical interpretability.&#xA;Fortunately, in many physical applications, there is additional boundary information on the code beforehand, either from governing physics or scientific knowledge. We propose a new BdryGP model which incorporates such boundary information for prediction. We show that BdryGP not only enjoys improved convergence rates over standard GP models which do not incorporate boundaries, but is also more resistant to the ``curse-of-dimensionality&amp;rsquo;&amp;rsquo; in nonparametric regression. We then demonstrate the improved predictive performance and posterior contraction of the BdryGP model on several test problems in the literature.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A gentle introduction to generalized structured component analysis and its recent developments</title>
      <link>https://mcgillstat.github.io/post/2020winter/2020-03-27/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020winter/2020-03-27/</guid>
      <description>&lt;h4 id=&#34;date-2020-03-27&#34;&gt;Date: 2020-03-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1205&#34;&gt;Location: BURNSIDE 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Generalized structured component analysis (GSCA) was developed as a component-based approach to structural equation modeling, where constructs are represented by components or weighted composites of observed variables, rather than (common) factors. Unlike another long-lasting component-based approach – partial least squares path modeling, GSCA is a full-information method that optimizes a single criterion to estimate model parameters simultaneously, utilizing all information available in the entire system of equations. Over the decade, this approach has been refined and extended in various ways to enhance its data-analytic capability. I will briefly discuss the theoretical underpinnings of GSCA and demonstrate the use of an R package for GSCA - gesca. Moreover, I will outline some recent developments in GSCA, which include GSCA_M for estimating models with factors and integrated GSCA (IGSCA) for estimating models with both factors and components.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Informative Prior Elicitation from Historical Individual Patient Data</title>
      <link>https://mcgillstat.github.io/post/2020winter/2020-03-20/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020winter/2020-03-20/</guid>
      <description>&lt;h4 id=&#34;date-2020-03-20&#34;&gt;Date: 2020-03-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1205&#34;&gt;Location: BURNSIDE 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Historical data from previous studies may be utilized to strengthen statistical inference. Under the Bayesian framework incorporation of information obtained from any source other than the current data is facilitated through construction of an informative prior. The existing methodology for defining an informative prior based on historical data relies on measuring similarity to the current data at the study level that can result in discarding useful individual patient data (IPD). In this talk I present a family of priors that utilize IPD to strengthen statistical inference. IPD-based priors can be obtained as a weighted likelihood of the historical data where each individual&amp;rsquo;s weight is a function of their distance to the current study population. It is demonstrated that the proposed prior construction approach can considerably improve estimation accuracy and precision in compare with existing methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Geometry-based Data Exploration</title>
      <link>https://mcgillstat.github.io/post/2020winter/2020-03-13/</link>
      <pubDate>Fri, 13 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020winter/2020-03-13/</guid>
      <description>&lt;h4 id=&#34;date-2020-03-13&#34;&gt;Date: 2020-03-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1205&#34;&gt;Location: BURNSIDE 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;High-throughput data collection technologies are becoming increasingly common in many fields, especially in biomedical applications involving single cell data (e.g., scRNA-seq and CyTOF). These introduce a rising need for exploratory analysis to reveal and understand hidden structure in the collected (high-dimensional) Big Data. A crucial aspect in such analysis is the separation of intrinsic data geometry from data distribution, as (a) the latter is typically biased by collection artifacts and data availability, and (b) rare subpopulations and sparse transitions between meta-stable states are often of great interest in biomedical data analysis. In this talk, I will show several tools that leverage manifold learning, graph signal processing, and harmonic analysis for biomedical (in particular, genomic/proteomic) data exploration, with emphasis on visualization, data generation/augmentation, and nonlinear feature extraction. A common thread in the presented tools is the construction of a data-driven diffusion geometry that both captures intrinsic structure in data and provides a generalization of Fourier harmonics on it. These, in turn, are used to process data features along the data geometry for denoising and generative purposes. Finally, I will relate this approach to the recently-proposed geometric scattering transform that generalizes Mallat&amp;rsquo;s scattering to non-Euclidean domains, and provides a mathematical framework for theoretical understanding of the emerging field of geometric deep learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Non-central squared copulas: properties and applications</title>
      <link>https://mcgillstat.github.io/post/2020winter/2020-02-21/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020winter/2020-02-21/</guid>
      <description>&lt;h4 id=&#34;date-2020-02-21&#34;&gt;Date: 2020-02-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1205&#34;&gt;Location: BURNSIDE 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The goal of this presentation is to introduce new families of multivariate copulas, extending the chi-square copulas, the Fisher copula, and squared copulas. The new families are constructed from existing copulas by first transforming their margins to standard Gaussian distributions, then transforming these variables into non-central chi-square variables with one degree of freedom, and finally by considering the copula associated with these new variables. It is shown that by varying the non-centrality parameters, one can model non-monotonic dependence, and when one or many non-centrality parameters are outside a given hyper-rectangle, then the copula is almost the same as the one when these parameters are infinite. For these new families, the tail behavior, the monotonicity of dependence measures such as Kendall’s tau and Spearman’s rho are investigated, and estimation is discussed. Some examples will illustrate the usefulness of these new copula families.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sharing Sustainable Mobility in Smart Cities</title>
      <link>https://mcgillstat.github.io/post/2020winter/2020-02-14/</link>
      <pubDate>Fri, 14 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020winter/2020-02-14/</guid>
      <description>&lt;h4 id=&#34;date-2020-02-14&#34;&gt;Date: 2020-02-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1205&#34;&gt;Location: BURNSIDE 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Many cities worldwide are embracing electric vehicle (EV) sharing as a flexible and sustainable means of urban transit. However, it remains challenging for the operators to charge the fleet due to limited or costly access to charging facilities. In this work, we focus on answering the core question - how to charge the fleet to make EV sharing viable and profitable. Our work is motivated by the recent setback that struck San Diego, California, where car2go ceased its EV sharing operations. We integrate charging infrastructure planning and vehicle repositioning operations that were often considered separately in the literature. More interestingly, our modeling emphasizes the operator-controlled charging operations and customers&amp;rsquo; EV picking behavior, which are both central to EV sharing but were largely overlooked. Motivated by the actual data of car2go, our model explicitly characterizes how customers endogenously pick EVs based on energy levels, and how the operator dispatches EV charging under a targeted charging policy. We formulate the integrated model as a nonlinear optimization program with fractional constraints. We then develop both lower- and upper-bound formulations as mixed-integer second order cone programs, which are computationally tractable with small optimality gap. Contrary to car2go&amp;rsquo;s practice, we find that the viability of EV sharing can be enhanced by concentrating limited charger resources at selected locations. Charging EVs in a proactive fashion (rather than car2go&amp;rsquo;s policy of charging EVs only when their energy level drops below 20%) can boost the profit by 10.7%. Given the demand profile in San Diego, the fleet size may reduce by up to 34% without incurring significant profit loss. Moreover, sufficient charger availability is crucial when collaborating with a public charger network. Finally, increasing the charging power relieves the charger resource constraint, whereas extending per-charge range or adopting unmanned repositioning improves profitability. In summary, our work demonstrates a data-verified and high-granularity modeling approach. Both the high-level planning guidelines and operational policies can be useful for practitioners. We also highlight the value of jointly managing demand fulfilment and EV charging.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adapting black-box machine learning methods for causal inference</title>
      <link>https://mcgillstat.github.io/post/2020winter/2020-01-31/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020winter/2020-01-31/</guid>
      <description>&lt;h4 id=&#34;date-2020-01-31&#34;&gt;Date: 2020-01-31&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1104&#34;&gt;Location: BURNSIDE 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;ll discuss the use of observational data to estimate the causal effect of a treatment on an outcome.&#xA;This task is complicated by the presence of &amp;ldquo;confounders&amp;rdquo; that influence both treatment and outcome, inducing&#xA;observed associations that are not causal. Causal estimation is achieved by adjusting for this confounding by&#xA;using observed covariate information. I&amp;rsquo;ll discuss the case where we observe covariates that carry&#xA;sufficient information for the adjustment. But where explicit models relating treatment, outcome, covariates and&#xA;confounding are not available.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Estimation and inference for changepoint models</title>
      <link>https://mcgillstat.github.io/post/2020winter/2020-01-13/</link>
      <pubDate>Mon, 13 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020winter/2020-01-13/</guid>
      <description>&lt;h4 id=&#34;date-2020-01-13&#34;&gt;Date: 2020-01-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1205&#34;&gt;Location: BURNSIDE 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This talk is motivated by statistical challenges that arise in the analysis of calcium imaging data, a new technology in neuroscience that makes it possible to record from huge numbers of neurons at single-neuron resolution.  In the first part of this talk, I will consider the problem of estimating a neuron&amp;rsquo;s spike times from calcium imaging data. A simple and natural model suggests a non-convex optimization problem for this task. I will show that by recasting the non-convex problem as a changepoint detection problem, we can efficiently solve it for the global optimum using a clever dynamic programming strategy.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convergence rates for diffusions-based sampling and optimization methods</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-11-29/</link>
      <pubDate>Fri, 29 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-11-29/</guid>
      <description>&lt;h4 id=&#34;date-2019-11-29&#34;&gt;Date: 2019-11-29&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;An Euler discretization of the Langevin diffusion is known to converge to the global minimizers of certain convex and non-convex optimization problems. We show that this property holds for any suitably smooth diffusion and that different diffusions are suitable for optimizing different classes of convex and non-convex functions. This allows us to design diffusions suitable for globally optimizing convex and non-convex functions not covered by the existing Langevin theory. Our non-asymptotic analysis delivers computable optimization and integration error bounds based on easily accessed properties of the objective and chosen diffusion. Central to our approach are new explicit Stein factor bounds on the solutions of Poisson equations. We complement these results with improved optimization guarantees for targets other than the standard Gibbs measure.&lt;/p&gt;</description>
    </item>
    <item>
      <title> Logarithmic divergence: from finance to optimal transport and information geometry</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-11-15/</link>
      <pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-11-15/</guid>
      <description>&lt;h4 id=&#34;date-2019-11-15&#34;&gt;Date: 2019-11-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Divergences, such as the Kullback-Leibler divergence, are distance-like quantities which arise in many applications in probability, statistics and data science. We introduce a family of logarithmic divergences which is a non-linear extension of the celebrated Bregman divergence. It is defined for any exponentially concave function (a function whose exponential is concave). We motivate this divergence by mathematical finance and large deviations of Dirichlet process. It also arises naturally from the solution to an optimal transport problem. The logarithmic divergence enjoys remarkable mathematical properties including a generalized Pythagorean theorem in the sense of information geometry, and induces a generalized exponential family of probability densities. In the last part of the talk we present a new differential geometric framework which connects optimal transport and information geometry. Joint works with Soumik Pal and Jiaowen Yang.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Joint Robust Multiple Inference on Large Scale Multivariate Regression</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-11-08/</link>
      <pubDate>Fri, 08 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-11-08/</guid>
      <description>&lt;h4 id=&#34;date-2019-11-08&#34;&gt;Date: 2019-11-08&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Large scale multivariate regression with many heavy-tailed responses arises in a wide range of areas from genomics, financial asset pricing, banking regulation, to psychology and social studies. Simultaneously testing a large number of general linear hypotheses, such as multiple contrasts, based on the large scale multivariate regression reveals a variety of associations between responses and regression or experimental factors. Traditional multiple testing methods often ignore the effect of heavy-tailedness in the data and impose joint normality assumption that is arguably stringent in applications. This results in unreliable conclusions due to the lose of control on the false discovery proportion/rate (FDP/FDR) and severe compromise of power in practice. In this paper, we employ data-adaptive Huber regression to propose a framework of joint robust inference of the general linear hypotheses for large scale multivariate regression. With mild conditions, we show that the proposed method produces consistent estimate of the FDP and FDR at a prespecified level. Particularly, we employ a bias-correction robust covariance estimator and study its exponential-type deviation inequality to provide theoretical guarantee of our proposed multiple testing framework. Extensive numerical experiments demonstrate the gain in power of the proposed method compared to OLS and other procedures.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning Connectivity Networks from High-Dimensional Point Processes</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-10-25/</link>
      <pubDate>Fri, 25 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-10-25/</guid>
      <description>&lt;h4 id=&#34;date-2019-10-25&#34;&gt;Date: 2019-10-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;High-dimensional point processes have become ubiquitous in many scientific fields. For instance, neuroscientists use calcium florescent imaging to monitor the firing of thousands of neurons in live animals. In this talk, I will discuss new methodological, computational and theoretical developments for learning neuronal connectivity networks from high-dimensional point processes. Time permitting, I will also discuss a new approach for handling non-stationarity in high-dimensional time series.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Univariate and multivariate extremes of extendible random vectors</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-10-18/</link>
      <pubDate>Fri, 18 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-10-18/</guid>
      <description>&lt;h4 id=&#34;date-2019-10-18&#34;&gt;Date: 2019-10-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In its most common form extreme value theory is concerned with the limiting distribution of location-scale transformed block-maxima $M_n = \max(X_1,\dots,X_n)$ of a sequence of identically distributed random variables $(X_i)$, $i\geq 1$.&#xA;In case the members of the sequence $(X_i)$ are independent, the weak&#xA;limiting behaviour of $M_n$ is adequately described by the classical Fisher-Tippett-Gnedenko theorem.&#xA;In this presentation we are interested in the case of dependent random variables $(X_i)$ while retaining a common marginal distribution function $F$ for all&#xA;$X_i$, $i\in\mathbb{N}$.&#xA;Complementary to the well established extreme value theory in a time series setting we consider a framework in which the dependence between (extreme) events does not decay over time.&#xA;This approach is facilitated by highlighting the connection between block-maxima and copula diagonals in an asymptotic context.&#xA;The main goal of this presentation is to discuss a generalization of the Fisher&amp;ndash;Tippett&amp;ndash;Gnedenko theorem in this setting, leading to limiting distributions that are not in the class of generalized extreme value distributions.&#xA;This result is exemplified for popular dependence structures related to extreme value, Archimedean and Archimax copulas.&#xA;Focusing on the class of hierarchical Archimedean copulas the results can further be extended to the multivariate setting.&#xA;Finally, we illustrate the resulting limit laws and discuss their properties.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Repulsiveness for integration (not my social program)</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-10-11/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-10-11/</guid>
      <description>&lt;h4 id=&#34;date-2019-10-11&#34;&gt;Date: 2019-10-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Integral estimation in any dimension is an extensive topic, largely treated in the literature, with a broad range of applications. Monte-Carlo type methods arise naturally when one looks forward to quantifying/controlling the error. Many methods have already been developped: MCMC, Poisson disk sampling, QMC (and randomized versions), Bayesian quadrature, etc. In this talk, I&amp;rsquo;ll consider a different approach which consists in defining the quadrature nodes as the realization of a spatial point process. In particular I&amp;rsquo;ll show that a very specific class of determinantal point processes, a class of repulsive point patterns, has excellent properties and is able to estimate efficiently integrals for non-differentiable functions with an explicit and faster rate of convergence than current methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Regression Models for Spatial Images</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-09-27/</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-09-27/</guid>
      <description>&lt;h4 id=&#34;date-2019-09-27&#34;&gt;Date: 2019-09-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-mcintyre-medical-building-room-521&#34;&gt;Location: McIntyre Medical Building, Room 521&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This work is motivated by a problem in describing forest nitrogen&#xA;cycling, and a consequent goal of constructing regression models for&#xA;spatial images. Specifically, I present a functional concurrent linear&#xA;model (FLCM) with varying coefficients for two-dimensional spatial&#xA;images. To address overparameterization issues, the parameter surfaces&#xA;in this model are transformed into the wavelet domain and then sparse&#xA;representations are found using two different methods: LASSO and&#xA;Bayesian variable selection. I will briefly discuss extensions to&#xA;address missing data problems for colocated spatial images and the&#xA;modeling of tree species in landscape ecology. In addition I will&#xA;discuss the use of the sextant in marine navigation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Representation Learning using Discrete Domain Symmetries</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-09-20/</link>
      <pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-09-20/</guid>
      <description>&lt;h4 id=&#34;date-2019-09-20&#34;&gt;Date: 2019-09-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Symmetry has played a significant role in modern physics, in part by constraining the physical laws. I will discuss how it could play a fundamental role in AI by constraining the deep model design. In particular, I focus on discrete domain symmetries and through examples show how we can use this inductive bias as a principled means for constraining a feedforward layer and significantly improving its sample efficiency.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Integrative computational approach in genomics and healthcare</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-09-13/</link>
      <pubDate>Fri, 13 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-09-13/</guid>
      <description>&lt;h4 id=&#34;date-2019-09-13&#34;&gt;Date: 2019-09-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the current era of multi-omics and digital healthcare, we are facing unprecedented amount of data with tremendous opportunities to link molecular phenotypes with complex diseases. However, the lack of integrative statistical method hinders system-level interrogation of relevant disease-related pathways and the genetic implication in various healthcare outcome.&lt;/p&gt;&#xA;&lt;p&gt;In this talk, I will present our current progress in mining genomics and healthcare data. In particular, I will cover two main topics: (1) a statistical approach to assess gene set enrichments using genetic and transcriptomic data; (2) multimodal latent topic model for mining electronic healthcare and whole genome sequencing data from small patient cohort.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MAPLE; Semiparametric Estimation and Variable Selection for Length-biased Data with Heavy Censoring</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-09-06/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-09-06/</guid>
      <description>&lt;h4 id=&#34;date-2019-09-06&#34;&gt;Date: 2019-09-06&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, we discuss two problems of semiparametric estimation and variable selection for&#xA;length-biased data with heavy censoring.&#xA;The common feature of the proposed estimation procedures in the literature is that they only&#xA;put probability mass on failure times.&#xA;Under length-biased sampling, however, censoring is informative and failing to incorporate&#xA;censored observations into estimation can lead to&#xA;a substantial loss of efficiency. We propose two estimation procedures by computing the&#xA;likelihood contribution of both uncensored and censored observations.&#xA;For variable selection problem, we introduce a unified penalized estimating function and use an&#xA;optimization algorithm to solve it. We discuss&#xA;the asymptotic properties of the resulting penalized estimators. The work is motivated by the&#xA;International stroke Trial dataset collected in&#xA;Argentina in which the survival times of about 88% of the 545 cases are censored.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Graph Representation Learning and Applications</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-04-26/</link>
      <pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-04-26/</guid>
      <description>&lt;h4 id=&#34;date-2019-04-26&#34;&gt;Date: 2019-04-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1205&#34;&gt;Location: BURNSIDE 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Graphs, a general type of data structures for capturing interconnected objects, are ubiquitous in a variety of disciplines and domains ranging from computational social science, recommender systems, medicine, bioinformatics to chemistry. Representative examples of real-world graphs include social networks, user-item networks, protein-protein interaction networks, and molecular structures, which are represented as graphs. In this talk, I will introduce our work on learning effective representations of graphs such as learning low-dimensional node representations of large graphs (e.g., social networks, protein-protein interaction graphs, and knowledge graphs) and learning representations of entire graphs (e.g., molecule structures).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Estimating Time-Varying Causal Excursion Effect in Mobile Health with Binary Outcomes</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-04-12/</link>
      <pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-04-12/</guid>
      <description>&lt;h4 id=&#34;date-2019-04-12&#34;&gt;Date: 2019-04-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1205&#34;&gt;Location: BURNSIDE 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Advances in wearables and digital technology now make it possible to deliver behavioral, mobile health, interventions to individuals in their every-day life.  The micro-randomized trial (MRT) is increasingly used to provide data to inform the construction of these  interventions. This work is motivated by multiple MRTs that have been conducted or are currently in the field in which the primary outcome is a longitudinal binary outcome. The first, often called the primary, analysis in these trials is a marginal analysis that seeks to answer whether the data indicates that  a particular intervention component has an effect on the longitudinal binary outcome.  Under rather restrictive assumptions one can, based on existing literature, derive a semi-parametric, locally efficient estimator of the causal effect. In this talk, starting from this estimator, we develop multiple estimators that can be used as the basis of a primary analysis under more plausible assumptions. Simulation studies are conducted to compare the estimators.  We illustrate the developed methods using data from the MRT,  BariFit.  In BariFit, the goal is to support weight maintenance for individuals who received bariatric surgery.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian Estimation of Individualized Treatment-Response Curves in Populations with Heterogeneous Treatment Effects</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-04-05/</link>
      <pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-04-05/</guid>
      <description>&lt;h4 id=&#34;date-2019-04-05&#34;&gt;Date: 2019-04-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1104&#34;&gt;Location: BURNSIDE 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Estimating individual treatment effects is crucial for individualized or precision medicine. In reality, however, there is no way to obtain both the treated and untreated outcomes from the same person at the same time. An approximation can be obtained from randomized controlled trials (RCTs). Despite the limitations that randomizations are usually expensive, impractical or unethical, pre-specified variables may still not fully incorporate all the relevant characteristics capturing individual heterogeneity in treatment response. In this work, we use non-experimental data; we model heterogenous treatment effects in the studied population and provide a Bayesian estimator of the individual treatment response. More specifically, we develop a novel Bayesian nonparametric (BNP) method that leverages the G-computation formula to adjust for time-varying confounding in observational data, and it flexibly models sequential data to provide posterior inference over the treatment response at both group level and individual level. On a challenging dataset containing time series from patients admitted to intensive care unit (ICU), our approach reveals that these patients have heterogenous responses to the treatments used in managing kidney function. We also show that on held out data the resulting predicted outcome in response to treatment (or no treatment) is more accurate than alternative approaches.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Introduction to Statistical Network Analysis</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-03-29/</link>
      <pubDate>Fri, 29 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-03-29/</guid>
      <description>&lt;h4 id=&#34;date-2019-03-29&#34;&gt;Date: 2019-03-29&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1300-1630&#34;&gt;Time: 13:00-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-mcintyre----room-521&#34;&gt;Location: McIntyre &amp;ndash; Room 521&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Classical statistics often makes assumptions about conditional independence in&#xA;order to fit models but in the modern world connectivity is key. Nowadays we need&#xA;to account for many dependencies and sometimes the associations and&#xA;dependencies themselves are the key items of interest e.g. how do we predict&#xA;conflict between countries, how can we use friendships between school children to&#xA;choose the best groups for study tips/help, how does the pattern of needle-sharing&#xA;among partners correlate to HIV transmission and where interventions can best be&#xA;made. Basically any type of study where we are interested in connections or&#xA;associations between pairs of actors, be they people, companies, countries or&#xA;anything else, we are looking at a network analysis. The methods falling under this&#xA;area are collectively known as &amp;ldquo;Statistical Network Analysis&amp;rdquo; or sometimes &amp;ldquo;Social&#xA;Network Analysis&amp;rdquo; (which can be a bit misleading as we are not only talking about&#xA;Facebook and the like). This workshop will give a general introduction to networks,&#xA;their visualisation, summary measures and statistical models that can be used to&#xA;analyse them. The practical component will be in R and attendees will get the&#xA;most benefit if they are able to bring a laptop along to work through examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Challenges in Bayesian Computing</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-03-22/</link>
      <pubDate>Fri, 22 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-03-22/</guid>
      <description>&lt;h4 id=&#34;date-2019-03-22&#34;&gt;Date: 2019-03-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Computing is both the most mathematical and most applied aspect of statistics.  We shall talk about various urgent computing-related topics in statistical (in particular, Bayesian) workflow, including exploratory data analysis and model checking, Hamiltonian Monte Carlo, monitoring convergence of iterative simulations, scalable computing, evaluation of approximate algorithms, predictive model evaluation, and simulation-based calibration.  This work is inspired by applications including survey research, drug development, and environmental decision making.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hierarchical Bayesian Modelling for Wireless Cellular Networks</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-03-15/</link>
      <pubDate>Fri, 15 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-03-15/</guid>
      <description>&lt;h4 id=&#34;date-2019-03-15&#34;&gt;Date: 2019-03-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;With the recent advances in wireless technologies, base stations are becoming more sophisticated. The network operators are also able to collect more data to improve network performance and user experience. In this paper we concentrate on modeling performance of wireless cells using hierarchical Bayesian modeling framework. This framework provides a principled way to navigate the space between the option of creating one model to represent all cells in a network and the option of creating separate models at each cell. The former option ignores the variations between cells (complete pooling) whereas the latter is overly noisy and ignores the common patterns in cells (no pooling). The hierarchical Bayesian model strikes a trade-off between these two extreme cases and enables us to do partial pooling of the data from all cells. This is done by estimating a parametric population distribution and assuming that each cell is a sample from this distribution. Because this model is fully Bayesian, it provides uncertainty intervals around each estimated parameter which can be used by network operators making network management decisions. We examine the performance of this method on a synthetic dataset and a real dataset collected from a cellular network.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical Inference for partially observed branching processes, with application to hematopoietic lineage tracking</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-03-01/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-03-01/</guid>
      <description>&lt;h4 id=&#34;date-2019-03-01&#34;&gt;Date: 2019-03-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The likelihood function is central to many statistical procedures, but poses challenges in classical and modern data settings. Motivated by cell lineage tracking experiments to study hematopoiesis (the process of blood cell production), we present recent methodology enabling likelihood-based inference for partially observed data arising from continuous-time branching processes. These computational advances allow principled procedures such as maximum likelihood estimation, posterior inference, and expectation-maximization (EM) algorithms in previously intractable data settings. We then discuss limitations and alternatives when data are very large or generated from a hidden process, and potential ways forward using ideas from sparse optimization.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Uniform, nonparametric, non-asymptotic confidence sequences</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-02-22/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-02-22/</guid>
      <description>&lt;h4 id=&#34;date-2019-02-22&#34;&gt;Date: 2019-02-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A confidence sequence is a sequence of confidence intervals that is uniformly valid over an unbounded time horizon. In this paper, we develop non-asymptotic confidence sequences under nonparametric conditions that achieve arbitrary precision. Our technique draws a connection between the classical Cramer-Chernoff method, the law of the iterated logarithm (LIL), and the sequential probability ratio test (SPRT)—our confidence sequences extend the first to produce time-uniform concentration bounds, provide tight non-asymptotic characterizations of the second, and generalize the third to nonparametric settings, including sub-Gaussian and Bernstein conditions, self-normalized processes, and matrix martingales. We strengthen and generalize existing constructions of finite-time iterated logarithm (“finite LIL”) bounds. We illustrate the generality of our proof techniques by deriving an empirical-Bernstein finite LIL bound as well as a novel upper LIL bound for the maximum eigenvalue of a sum of random matrices. Finally, we demonstrate the utility of our approach with applications to covariance matrix estimation and to estimation of sample average treatment effect under the Neyman-Rubin potential outcomes model, for which we give a non-asymptotic, sequential estimation strategy which handles adaptive treatment mechanisms such as Efron’s biased coin design.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Causal Inference with Unmeasured Confounding: an Instrumental Variable Approach</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-02-15/</link>
      <pubDate>Fri, 15 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-02-15/</guid>
      <description>&lt;h4 id=&#34;date-2019-02-15&#34;&gt;Date: 2019-02-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Causal inference is a challenging problem because causation cannot be established from observational data alone. Researchers typically rely on additional sources of information to infer causation from association. Such information may come from powerful designs such as randomization, or background knowledge such as information on all confounders. However, perfect designs or background knowledge required for establishing causality may not always be available in practice. In this talk, I use novel causal identification results to show that the instrumental variable approach can be used to combine the power of design and background knowledge to draw causal conclusions. I also introduce novel estimation tools to construct estimators that are robust, efficient and enjoy good finite sample properties. These methods will be discussed in the context of a randomized encouragement design for a flu vaccine.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Patient-Specific Finite Element Analysis of Human Heart: Mathematical and Statistical Opportunities and Challenges</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-02-08/</link>
      <pubDate>Fri, 08 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-02-08/</guid>
      <description>&lt;h4 id=&#34;date-2019-02-08&#34;&gt;Date: 2019-02-08&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Cardiovascular diseases (CVD) are the leading cause of death globally and ranks second in&#xA;Canada, costing the Canadian economy over $20 billion every year. Despite the recent progress in CVD through prevention, lifestyle changes, and the use of biomedical treatments to improve survival rates and quality of life, there has been a lack in the integration of computer-aided engineering (CAE) in this field. Clinically, proposing cut-off values while taking into consideration patient-specific risk is of paramount importance for increased rate ofsurvival and improved quality of life. Computational modeling has proved to be used in determining parameters that cannot be assessed experimentally. The latest developments in computational modelling of human heart are presented and the constitutive equations, the key ingredient of these in-silico modellings of human heart, are discussed. Finite Element analysis of cardiac diseases provide a framework to generate synthetic data for developing statistical models when collecting the real data require invasive procedure. The idea of virtual personalized cardiology will be discussed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modern Non-Problems in Optimization: Applications to Statistics and Machine Learning</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-01-25/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-01-25/</guid>
      <description>&lt;h4 id=&#34;date-2019-01-25&#34;&gt;Date: 2019-01-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1600-1700&#34;&gt;Time: 16:00-17:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-920&#34;&gt;Location: BURN 920&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We have witnessed a lot of exciting development of data science in recent years. From the perspective of optimization, many modern data-science problems involve some basic ``non’’-properties that lack systematic treatment by the current approaches for the sake of the computation convenience. These non-properties include the coupling of the non-convexity, non-differentiability and non-determinism. In this talk, we present rigorous computational methods for solving two typical non-problems: the piecewise linear regression and the feed-forward deep neural network. The algorithmic framework is an integration of the first order non-convex majorization-minimization method and the second order non-smooth Newton methods. Numerical experiments demonstrate the effectiveness of our proposed approach. Contrary to existing methods for solving non-problems which provide at best very weak guarantees on the computed solutions obtained in practical implementation, our rigorous mathematical treatment aims to understand properties of these computed solutions with reference to both the empirical and the population risk minimizations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Singularities of the information matrix and longitudinal data with change points</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-01-18/</link>
      <pubDate>Fri, 18 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-01-18/</guid>
      <description>&lt;h4 id=&#34;date-2019-01-18&#34;&gt;Date: 2019-01-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Non-singularity of the information matrix plays a key role in model identification and the asymptotic theory of statistics. For many statistical models, however, this condition seems virtually impossible to verify. An example of such models is a class of mixture models associated with multi-path change-point problems (MCP) which can model longitudinal data with change points. The MCP models are similar in nature to mixture-of-experts models in machine learning. The question then arises as to how often the non-singularity assumption of the information matrix fails to hold. We show that&lt;/p&gt;</description>
    </item>
    <item>
      <title>Magic Cross-Validation Theory for Large-Margin Classification</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-01-11/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-01-11/</guid>
      <description>&lt;h4 id=&#34;date-2019-01-11&#34;&gt;Date: 2019-01-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Cross-validation (CV) is perhaps the most widely used tool for tuning supervised machine learning algorithms in order to achieve better generalization error rate. In this paper, we focus on leave-one-out cross-validation (LOOCV) for the support vector machine (SVM) and related algorithms. We first address two wide-spreading misconceptions on LOOCV. We show that LOOCV, ten-fold, and five-fold CV are actually well-matched in estimating the generalization error, and the computation speed of LOOCV is not necessarily slower than that of ten-fold and five-fold CV. We further present a magic CV theory with a surprisingly simple recipe which allows users to very efficiently tune the SVM. We then apply the magic CV theory to demonstrate a straightforward way to prove the Bayes risk consistency of the SVM. We have implemented our algorithms in a publicly available R package magicsvm, which is much faster than the state-of-the-art SVM solvers. We demonstrate our methods on extensive simulations and benchmark examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>p-values vs Bayes factors: Is there a compromise?</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-11-23/</link>
      <pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-11-23/</guid>
      <description>&lt;h4 id=&#34;date-2018-11-23&#34;&gt;Date: 2018-11-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This is not a research talk. Rather, the goal is to address the topic of the talk title through a&#xA;2017 multi-authored paper published in Nature Human Behaviour. The Nature article proposes that the standard cut-off significance level of .05 should be replaced by a cut-off level of .005 when new discoveries are being claimed. The authors attribute the high proportion of irreducible results in the literature that accompany claimed new discoveries, in part, to the low-bar cut-off of .05. Their fix is built around the Bayes factor. I will begin with a brief presentation of the difference between the frequentist and Bayesian approaches to statistical inference, and lead into p-values vs Bayes factors for hypothesis testing before discussing the Nature article itself. It is hoped that the talk will provoke thought about the way we do statistics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Estimation of the Median Residual Lifetime Function for Length-Biased Failure Time Data</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-11-16/</link>
      <pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-11-16/</guid>
      <description>&lt;h4 id=&#34;date-2018-11-16&#34;&gt;Date: 2018-11-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The median residual lifetime function is a statistical quantity which describes the future point in time at which the probability of current survival has dropped by 50%. In deriving an estimator for the median residual lifetime function for length-biased data, the added features of left-truncation and right-censoring must be taken into account.&lt;/p&gt;&#xA;&lt;p&gt;In this talk, we give a brief description of length-biased failure time data and show that by using a particular non-parametric estimator for the survival function that it is possible to derive the asymptotically most-efficient non-parametric estimator for the median residual lifetime function. We give some details on the proof of the asymptotic results and examine the performance of the estimator using simulated data. We also apply the proposed estimator to the Canadian Study of Health and Aging data set to study the median residual lifetime function of patients with dementia.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Density estimation of mixtures of Gaussians and Ising models</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-11-09/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-11-09/</guid>
      <description>&lt;h4 id=&#34;date-2018-11-09&#34;&gt;Date: 2018-11-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Density estimation lies at the intersection of statistics, theoretical computer science, and machine learning. We review some old and new results on the sample complexities (also known as minimax convergence rates) of estimating densities of high-dimensional distributions, in particular mixtures of Gaussians and Ising models.&lt;/p&gt;&#xA;&lt;p&gt;Based on joint work with Hassan Ashtiani, Shai Ben-David, Luc Devroye, Nick Harvey, Christopher Liaw, Yani Plan, and Tommy Reddad.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Terrorists never congregate in even numbers (and other strange results in fragmentation-coalescence)</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-11-02/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-11-02/</guid>
      <description>&lt;h4 id=&#34;date-2018-11-02&#34;&gt;Date: 2018-11-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The rigorous mathematical treatment of random fragmentation-coalescent models in the literature is difficult to find, and perhaps for good reason. We examine two different types of random fragmentation-coalescent models which produce somewhat unexpected results.&lt;/p&gt;&#xA;&lt;p&gt;The first concerns an agent-based model in which, with a rate that depends on the configuration of the system, agents coalesce into clusters that also fragment into their individual constituent membership. We consider the large-scale, long-term behaviour of this system in a similar spirit to recent use of such models to characterise the evolution of terrorist cells. Under appropriate assumptions we find an unusual behaviour; the system displays stabilisation with clusters that only contain an odd number of individuals.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Object Oriented Data Analysis with Application to Neuroimaging Studies</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-10-26/</link>
      <pubDate>Fri, 26 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-10-26/</guid>
      <description>&lt;h4 id=&#34;date-2018-10-26&#34;&gt;Date: 2018-10-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, I will first briefly introduce my research on object oriented data analysis with application to neuroimaging studies. I will then talk about a detailed example on imaging genetics. In this project, we develop a high-dimensional matrix linear regression model to correlate 2D imaging responses with high-dimensional genetic covariates. We propose a fast and efficient screening procedure based on the spectral norm to deal with the case that the dimension of scalar covariates is much larger than the sample size. We develop an efficient estimation procedure based on the nuclear norm regularization, which explicitly borrows the matrix structure of coefficient matrices. We examine the finite-sample performance of our methods using simulations and a large-scale imaging genetic dataset from the Alzheimer&amp;rsquo;s Disease Neuroimaging Initiative study.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multilevel clustering and optimal transport</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-10-19/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-10-19/</guid>
      <description>&lt;h4 id=&#34;date-2018-10-19&#34;&gt;Date: 2018-10-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Optimal transport plays an increasingly relevant and useful role in the theory and application of mixture model based clustering and inference. In this talk I will describe some recent progress in characterizing the convergence behavior of mixing distributions when one fits a mixture model to the data. This theory hinges on the relationship between the space of mixture densities, which is endowed with variational or Hellinger distance, and the space of mixing measures endowed with optimal transport distance metrics. Next, I will introduce an optimal transport based technique for the problem of multilevel clustering, which aims to simultaneously partition data in each group and discover grouping patterns among groups in a potentially large hierarchically structured corpus of data. Our method involves a joint optimization formulation over several spaces of discrete probability measures. We propose a number of variants of this problem, which admit fast optimization algorithms, by exploiting the connection to the problem of finding Wasserstein barycenters.  Some theoretical and experimental results will be presented.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dimension Reduction for Causal Inference</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-10-05/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-10-05/</guid>
      <description>&lt;h4 id=&#34;date-2018-10-05&#34;&gt;Date: 2018-10-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, we discuss how sufficient dimension reduction can be used to aid causal inference. We propose a new matching approach based on the reduced covariates obtained from sufficient dimension reduction. Compared with the original covariates and the propensity scores, which are commonly used for matching in the literature, the reduced covariates are estimable nonparametrically and are effective in imputing the missing potential outcomes. Under the ignorability assumption, the consistency of the proposed approach requires a weaker common support condition than the one we often assume for propensity score-based methods. We develop asymptotic properties, and conduct simulation studies as well as real data analysis to illustrate the proposed approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Selective inference for dynamic treatment regimes via the LASSO</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-09-28/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-09-28/</guid>
      <description>&lt;h4 id=&#34;date-2018-09-28&#34;&gt;Date: 2018-09-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Constructing an optimal dynamic treatment regime become complex when there are large number of prognostic factors, such as patient’s genetic information, demographic characteristics, medical history over time. Existing methods only focus on selecting the important variables for the decision-making process and fall short in providing inference for the selected model. We fill this gap by leveraging the conditional selective inference methodology. We show that the proposed method is asymptotically valid given certain rate assumptions in semiparametric regression.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Possession Sketches: Mapping NBA Strategies</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-09-21/</link>
      <pubDate>Fri, 21 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-09-21/</guid>
      <description>&lt;h4 id=&#34;date-2018-09-21&#34;&gt;Date: 2018-09-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-0930-1015&#34;&gt;Time: 09:30-10:15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-bronfman-building-001&#34;&gt;Location: Bronfman Building 001&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We present Possession Sketches, a new machine learning method for organizing and exploring a database of basketball player-tracks. Our method organizes basketball possessions by offensive structure. We first develop a model for populating a dictionary of short, repeated, and spatially registered actions. Each action corresponds to an interpretable type of player movement. We examine statistical patterns in these actions, and show how they can be used to describe individual player behavior. Leveraging this vocabulary of actions, we develop a hierarchical model that describes interactions between players. Our approach draws on the topic-modeling literature, extending Latent Dirichlet Allocation (LDA) through a novel representation of player movement data which uses techniques common in animation and video game design. We show that our model is able to group together possessions with similar offensive structure, allowing for efficient search and exploration of the entire database of player-tracking data. We show that our model finds repeated offensive structure in teams (e.g. strategy), providing a much more sophisticated, yet interpretable lens into basketball player-tracking data. This is joint work with Andrew Miller.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantile LASSO in Nonparametric Models with Changepoints Under Optional Shape Constraints</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-09-14/</link>
      <pubDate>Fri, 14 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-09-14/</guid>
      <description>&lt;h4 id=&#34;date-2018-09-14&#34;&gt;Date: 2018-09-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Nonparametric models are popular modeling tools because of their natural overall flexibility. In our&#xA;approach, we apply nonparametric techniques for panel data structures with changepoints and optional&#xA;shape constraints and the estimation is performed in a fully data driven manner by utilizing atomic pursuit&#xA;methods – LASSO regularization techniques in particular. However, in order to obtain robust estimates&#xA;and, also, to have a more complex insight into the underlying data structure, we target conditional&#xA;quantiles rather then the conditional mean only. The whole estimation process and the following inference&#xA;become both more challenging but the results are more useful in practical applications. The underlying&#xA;model is firstly introduced and some theoretical results are presented. The proposed methodology is&#xA;applied for a real data scenario and some finite sample properties are investigated via an extensive&#xA;simulation study. This is a joint work with Ivan Mizera, University of Alberta and Gabriela Ciuperca, University of Lyon&lt;/p&gt;</description>
    </item>
    <item>
      <title>Association Measures for Clustered Competing Risks Data</title>
      <link>https://mcgillstat.github.io/post/2018fall/2018-09-07/</link>
      <pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018fall/2018-09-07/</guid>
      <description>&lt;h4 id=&#34;date-2018-09-07&#34;&gt;Date: 2018-09-07&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this work, we propose a semiparametric model for multivariate clustered competing&#xA;risks data when the cause-specific failure times and the occurrence of competing risk&#xA;events among subjects within the same cluster are of interest. The cause-specific&#xA;hazard functions are assumed to follow Cox proportional hazard models, and the&#xA;associations between failure times given the same or different cause events and the&#xA;associations between occurrences of competing risk events within the same cluster are&#xA;investigated through copula models. A cross-odds ratio measure is explored under our&#xA;proposed models. Two-stage estimation procedure is proposed in which the marginal&#xA;models are estimated in the first stage, and the dependence parameters are estimated&#xA;via an Expectation-Maximization algorithm in the second stage. The proposed&#xA;estimators are shown to yield consistent and asymptotically normal under mild&#xA;regularity conditions. Simulation studies are conducted to assess finite sample&#xA;performance of the proposed method. The proposed technique is demonstrated&#xA;through an application to a multicenter Bone Marrow transplantation dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Methodological challenges in using point-prevalence versus cohort data in risk factor analyses of hospital-acquired infections</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-04-27/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-04-27/</guid>
      <description>&lt;h4 id=&#34;date-2018-04-27&#34;&gt;Date: 2018-04-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;To explore the impact of length-biased sampling on the evaluation of risk&#xA;factors of nosocomial infections in point-prevalence studies.&#xA;We used cohort data with full information including the exact date of the&#xA;nosocomial infection and mimicked an artificial one-day prevalence study by&#xA;picking a sample from this cohort study. Based on the cohort data, we studied&#xA;the underlying multi-state model which accounts for nosocomial infection as an&#xA;intermediate and discharge/death as competing events. Simple formulas are&#xA;derived to display relationships between risk-, hazard- and prevalence odds&#xA;ratios.&#xA;Due to length-biased sampling, long-stay and thus sicker patients are more&#xA;likely to be sampled. In addition, patients with nosocomial infections usually stay longer in hospital. We explored mechanisms which are -due to the design-&#xA;hidden in prevalence data. In our example, we showed that prevalence odds&#xA;ratios were usually less pronounced than risk odds ratios but more pronounced&#xA;than hazard ratios.&#xA;Thus, to avoid misinterpretation, knowledge of the mechanisms from the&#xA;underlying multi-state model are essential for the interpretation of risk factors&#xA;derived from point-prevalence data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernel Nonparametric Overlap-based Syncytial Clustering</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-04-20/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-04-20/</guid>
      <description>&lt;h4 id=&#34;date-2018-04-20&#34;&gt;Date: 2018-04-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Standard clustering algorithms can find regular-structured clusters such as ellipsoidally- or spherically-dispersed groups, but are more challenged  with groups lacking formal structure or definition. Syncytial clustering is the name that we introduce for methods that merge groups obtained from standard clustering algorithms in order to reveal complex group structure in the data. Here, we develop a distribution-free fully-automated syncytial algorithm that can be used with the computationally efficient k-means or other algorithms. Our approach computes the cumulative distribution function of the normed residuals from an appropriately fit k-groups model and calculates  the nonparametric overlap between all pairs of groups. Groups with high pairwise overlap are merged as long as the generalized overlap decreases. Our methodology is always a top performer in identifying groups with regular and irregular structures in many datasets. We use our method to identify the distinct kinds of activation in a functional Magnetic Resonance Imaging study.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Empirical likelihood and robust regression in diffusion tensor imaging data analysis</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-04-06/</link>
      <pubDate>Fri, 06 Apr 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-04-06/</guid>
      <description>&lt;h4 id=&#34;date-2018-04-06&#34;&gt;Date: 2018-04-06&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;With modern technology development, functional responses are  observed frequently in various scientific fields including neuroimaging data analysis. Empirical likelihood as a nonparametric data-driven technique has become an important statistical inference methodology. In this paper, motivated by diffusion tensor imaging (DTI) data we propose three generalized empirical likelihood-based methods that accommodate within-curve dependence on the varying coefficient model with functional responses and embed a robust regression idea. To avoid the loss of efficiency in statistical inference, we take into consideration within-curve variance-covariance matrix in the subjectwise and elementwise empirical likelihood methods. We develop several statistical inference procedures for maximum empirical likelihood estimators (MELEs) and empirical log likelihood (ELL) ratio functions, and systematically study their asymptotic properties. We first establish the weak convergence of the MELEs and the ELL ratio processes, and derived a nonparametric version of the Wilks theorem for the limiting distributions of the ELLs at any designed point. We propose a global test for linear hypotheses of varying coefficient functions and construct simultaneous confidence bands for each individual effect curve based on MELEs, and construct simultaneous confidence regions for varying coefficient functions based on ELL ratios. A Monte Carlo simulation is conducted to examine the finite-sample performance of the proposed procedures. Finally, we illustrate the estimation and inference procedures on MELEs of varying coefficient model to a diffusion tensor imaging data from Alzheimer&amp;rsquo;s Disease Neuroimaging Initiative (ADNI) study. Joint work with Xingcai Zhou (Nanjing Audit University), Rohana Karunamuni and Adam Kashlak (University of Alberta).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some development on dynamic computer experiments</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-03-23/</link>
      <pubDate>Fri, 23 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-03-23/</guid>
      <description>&lt;h4 id=&#34;date-2018-03-23&#34;&gt;Date: 2018-03-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Computer experiments refer to the study of real systems using complex simulation models.&#xA;They have been widely used as efficient, economical alternatives to physical experiments.&#xA;Computer experiments with time series outputs are called dynamic computer experiments.&#xA;In this talk, we consider two problems of such experiments: emulation of large-scale dynamic&#xA;computer experiments and inverse problem. For the first problem, we proposed a&#xA;computationally efficient modelling approach which sequentially finds a set of local design&#xA;points based on a new criterion specifically designed for emulating dynamic computer&#xA;simulators. Singular value decomposition based Gaussian process models are built with the&#xA;sequentially chosen local data. To update the models efficiently, an empirical Bayesian&#xA;approach is introduced. The second problem aims to extract an optimal input of dynamic&#xA;computer simulator whose response matches a field observation as closely as possible. A&#xA;sequential design approach is employed and a novel expected improvement criterion is&#xA;proposed. A real application is discussed to support the efficiency of the proposed approaches.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical Genomics for Understanding Complex Traits</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-03-16/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-03-16/</guid>
      <description>&lt;h4 id=&#34;date-2018-03-16&#34;&gt;Date: 2018-03-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Over the last decade, advances in measurement technologies has enabled researchers to generate multiple types of high-dimensional &amp;ldquo;omics&amp;rdquo; datasets for large cohorts. These data provide an opportunity to derive a mechanistic understanding of human complex traits. However, inferring meaningful biological relationships from these data is challenging due to high-dimensionality , noise, and abundance of confounding factors. In this talk, I&amp;rsquo;ll describe statistical approaches for robust analysis of genomic data from large population studies, with a focus on 1) understanding the nature of confounding and approaches for addressing them and 2) understanding the genomic correlates of aging and dementia.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sparse Penalized Quantile Regression: Method, Theory, and Algorithm</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-02-23/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-02-23/</guid>
      <description>&lt;h4 id=&#34;date-2018-02-23&#34;&gt;Date: 2018-02-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Sparse penalized quantile regression is a useful tool for variable selection, robust estimation, and heteroscedasticity detection in high-dimensional data analysis. We discuss the variable selection and estimation properties of the lasso and folded concave penalized quantile regression via non-asymptotic arguments. We also consider consistent parameter tuning therein. The computational issue of the sparse penalized quantile regression has not yet been fully resolved in the literature, due to non-smoothness of the quantile regression loss function. We introduce fast alternating direction method of multipliers (ADMM) algorithms for computing the sparse penalized quantile regression. Numerical examples demonstrate the competitive performance of our algorithm: it significantly outperforms several other fast solvers for high-dimensional penalized quantile regression.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Methodological considerations for the analysis of relative treatment effects in multi-drug-resistant tuberculosis from fused observational studies</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-02-09/</link>
      <pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-02-09/</guid>
      <description>&lt;h4 id=&#34;date-2018-02-09&#34;&gt;Date: 2018-02-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Multi-drug-resistant tuberculosis (MDR-TB) is defined as strains of tuberculosis that do not respond to at least the two most used anti-TB drugs. After diagnosis, the intensive treatment phase for MDR-TB involves taking several alternative antibiotics concurrently. The Collaborative Group for Meta-analysis of Individual Patient Data in MDR-TB has assembled a large, fused dataset of over 30 observational studies comparing the effectiveness of 15 antibiotics. The particular challenges that we have considered in the analysis of this dataset are the large number of potential drug regimens, the resistance of MDR-TB strains to specific antibiotics, and the identifiability of a generalized parameter of interest though most drugs were not observed in all studies. In this talk, I describe causal inference theory and methodology that we have appropriated or developed for the estimation of treatment importance and relative effectiveness of different antibiotic regimens with a particular emphasis on targeted learning approaches&lt;/p&gt;</description>
    </item>
    <item>
      <title>A new approach to model financial data: The Factorial Hidden Markov Volatility Model</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-02-02/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-02-02/</guid>
      <description>&lt;h4 id=&#34;date-2018-02-02&#34;&gt;Date: 2018-02-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A new process, the factorial hidden Markov volatility (FHMV) model, is proposed to model financial returns or realized variances. This process is constructed based on a factorial hidden Markov model structure and corresponds to a parsimoniously parametrized hidden Markov model that includes thousands of volatility states. The transition probability matrix of the underlying Markov chain is structured so that the multiplicity of its second largest eigenvalue can be greater than one. This distinctive feature allows for a better representation of volatility persistence in financial data. Jumps and a leverage effect are also incorporated into the model and statistical properties are discussed. An empirical study on six financial time series shows that the FHMV process compares favorably to state-of-the-art volatility models in terms of in-sample fit and out-of-sample forecasting performance over time horizons ranging from one to one hundred days.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generalized Sparse Additive Models</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-01-19/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-01-19/</guid>
      <description>&lt;h4 id=&#34;date-2018-01-19&#34;&gt;Date: 2018-01-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;I will present a unified approach to the estimation of generalized sparse additive models in high dimensional regression problems. Our approach is based on combining structure-inducing and sparsity penalties in a single regression problem. It allows for the use of a large family of structure-inducing penalties: Those characterized by semi-norm constraints. This includes finite dimensional linear subspaces, sobolev and holder classes, classes with bounded total variation, among others. We give an efficient computational algorithm to fit this family of models that easily scales to thousands of observations and features. In addition we develop a framework for proving convergence bounds on these estimators; and show that our estimators converge at the minimax optimal rate under suitable conditions. We also compare the performance of existing methods in an empirical study and discuss directions for future work.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modelling RNA stability for decoding the regulatory programs that drive human diseases</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-01-12/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-01-12/</guid>
      <description>&lt;h4 id=&#34;date-2018-01-12&#34;&gt;Date: 2018-01-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The key determinant of the identity and behaviour of the cell is gene regulation, i.e. which genes are active and which genes are inactive in a particular cell. One of the least understood aspects of gene regulation is RNA stability: genes produce RNA molecules to carry their genetic information – the more stable these RNA molecules are, the longer they can function within the cell, and the less stable they are, the more rapidly they are removed from the pool of active molecules. The cell can effectively switch the genes on and off by regulating RNA stability. However, we do not know which genes are regulated at the RNA stability level, and what factors affect their stability. The focus of our research is development of novel computational methods that enables the measurement of RNA stability and decay rate from functional genomics data, and inference of models that explain how human cells regulate RNA stability. We are particularly interested in how defects in regulation of RNA stability can lead to development and progression of various human diseases, such as cancer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fisher’s method revisited: set-based genetic association and interaction studies</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-12-01/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-12-01/</guid>
      <description>&lt;h4 id=&#34;date-2017-12-01&#34;&gt;Date: 2017-12-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Fisher’s method, also known as Fisher’s combined probability test, is commonly used in meta-analyses to combine p-values from the same test applied to K independent samples to evaluate a common null hypothesis. Here we propose to use it to combine p-values from different tests applied to the same sample in two settings: when jointly analyzing multiple genetic variants in set-based genetic association studies, or when jointly capturing main and interaction effects in the presence of missing one of the interacting variables. In the first setting, we show that many existing methods (e.g. the so called burden test and SKAT) can be classified into a class of linear statistics and another class of quadratic statistics, where each class is powerful only in part of the high-dimensional parameter space. In the second setting, we show that the class of scale-tests for heteroscedasticity can be utilized to indirectly identify unspecified interaction effects, complementing the class of location-tests designed for detecting main effects only. In both settings, we show that the two classes of tests are asymptotically independent of each other under the global null hypothesis. Thus, we can evaluate the significance of the resulting Fisher’s test statistic using the chi-squared distribution with four degrees of freedom; this is a desirable feature for analyzing big data. In addition to analytical results, we provide empirical evidence to show that the new class of joint test is not only robust but can also have better power than the individual tests. This is based on join work with formal graduate students Andriy Derkach (Derkach et al. 2013, Genetic Epidemiology; Derkach et al. 2014, Statistical Science) and David Soave (Soave et al. 2015, The American Journal of Human Genetics; Soave and Sun 2017, Biometrics).&lt;/p&gt;</description>
    </item>
    <item>
      <title>A log-linear time algorithm for constrained changepoint detection</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-11-17/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-11-17/</guid>
      <description>&lt;h4 id=&#34;date-2017-11-17&#34;&gt;Date: 2017-11-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Changepoint detection is a central problem in time series and genomic data. For some applications, it is natural to impose constraints on the directions of changes. One example is ChIP-seq data, for which adding an up-down constraint improves peak detection accuracy, but makes the optimization problem more complicated. In this talk I will explain how a recently proposed functional pruning algorithm can be generalized to solve such constrained changepoint detection problems. Our proposed log-linear time algorithm achieves state-of-the-art peak detection accuracy in a benchmark of several genomic data sets, and is orders of magnitude faster than our previous quadratic time algorithm. Our implementation is available as the PeakSegPDPA function in the PeakSegOptimal R package, &lt;a href=&#34;https://cran.r-project.org/package=PeakSegOptimal&#34;&gt;https://cran.r-project.org/package=PeakSegOptimal&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>PAC-Bayesian Generalizations Bounds for Deep Neural Networks</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-11-10/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-11-10/</guid>
      <description>&lt;h4 id=&#34;date-2017-11-10&#34;&gt;Date: 2017-11-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this &amp;ldquo;deep learning&amp;rdquo; regime. Logically, in order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization. Time permitting, I will discuss recent work on computing even tighter generalization bounds associated with a learning algorithm introduced by Chaudhari et al. (2017), called Entropy-SGD. We show that Entropy-SGD indirectly optimizes a PAC-Bayes bound, but does so by optimizing the &amp;ldquo;prior&amp;rdquo; term, violating the hypothesis that the prior be independent of the data. We show how to fix this defect using differential privacy. The result is a new PAC-Bayes bound for data-dependent priors, which we show, up to some approximations, delivers even tighter generalization bounds. Joint work with Gintare Karolina Dziugaite, based on &lt;a href=&#34;https://arxiv.org/abs/1703.11008&#34;&gt;https://arxiv.org/abs/1703.11008&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to do statistics</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-11-03/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-11-03/</guid>
      <description>&lt;h4 id=&#34;date-2017-11-03&#34;&gt;Date: 2017-11-03&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, I will outline how to do (Bayesian) statistics. I will focus particularly on the things that need to be done before you see data, including prior specification and checking that your inference algorithm actually works.&lt;/p&gt;&#xA;&lt;h2 id=&#34;speaker&#34;&gt;Speaker&lt;/h2&gt;&#xA;&lt;p&gt;Daniel Simpson is an Assistant Professor in the Department of Statistical Sciences, University of Toronto&lt;/p&gt;</description>
    </item>
    <item>
      <title>Penalized robust regression estimation with applications to proteomics</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-10-27/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-10-27/</guid>
      <description>&lt;h4 id=&#34;date-2017-10-27&#34;&gt;Date: 2017-10-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In many current applications, scientists can easily measure a very large number of variables (for example, hundreds of protein levels), some of which are expected be useful to explain or predict a specific response variable of interest. These potential explanatory variables are most likely to contain redundant or irrelevant information, and in many cases, their quality and reliability may be suspect. We developed two penalized robust regression estimators that can be used to identify a useful subset of explanatory variables to predict the response, while protecting the resulting estimator against possible aberrant observations in the data set. Using an elastic net penalty, the proposed estimator can be used to select variables, even in cases with more variables than observations or when many of the candidate explanatory variables are correlated. In this talk, I will present the new estimator and an algorithm to compute it. I will also illustrate its performance in a simulation study and a real data set. This is joint work with Professor Matias Salibian-Barrera, my PhD student David Kepplinger, and my PDF Ezequiel Smuggler.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical optimization and nonasymptotic robustness</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-10-20/</link>
      <pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-10-20/</guid>
      <description>&lt;h4 id=&#34;date-2017-10-20&#34;&gt;Date: 2017-10-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statistical optimization has generated quite some interest recently. It refers to the case where hidden and local convexity can be discovered in most cases for nonconvex problems, making polynomial algorithms possible. It relies on a careful analysis of the geometry near global optima. In this talk, I will explore this issue by focusing on sparse regression problems in high dimensions. A computational framework named iterative local adaptive majorize-minimization (I-LAMM) will be proposed to simultaneously control algorithmic complexity and statistical error. I-LAMM effectively turns the nonconvex penalized regression problem into a series of convex programs by utilizing the locally strong convexity of the problem when restricting the solution set in an L_1 cone. Computationally, we establish a phase transition phenomenon: it enjoys a linear rate of convergence after a sub-linear burn-in. Statistically, it provides solutions with optimal statistical errors. Extensions to robust regression will be discussed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantifying spatial flood risks: A comparative study of max-stable models</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-10-13/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-10-13/</guid>
      <description>&lt;h4 id=&#34;date-2017-10-13&#34;&gt;Date: 2017-10-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In various applications, evaluating spatial risks (such as floods, heatwaves or storms) is a key problem. The aim of this talk is to make use of extreme value theory and max-stable processes to provide quantitative answers to this issue. A review of the literature will be provided, as well as a wide comparative study based on a simulation design mimicking daily rainfall in France. This is a joint work with Cécile Mercadier (Université Claude-Bernard Lyon 1 (UCBL)) and Quentin Sebille (UCBL).&lt;/p&gt;</description>
    </item>
    <item>
      <title>BET on independence</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-09-22/</link>
      <pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-09-22/</guid>
      <description>&lt;h4 id=&#34;date-2017-09-22&#34;&gt;Date: 2017-09-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1400-1500&#34;&gt;Time: 14:00-15:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-bronf179&#34;&gt;Location: BRONF179&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We study the problem of nonparametric dependence detection. Many existing methods suffer severe power loss due to non-uniform consistency, which we illustrate with a paradox. To avoid such power loss, we approach the nonparametric test of independence through the new framework of binary expansion statistics (BEStat) and binary expansion testing (BET), which examine dependence through a filtration induced by marginal binary expansions. Through a novel decomposition of the likelihood of contingency tables whose sizes are powers of 2, we show that the interactions of binary variables in the filtration are complete sufficient statistics for dependence. These interactions are also pairwise independent under the null. By utilizing these interactions, the BET avoids the problem of non-uniform consistency and improves upon a wide class of commonly used methods (a) by achieving the optimal rate in sample complexity and (b) by providing clear interpretations of global and local relationships upon rejection of independence. The binary expansion approach also connects the test statistics with the current computing system to allow efficient bitwise implementation. We illustrate the BET by a study of the distribution of stars in the night sky and by an exploratory data analysis of the TCGA breast cancer data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Our quest for robust time series forecasting at scale</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-09-15/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-09-15/</guid>
      <description>&lt;h4 id=&#34;date-2017-09-15&#34;&gt;Date: 2017-09-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The demand for time series forecasting at Google has grown rapidly along with the company since its founding. Initially, the various business and engineering needs led to a multitude of forecasting approaches, most reliant on direct analyst support. The volume and variety of the approaches, and in some cases their inconsistency, called out for an attempt to unify, automate, and extend forecasting methods, and to distribute the results via tools that could be deployed reliably across the company. That is, for an attempt to develop methods and tools that would facilitate accurate large-scale time series forecasting at Google. We were part of a team of data scientists in Search Infrastructure at Google that took on the task of developing robust and automatic large-scale time series forecasting for our organization. In this talk, we recount how we approached the task, describing initial stakeholder needs, the business and engineering contexts in which the challenge arose, and theoretical and pragmatic choices we made to implement our solution. We describe our general forecasting framework, offer details on various tractable subproblems into which we decomposed our overall forecasting task, and provide an example of our forecasting routine applied to publicly available Turkish Electricity data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Genomics like it&#39;s 1960: Inferring human history</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-09-08/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-09-08/</guid>
      <description>&lt;h4 id=&#34;date-2017-09-08&#34;&gt;Date: 2017-09-08&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A central goal of population genetics is the inference of the biological, evolutionary and demographic forces that shaped human diversity. Large-scale sequencing experiments provide fantastic opportunities to learn about human history and biology if we can overcome computational and statistical challenges. I will discuss how simple mid-century statistical approaches, such as the jackknife and Kolmogorov equations, can be combined in unexpected ways to solve partial differential equations, optimize genomic study design, and learn about the spread of modern humans since our common African origins.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Distributed kernel regression for large-scale data</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-03-31/</link>
      <pubDate>Fri, 31 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-03-31/</guid>
      <description>&lt;h4 id=&#34;date-2017-03-31&#34;&gt;Date: 2017-03-31&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In modern scientific research, massive datasets with huge numbers of observations are frequently encountered. To facilitate the computational process, a divide-and-conquer scheme is often used for the analysis of big data. In such a strategy, a full dataset is first split into several manageable segments; the final output is then aggregated from the individual outputs of the segments. Despite its popularity in practice, it remains largely unknown that whether such a distributive strategy provides valid theoretical inferences to the original data; if so, how efficient does it work? In this talk, I address these fundamental issues for the non-parametric distributed kernel regression, where accurate prediction is the main learning task. I will begin with the naive simple averaging algorithm and then talk about an improved approach via ADMM. The promising preference of these methods is supported by both simulation and real data examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian sample size determination for clinical trials</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-03-24/</link>
      <pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-03-24/</guid>
      <description>&lt;h4 id=&#34;date-2017-03-24&#34;&gt;Date: 2017-03-24&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Sample size determination problem is an important task in the planning of clinical trials. The problem may be formulated formally in statistical terms. The most frequently used methods are based on the required size, and power of the trial for a specified treatment effect. In contrast to the Bayesian decision-theoretic approach, there is no explicit balancing of the cost of a possible increase in the size of the trial against the benefit of the more accurate information which it would give. In this talk a fully Bayesian approach to the sample size determination problem is discussed. This approach treats the problem as a decision problem and employs a utility function to find the optimal sample size of a trial. Furthermore, we assume that a regulatory authority, which is deciding on whether or not to grant a licence to a new treatment, uses a frequentist approach. The optimal sample size for the trial is then found by maximising the expected net benefit, which is the expected benefit of subsequent use of the new treatment minus the cost of the trial.&lt;/p&gt;</description>
    </item>
    <item>
      <title>High-throughput single-cell biology: The challenges and opportunities for machine learning scientists</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-03-10/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-03-10/</guid>
      <description>&lt;h4 id=&#34;date-2017-03-10&#34;&gt;Date: 2017-03-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The immune system does a lot more than killing “foreign” invaders. It’s a powerful sensory system that can detect stress levels, infections, wounds, and even cancer tumors. However, due to the complex interplay between different cell types and signaling pathways, the amount of data produced to characterize all different aspects of the immune system (tens of thousands of genes measured and hundreds of millions of cells, just from a single patient) completely overwhelms existing bioinformatics tools. My laboratory specializes in the development of machine learning techniques that address the unique challenges of high-throughput single-cell immunology. Sharing our lab space with a clinical and an immunological research laboratory, my students and fellows are directly exposed to the real-world challenges and opportunities of bringing machine learning and immunology to the (literal) bedside.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The first pillar of statistical wisdom</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-02-24/</link>
      <pubDate>Fri, 24 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-02-24/</guid>
      <description>&lt;h4 id=&#34;date-2017-02-24&#34;&gt;Date: 2017-02-24&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This talk will provide an introduction to the first of the pillars in Stephen Stigler&amp;rsquo;s 2016 book The Seven Pillars of Statistical Wisdom, namely “Aggregation.” It will focus on early instances of the sample mean in scientific work, on the early error distributions, and on how their “centres” were fitted.&lt;/p&gt;&#xA;&lt;h2 id=&#34;speaker&#34;&gt;Speaker&lt;/h2&gt;&#xA;&lt;p&gt;James A. Hanley is a Professor in the Department of Epidemiology, Biostatistics and Occupational Health, at McGill University.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building end-to-end dialogue systems using deep neural architectures</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-02-17/</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-02-17/</guid>
      <description>&lt;h4 id=&#34;date-2017-02-17&#34;&gt;Date: 2017-02-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The ability for a computer to converse in a natural and coherent manner with a human has long been held as one of the important steps towards solving artificial intelligence. In this talk I will present recent results on building dialogue systems from large corpuses using deep neural architectures. I will highlight several challenges related to data acquisition, algorithmic development, and performance evaluation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sparse envelope model: Efficient estimation and response variable selection in multivariate linear regression</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-02-10/</link>
      <pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-02-10/</guid>
      <description>&lt;h4 id=&#34;date-2017-02-10&#34;&gt;Date: 2017-02-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The envelope model is a method for efficient estimation in multivariate linear regression. In this article, we propose the sparse envelope model, which is motivated by applications where some response variables are invariant to changes of the predictors and have zero regression coefficients. The envelope estimator is consistent but not sparse, and in many situations it is important to identify the response variables for which the regression coefficients are zero. The sparse envelope model performs variable selection on the responses and preserves the efficiency gains offered by the envelope model. Response variable selection arises naturally in many applications, but has not been studied as thoroughly as predictor variable selection. In this article, we discuss response variable selection in both the standard multivariate linear regression and the envelope contexts. In response variable selection, even if a response has zero coefficients, it still should be retained to improve the estimation efficiency of the nonzero coefficients. This is different from the practice in predictor variable selection. We establish consistency, the oracle property and obtain the asymptotic distribution of the sparse envelope estimator.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MM algorithms for variance component models</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-02-03/</link>
      <pubDate>Fri, 03 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-02-03/</guid>
      <description>&lt;h4 id=&#34;date-2017-02-03&#34;&gt;Date: 2017-02-03&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Variance components estimation and mixed model analysis are central themes in statistics with applications in numerous scientific disciplines. Despite the best efforts of generations of statisticians and numerical analysts, maximum likelihood estimation and restricted maximum likelihood estimation of variance component models remain numerically challenging. In this talk, we present a novel iterative algorithm for variance components estimation based on the minorization-maximization (MM) principle. MM algorithm is trivial to implement and competitive on large data problems. The algorithm readily extends to more complicated problems such as linear mixed models, multivariate response models possibly with missing data, maximum a posteriori estimation, and penalized estimation. We demonstrate, both numerically and theoretically, that it converges faster than the classical EM algorithm when the number of variance components is greater than two.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Order selection in multidimensional finite mixture models</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-01-20/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-01-20/</guid>
      <description>&lt;h4 id=&#34;date-2017-01-20&#34;&gt;Date: 2017-01-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Finite mixture models provide a natural framework for analyzing data from heterogeneous populations. In practice, however, the number of hidden subpopulations in the data may be unknown. The problem of estimating the order of a mixture model, namely the number of subpopulations, is thus crucial for many applications. In this talk, we present a new penalized likelihood solution to this problem, which is applicable to models with a multidimensional parameter space. The order of the model is estimated by starting with a large number of mixture components, which are clustered and then merged via two penalty functions. Doing so estimates the unknown parameters of the mixture, at the same time as the order. We will present extensive simulation studies, showing our approach outperforms many of the most common methods for this problem, such as the Bayesian Information Criterion. Real data examples involving normal and multinomial mixtures further illustrate its performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Sparse) exchangeable graphs</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-01-13/</link>
      <pubDate>Fri, 13 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-01-13/</guid>
      <description>&lt;h4 id=&#34;date-2017-01-13&#34;&gt;Date: 2017-01-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Many popular statistical models for network valued datasets fall under the remit of the graphon framework, which (implicitly) assumes the networks are densely connected. However, this assumption rarely holds for the real-world networks of practical interest. We introduce a new class of models for random graphs that generalises the dense graphon models to the sparse graph regime, and we argue that this meets many of the desiderata one would demand of a model to serve as the foundation for a statistical analysis of real-world networks. The key insight is to define the models by way of a novel notion of exchangeability; this is analogous to the specification of conditionally i.i.d. models by way of de Finetti&amp;rsquo;s representation theorem. We further develop this model class by explaining the foundations of sampling and estimation of network models in this setting. The later result can be can be understood as the (sparse) graph analogue of estimation via the empirical distribution in the i.i.d. sequence setting.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modeling dependence in bivariate multi-state processes: A frailty approach</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-12-02/</link>
      <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-12-02/</guid>
      <description>&lt;h4 id=&#34;date-2016-12-02&#34;&gt;Date: 2016-12-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The aim of this talk is to present a statistical framework for the analysis of dependent bivariate multistate processes, allowing one to study the dependence both across subjects in a pair and among individual-specific events. As for the latter, copula- based models are employed, whereas dependence between multi-state models can be accomplished by means of frailties. The well known Marshall-Olkin Bivariate Exponential Distribution (MOBVE) is considered for the joint distribution of frailties. The reason is twofold: on the one hand, it allows one to model shocks that affect the two individual-specific frailties; on the other hand, the MOBVE is the only bivariate exponential distribution with exponential marginals, which allows for the modeling of each multi-state process as a shared frailty model. We first discuss a frailty bivariate survival model with some new results, and then move to the construction of the frailty bivariate multi-state model, with the corresponding observed data likelihood maximization estimating procedure in presence of right censoring. The last part of the talk will be dedicated to some open problems related to the modeling of multiple multi-state processes in presence of Marshall-Olkin type copulas.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spatio-temporal models for skewed processes</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-11-25/</link>
      <pubDate>Fri, 25 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-11-25/</guid>
      <description>&lt;h4 id=&#34;date-2016-11-25&#34;&gt;Date: 2016-11-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the analysis of most spatio-temporal processes in environmental studies, observations present skewed distributions. Usually, a single transformation of the data is used to approximate normality, and stationary Gaussian processes are assumed to model the transformed data. The choice of transformation is key for spatial interpolation and temporal prediction. We propose a spatio-temporal model for skewed data that does not require the use of data transformation. The process is decomposed as the sum of a purely temporal structure with two independent components that are considered to be partial realizations from independent spatial Gaussian processes, for each time t. The model has an asymmetry parameter that might vary with location and time, and if this is equal to zero, the usual Gaussian model results. The inference procedure is performed under the Bayesian paradigm, and uncertainty about parameters estimation is naturally accounted for. We fit our model to different synthetic data and to monthly average temperature observed between 2001 and 2011 at monitoring locations located in the south of Brazil. Different model comparison criteria, and analysis of the posterior distribution of some parameters, suggest that the proposed model outperforms standard ones used in the literature. This is joint work with Kelly Gonçalves (UFRJ, Brazil) and Patricia L. Velozo (UFF, Brazil)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Progress in theoretical understanding of deep learning</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-11-18/</link>
      <pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-11-18/</guid>
      <description>&lt;h4 id=&#34;date-2016-11-18&#34;&gt;Date: 2016-11-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Deep learning has arisen around 2006 as a renewal of neural networks research allowing such models to have more layers. Theoretical investigations have shown that functions obtained as deep compositions of simpler functions (which includes both deep and recurrent nets) can express highly varying functions (with many ups and downs and different input regions that can be distinguished) much more efficiently (with fewer parameters) than otherwise, under a prior which seems to work well for artificial intelligence tasks. Empirical work in a variety of applications has demonstrated that, when well trained, such deep architectures can be highly successful, remarkably breaking through previous state-of-the-art in many areas, including speech recognition, object recognition, language models, machine translation and transfer learning. Although neural networks have long been considered lacking in theory and much remains to be done, theoretical advances have been made and will be discussed, to support distributed representations, depth of representation, the non-convexity of the training objective, and the probabilistic interpretation of learning algorithms (especially of the auto-encoder type, which were lacking one). The talk will focus on the intuitions behind these theoretical results.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tyler&#39;s M-estimator: Subspace recovery and high-dimensional regime</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-11-11/</link>
      <pubDate>Fri, 11 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-11-11/</guid>
      <description>&lt;h4 id=&#34;date-2016-11-11&#34;&gt;Date: 2016-11-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Given a data set, Tyler&amp;rsquo;s M-estimator is a widely used covariance matrix estimator with robustness to outliers or heavy-tailed distribution. We will discuss two recent results of this estimator. First, we show that when a certain percentage of the data points are sampled from a low-dimensional subspace, Tyler&amp;rsquo;s M-estimator can be used to recover the subspace exactly. Second, in the high-dimensional regime that the number of samples n and the dimension p both go to infinity, p/n converges to a constant y between 0 and 1, and when the data samples are identically and independently generated from the Gaussian distribution N(0,I), we showed that the difference between the sample covariance matrix and a scaled version of Tyler&amp;rsquo;s M-estimator tends to zero in spectral norm, and the empirical spectral densities of both estimators converge to the Marcenko-Pastur distribution. We also prove that when the data samples are generated from an elliptical distribution, the limiting distribution of Tyler&amp;rsquo;s M-estimator converges to a Marcenko-Pastur-Type distribution. The second part is joint work with Xiuyuan Cheng and Amit Singer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lawlor: Time-varying mixtures of Markov chains: An application to traffic modeling Piché: Bayesian nonparametric modeling of heterogeneous groups of censored data</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-11-04/</link>
      <pubDate>Fri, 04 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-11-04/</guid>
      <description>&lt;h4 id=&#34;date-2016-11-04&#34;&gt;Date: 2016-11-04&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Piché&lt;/em&gt;: Analysis of survival data arising from different groups, whereby the data in each group is scarce, but abundant overall, is a common issue in applied statistics. Bayesian nonparametrics are tools of choice to handle such datasets given their ability to share information across groups. In this presentation, we will compare three popular Bayesian nonparametric methods on the modeling of survival functions coming from related heterogeneous groups. Specifically, we will first compare the modeling accuracy of the Dirichlet process, the hierarchical Dirichlet process, and the nested Dirichlet process on simulated datasets of different sizes, where groups differ in shape or in expectation, and finally we will compare the models on real world injury datasets.&lt;/p&gt;</description>
    </item>
    <item>
      <title>First talk: Bootstrap in practice | Second talk: Statistics and Big Data at Google</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-11-02/</link>
      <pubDate>Wed, 02 Nov 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-11-02/</guid>
      <description>&lt;h4 id=&#34;date-2016-11-02&#34;&gt;Date: 2016-11-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1500-1600-1735-1825&#34;&gt;Time: 15:00-16:00 17:35-18:25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-1st-burn-306-2nd-adams-aud&#34;&gt;Location: 1st: BURN 306 2nd: ADAMS AUD&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;First talk&lt;/em&gt;: This talk focuses on three practical aspects of resampling: communication, accuracy, and software. I&amp;rsquo;ll introduce the bootstrap and permutation tests, and discussed how they may be used to help clients understand statistical results. I&amp;rsquo;ll talk about accuracy &amp;ndash; there are dramatic differences in how accurate different bootstrap methods are. Surprisingly, the most common bootstrap methods are less accurate than classical methods for small samples, and more accurate for larger samples. There are simple variations that dramatically improve the accuracy. Finally, I&amp;rsquo;ll compare two R packages, the the easy-to-use &amp;ldquo;resample&amp;rdquo; package, and the more-powerful &amp;ldquo;boot&amp;rdquo; package.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical analysis of two-level hierarchical clustered data</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-10-21/</link>
      <pubDate>Fri, 21 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-10-21/</guid>
      <description>&lt;h4 id=&#34;date-2016-10-21&#34;&gt;Date: 2016-10-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Multi-level hierarchical clustered data are commonly seen in financial and biostatistics applications. In this talk, we introduce several modeling strategies for describing the dependent relationships for members within a cluster or between different clusters (in the same or different levels). In particular we will apply the hierarchical Kendall copula, first proposed by Brechmann (2014), to model two-level hierarchical clustered survival data. This approach provides a clever way of dimension reduction in modeling complicated multivariate data. Based on the model assumptions, we propose statistical inference methods, including parameter estimation and a goodness-of-fit test, suitable for handling censored data. Simulation and data analysis results are also presented.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Bayesian finite mixture of bivariate regressions model for causal mediation analyses</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-10-14/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-10-14/</guid>
      <description>&lt;h4 id=&#34;date-2016-10-14&#34;&gt;Date: 2016-10-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Building on the work of Schwartz, Gelfand and Miranda (Statistics in Medicine (2010); 29(16), 1710-23), we propose a Bayesian finite mixture of bivariate regressions model for causal mediation analyses. Using an identifiability condition within each component of the mixture, we express the natural direct and indirect effects of the exposure on the outcome as functions of the component-specific regression coefficients. On the basis of simulated data, we examine the behaviour of the model for estimating these effects in situations where the associations between exposure, mediator and outcome are confounded, or not. Additionally, we demonstrate that this mixture model can be used to account for heterogeneity arising through unmeasured binary mediator-outcome confounders. Finally, we apply our mediation mixture model to estimate the natural direct and indirect effects of exposure to inhaled corticosteroids during pregnancy on birthweight using a cohort of asthmatic women from the province of Québec.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cellular tree classifiers</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-10-07/</link>
      <pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-10-07/</guid>
      <description>&lt;h4 id=&#34;date-2016-10-07&#34;&gt;Date: 2016-10-07&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Suppose that binary classification is done by a tree method in which the leaves of a tree correspond to a partition of d-space. Within a partition, a majority vote is used. Suppose furthermore that this tree must be constructed recursively by implementing just two functions, so that the construction can be carried out in parallel by using &amp;ldquo;cells&amp;rdquo;: first of all, given input data, a cell must decide whether it will become a leaf or internal node in the tree. Secondly, if it decides on an internal node, it must decide how to partition the space linearly. Data are then split into two parts and sent downstream to two new independent cells. We discuss the design and properties of such classifiers.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CoCoLasso for high-dimensional error-in-variables regression</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-09-30/</link>
      <pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-09-30/</guid>
      <description>&lt;h4 id=&#34;date-2016-09-30&#34;&gt;Date: 2016-09-30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Much theoretical and applied work has been devoted to high-dimensional regression with clean data. However, we often face corrupted data in many applications where missing data and measurement errors cannot be ignored. Loh and Wainwright (2012) proposed a non-convex modification of the Lasso for doing high-dimensional regression with noisy and missing data. It is generally agreed that the virtues of convexity contribute fundamentally the success and popularity of the Lasso. In light of this, we propose a new method named CoCoLasso that is convex and can handle a general class of corrupted datasets including the cases of additive measurement error and random missing data. We establish the estimation error bounds of CoCoLasso and its asymptotic sign-consistent selection property. We further elucidate how the standard cross validation techniques can be misleading in presence of measurement error and develop a novel corrected cross-validation technique by using the basic idea in CoCoLasso. The corrected cross-validation has its own importance. We demonstrate the superior performance of our method over the non-convex approach by simulation studies.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stein estimation of the intensity parameter of a stationary spatial Poisson point process</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-09-23/</link>
      <pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-09-23/</guid>
      <description>&lt;h4 id=&#34;date-2016-09-23&#34;&gt;Date: 2016-09-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We revisit the problem of estimating the intensity parameter of a homogeneous Poisson point process observed in a bounded window of $R^d$ making use of a (now) old idea going back to James and Stein. For this, we prove an integration by parts formula for functionals defined on the Poisson space. This formula extends the one obtained by Privault and Réveillac (Statistical inference for Stochastic Processes, 2009) in the one-dimensional case and is well-suited to a notion of derivative of Poisson functionals which satisfy the chain rule. The new estimators can be viewed as biased versions of the MLE with a tailored-made bias designed to reduce the variance of the MLE. We study a large class of examples and show that with a controlled probability the corresponding estimator outperforms the MLE. We illustrate in a simulation study that for very reasonable practical cases (like an intensity of 10 or 20 of a Poisson point process observed in the d-dimensional euclidean ball of with d = 1, &amp;hellip;, 5), we can obtain a relative (mean squared error) gain above 20% for the Stein estimator with respect to the maximum likelihood. This is a joint work with M. Clausel and J. Lelong (Univ. Grenoble Alpes, France).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Two-set canonical variate model in multiple populations with invariant loadings</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-09-09/</link>
      <pubDate>Fri, 09 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-09-09/</guid>
      <description>&lt;h4 id=&#34;date-2016-09-09&#34;&gt;Date: 2016-09-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Goria and Flury (Definition 2.1, 1996) proposed the two-set canonical variate model (referred to as the CV-2 model hereafter) and its extension in multiple populations with invariant weight coefficients (Definition 2.2). The equality constraints imposed on the weight coefficients are in line with the approach to interpreting the canonical variates (i.e., the linear combinations of original variables) advocated by Harris (1975, 1989), Rencher (1988, 1992), and Rencher and Christensen (2003). However, the literature in psychology and education shows that the standard approach adopted by most researchers, including Anderson (2003), is to use the canonical loadings (i.e., the correlations between the canonical variates and the original variables in the same set) to interpret the canonical variates. In case of multicollinearity (giving rise to the so-called suppression effects) among the original variables, it is not uncommon to obtain different interpretations from the two approaches. Therefore, following the standard approach in practice, an alternative (probably more realistic) extension of Goria and Flury’s CV-2 model in multiple populations is to impose the equality constraints on the canonical loadings. The utility of this multiple-population extension are illustrated with two numeric examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multivariate tests of associations based on univariate tests</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-04-08/</link>
      <pubDate>Fri, 08 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-04-08/</guid>
      <description>&lt;h4 id=&#34;date-2016-04-08&#34;&gt;Date: 2016-04-08&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;For testing two random vectors for independence, we consider testing whether the distance of one vector from an arbitrary center point is independent from the distance of the other vector from an arbitrary center point by a univariate test. We provide conditions under which it is enough to have a consistent univariate test of independence on the distances to guarantee that the power to detect dependence between the random vectors increases to one, as the sample size increases. These conditions turn out to be minimal. If the univariate test is distribution-free, the multivariate test will also be distribution-free. If we consider multiple center points and aggregate the center-specific univariate tests, the power may be further improved. We suggest a specific aggregation method for which the resulting multivariate test will be distribution-free if the univariate test is distribution-free. We show that several multivariate tests recently proposed in the literature can be viewed as instances of this general approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Asymptotic behavior of binned kernel density estimators for locally non-stationary random fields</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-04-01/</link>
      <pubDate>Fri, 01 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-04-01/</guid>
      <description>&lt;h4 id=&#34;date-2016-04-01&#34;&gt;Date: 2016-04-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, I will describe the finite- and large-sample behavior of binned kernel density estimators for dependent and locally non-stationary random fields converging to stationary random fields. In addition to looking at the bias and asymptotic normality of the estimators, I will present results from a simulation study which shows that the kernel density estimator and the binned kernel density estimator have the same behavior and both estimate accurately the true density when the number of fields increases. This work finds applications in various fields, including the study of epidemics and mining research. My specific illustration will be concerned with the 2002 incidence rates of tuberculosis in the departments of France.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Robust minimax shrinkage estimation of location vectors under concave loss</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-03-18/</link>
      <pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-03-18/</guid>
      <description>&lt;h4 id=&#34;date-2016-03-18&#34;&gt;Date: 2016-03-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider the problem of estimating the mean vector, q, of a multivariate spherically symmetric&#xA;distribution under a loss function which is a concave function of squared error. In particular we find&#xA;conditions on the shrinkage factor under which Stein-type shrinkage estimators dominate the usual&#xA;minimax best equivariant estimator. In problems where the scale is known, minimax shrinkage factors&#xA;which generally depend on both the loss and the sampling distribution are found. When the scale is&#xA;estimated through the squared norm of a residual vector, for a large subclass of concave losses, we find&#xA;minimax shrinkage factors which are independent of both the loss and the underlying distribution.&#xA;Recent applications in predictive density estimation are examples where such losses arise naturally.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Nonparametric graphical models: Foundation and trends</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-03-11/</link>
      <pubDate>Fri, 11 Mar 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-03-11/</guid>
      <description>&lt;h4 id=&#34;date-2016-03-11&#34;&gt;Date: 2016-03-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider the problem of learning the structure of a non-Gaussian graphical model. We introduce two strategies for constructing tractable nonparametric graphical model families. One approach is through semiparametric extension of the Gaussian or exponential family graphical models that allows arbitrary graphs. Another approach is to restrict the family of allowed graphs to be acyclic, enabling the use of fully nonparametric density estimation in high dimensions. These two approaches can both be viewed as adding structural regularization to a general pairwise nonparametric Markov random field and reflect an interesting tradeoff of model flexibility with structural complexity. In terms of graph estimation, these methods achieve the optimal parametric rates of convergence. In terms of computation, these methods are as scalable as the best implemented parametric methods. Such a &amp;ldquo;free-lunch phenomenon&amp;rdquo; makes them extremely attractive for large-scale applications. We will also introduce several new research directions along this line of work, including latent-variable extension, model-based nonconvex optimization, graph uncertainty assessment, and nonparametric graph property testing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Aggregation methods for portfolios of dependent risks with Archimedean copulas</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-02-26/</link>
      <pubDate>Fri, 26 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-02-26/</guid>
      <description>&lt;h4 id=&#34;date-2016-02-26&#34;&gt;Date: 2016-02-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, we will consider a portfolio of dependent risks represented by a vector of dependent random variables whose joint cumulative distribution function (CDF) is defined with an Archimedean copula. Archimedean copulas are very popular and their extensions, nested Archimedean copulas, are well suited for vectors of random vectors in high dimension. I will describe a simple approach which makes it possible to compute the CDF of the sum or a variety of other functions of those random variables. In particular, I will derive the CDF and the TVaR of the sum of those risks using the Frank copula, the Shifted Negative Binomial copula, and the Ali-Mikhail-Haq (AMH) copula. The computation of the contribution of each risk under the TVaR-based allocation rule will also be illustrated. Finally, the links between the Clayton copula, the Shifted Negative Binomial copula, and the AMH copula will be discussed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An introduction to statistical lattice models and observables</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-02-19/</link>
      <pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-02-19/</guid>
      <description>&lt;h4 id=&#34;date-2016-02-19&#34;&gt;Date: 2016-02-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The study of convergence of random walks to well defined curves is founded in the fields of complex analysis, probability theory, physics and combinatorics. The foundations of this subject were motivated by physicists interested in the properties of one-dimensional models that represented some form of physical phenomenon. By taking physical models and generalizing them into abstract mathematical terms, macroscopic properties about the model could be determined from the microscopic level. By using model specific objects known as observables, the convergence of the random walks on particular lattice structures can be proven to converge to continuous curves such as Brownian Motion or Stochastic Loewner Evolution as the size of the lattice step approaches 0. This seminar will introduce the field of statistical lattice models, the types of observables that can be used to prove convergence as well as a proof for the q-state Potts model showing that local non-commutative matrix observables do not exist. No prior physics knowledge is required for this seminar.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Bayesian causal effect estimation algorithm</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-02-05/</link>
      <pubDate>Fri, 05 Feb 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-02-05/</guid>
      <description>&lt;h4 id=&#34;date-2016-02-05&#34;&gt;Date: 2016-02-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1214&#34;&gt;Location: BURN 1214&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Estimating causal exposure effects in observational studies ideally requires the analyst to have a vast knowledge of the domain of application. Investigators often bypass difficulties related to the identification and selection of confounders through the use of fully adjusted outcome regression models. However, since such models likely contain more covariates than required, the variance of the regression coefficient for exposure may be unnecessarily large. Instead of using a fully adjusted model, model selection can be attempted. Most classical statistical model selection approaches, such as Bayesian model averaging, do not readily address causal effect estimation. We present a new model averaged approach to causal inference, Bayesian causal effect estimation (BCEE), which is motivated by the graphical framework for causal inference. BCEE aims to unbiasedly estimate the causal effect of a continuous exposure on a continuous outcome while being more efficient than a fully adjusted approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Estimating high-dimensional networks with hubs with an application to microbiome data</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-01-29/</link>
      <pubDate>Fri, 29 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-01-29/</guid>
      <description>&lt;h4 id=&#34;date-2016-01-29&#34;&gt;Date: 2016-01-29&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, we investigate the problem of estimating high-dimensional networks in which there are a few highly connected “hub&amp;quot; nodes. Methods based on L1-regularization have been widely used for performing sparse selection in the graphical modelling context. However, the L1 penalty penalizes each edge equally and independently of each other without taking into account any structural information. We introduce a new method for estimating undirected graphical models with hubs, called the hubs weighted graphical lasso (HWGL). This is a two-step procedure with a hub screening step, followed by network reconstruction in the second step using a weighted lasso approach that incorporates the inferred network topology. Empirically, we show that the HWGL outperforms competing methods and illustrate the methodology with an application to microbiome data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Robust estimation in the presence of influential units in surveys</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-01-22/</link>
      <pubDate>Fri, 22 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-01-22/</guid>
      <description>&lt;h4 id=&#34;date-2016-01-22&#34;&gt;Date: 2016-01-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Influential units are those which make classical estimators (e.g., the Horvitz-Thompson estimator or calibration estimators) very unstable. The problem of influential units is particularly important in business surveys, which collect economic variables, whose distribution are highly skewed (heavy right tail). In this talk, we will attempt to answer the following questions:&lt;/p&gt;&#xA;&lt;p&gt;(1)   What is an influential value in surveys?&#xA;(2)   How measure the influence of unit?&#xA;(3)   How reduce the impact of influential units at the estimation stage?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prevalent cohort studies: Length-biased sampling with right censoring</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-11-13/</link>
      <pubDate>Fri, 13 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-11-13/</guid>
      <description>&lt;h4 id=&#34;date-2015-11-13&#34;&gt;Date: 2015-11-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Logistic or other constraints often preclude the possibility of conducting incident cohort studies. A feasible alternative in such cases is to conduct a cross-sectional prevalent cohort study for which we recruit prevalent cases, i.e., subjects who have already experienced the initiating event, say the onset of a disease. When the interest lies in estimating the lifespan between the initiating event and a terminating event, say death for instance, such subjects may be followed prospectively until the terminating event or loss to follow-up, whichever happens first. It is well known that prevalent cases have, on average, longer lifespans. As such, they do not form a representative random sample from the target population; they comprise a biased sample. If the initiating events are generated from a stationary Poisson process, the so-called stationarity assumption, this bias is called length bias. I present the basics of nonparametric inference using length-biased right censored failure time data. I&amp;rsquo;ll then discuss some recent progress and current challenges. Our study is mainly motivated by challenges and questions raised in analyzing survival data collected on patients with dementia as part of a nationwide study in Canada, called the Canadian Study of Health and Aging (CSHA). I&amp;rsquo;ll use these data throughout the talk to discuss and motivate our methodology and its applications.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian analysis of non-identifiable models, with an example from epidemiology and biostatistics</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-11-06/</link>
      <pubDate>Fri, 06 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-11-06/</guid>
      <description>&lt;h4 id=&#34;date-2015-11-06&#34;&gt;Date: 2015-11-06&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Most regression models in biostatistics assume identifiability, which means that each point in the parameter space corresponds to a unique likelihood function for the observable data. Recently there has been interest in Bayesian inference for non-identifiable models, which can better represent uncertainty in some contexts. One example is in the field of epidemiology, where the investigator is concerned with bias due to unmeasured confounders (omitted variables). In this talk, I will illustrate Bayesian analysis of a non-identifiable model from epidemiology using government administrative data from British Columbia. I will show how to use the software STAN, which is new software developed by Andrew Gelman and others in the USA. STAN allows the careful study of posterior distributions in a vast collection of Bayesian models, including non-identifiable models for bias in epidemiology, which are poorly suited to conventional Gibbs sampling.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Robust mixture regression and outlier detection via penalized likelihood</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-10-23/</link>
      <pubDate>Fri, 23 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-10-23/</guid>
      <description>&lt;h4 id=&#34;date-2015-10-23&#34;&gt;Date: 2015-10-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Finite mixture regression models have been widely used for modeling mixed regression relationships arising from a clustered and thus heterogenous population. The classical normal mixture model, despite of its simplicity and wide applicability, may fail dramatically in the presence of severe outliers. We propose a robust mixture regression approach based on a sparse, case-specific, and scale-dependent mean-shift parameterization, for simultaneously conducting outlier detection and robust parameter estimation. A penalized likelihood approach is adopted to induce sparsity among the mean-shift parameters so that the outliers are distinguished from the good observations, and a thresholding-embedded Expectation-Maximization (EM) algorithm is developed to enable stable and efficient computation. The proposed penalized estimation approach is shown to have strong connections with other robust methods including the trimmed likelihood and the M-estimation methods. Comparing with several existing methods, the proposed methods show outstanding performance in numerical studies.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Estimating high-dimensional multi-layered networks through penalized maximum likelihood</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-10-16/</link>
      <pubDate>Fri, 16 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-10-16/</guid>
      <description>&lt;h4 id=&#34;date-2015-10-16&#34;&gt;Date: 2015-10-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Gaussian graphical models represent a good tool for capturing interactions between nodes represent the underlying random variables. However, in many applications in biology one is interested in modeling associations both between, as well as within molecular compartments (e.g., interactions between genes and proteins/metabolites). To this end, inferring multi-layered network structures from high-dimensional data provides insight into understanding the conditional relationships among nodes within layers, after adjusting for and quantifying the effects of nodes from other layers. We propose an integrated algorithmic approach for estimating multi-layered networks, that incorporates a screening step for significant variables, an optimization algorithm for estimating the key model parameters and a stability selection step for selecting the most stable effects. The proposed methodology offers an efficient way of estimating the edges within and across layers iteratively, by solving an optimization problem constructed based on penalized maximum likelihood (under a Gaussianity assumption). The optimization is solved on a reduced parameter space that is identified through screening, which remedies the instability in high-dimension. Theoretical properties are considered to ensure identifiability and consistent estimation of the parameters and convergence of the optimization algorithm, despite the lack of global convexity. The performance of the methodology is illustrated on synthetic data sets and on an application on gene and metabolic expression data for patients with renal disease.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Parameter estimation of partial differential equations over irregular domains</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-10-09/</link>
      <pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-10-09/</guid>
      <description>&lt;h4 id=&#34;date-2015-10-09&#34;&gt;Date: 2015-10-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Spatio-temporal data are abundant in many scientific fields; examples include daily satellite images of the earth, hourly temperature readings from multiple weather stations, and the spread of an infectious disease over a particular region. In many instances the spatio-temporal data are accompanied by mathematical models expressed in terms of partial differential equations (PDEs). These PDEs determine the theoretical aspects of the behavior of the physical, chemical or biological phenomena considered. Azzimonti (2013) showed that including the associated PDE as a regularization term as opposed to the conventional two-dimensional Laplacian provides a considerable improvement in the estimation accuracy. The PDEs parameters often have interesting interpretations. Although they are typically unknown and must be inferred from expert knowledge of the phenomena considered. In this talk I will discuss extending the profiling with a parameter cascading procedure outlined in Ramsay et al. (2007) to incorporate PDE parameter estimation. I will also show how, following Sangalli et al. (2013), the estimation procedure can be extended to include finite-element methods (FEMs). This allows the proposed method to account for attributes of the geometry of the physical problem such as irregular shaped domains, external and internal boundary features, as well as strong concavities. Thus this talk will introduce a methodology for data-driven estimates of the parameters of PDEs defined over irregular domains.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Estimating covariance matrices of intermediate size</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-10-02/</link>
      <pubDate>Fri, 02 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-10-02/</guid>
      <description>&lt;h4 id=&#34;date-2015-10-02&#34;&gt;Date: 2015-10-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In finance, the covariance matrix of many assets is a key component of financial portfolio optimization and is usually estimated from historical data. Much research in the past decade has focused on improving estimation by studying the asymptotics of large covariance matrices in the so-called high-dimensional regime, where the dimension p grows at the same pace as the sample size n, and this approach has been very successful. This choice of growth makes sense in part because, based on results for eigenvalues, it appears that there are only two limits: the high-dimensional one when p grows like n, and the classical one, when p grows more slowly than n. In this talk, I will present evidence that this binary view is false, and that there could be hidden intermediate regimes lying in between. In turn, this allows for corrections to the sample covariance matrix that are more appropriate when the dimension is large but moderate with respect to the sample size, as is often the case; this can also lead to better optimization for portfolio volatility in many situations of interest.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Topics in statistical inference for the semiparametric elliptical copula model</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-09-25/</link>
      <pubDate>Fri, 25 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-09-25/</guid>
      <description>&lt;h4 id=&#34;date-2015-09-25&#34;&gt;Date: 2015-09-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This talk addresses aspects of the statistical inference problem for the semiparametric elliptical copula model. The semiparametric elliptical copula model is the family of distributions whose dependence structures are specified by parametric elliptical copulas but whose marginal distributions are left unspecified. An elliptical copula is uniquely characterized by a characteristic generator and a copula correlation matrix Sigma. In the first part of this talk, I will consider the estimation of Sigma. A natural estimate for Sigma is the plug-in estimator Sigmahat with Kendall&amp;rsquo;s tau statistic. I will first exhibit a sharp bound on the operator norm of Sigmahat - Sigma. I will then consider a factor model of Sigma, for which I will propose a refined estimator Sigmatilde by fitting a low-rank matrix plus a diagonal matrix to Sigmahat using least squares with a nuclear norm penalty on the low-rank matrix. The bound on the operator norm of Sigmahat - Sigma serves to scale the penalty term, and we obtained finite-sample oracle inequalities for Sigmatilde that I will present. In the second part of this talk, we will look at the classification of two distributions that have the same Gaussian copula but that are otherwise arbitrary in high dimensions. Under this semiparametric Gaussian copula setting, I will give an accurate semiparametric estimator of the log-density ratio, which leads to an empirical decision rule and a bound on its associated excess risk. Our estimation procedure takes advantage of the potential sparsity as well as the low noise condition in the problem, which allows us to achieve faster convergence rate of the excess risk than is possible in the existing literature on semiparametric Gaussian copula classification. I will demonstrate the efficiency of our semiparametric empirical decision rule by showing that the bound on the excess risk nearly achieves a convergence rate of 1 over square-root-n in the simple setting of Gaussian distribution classification.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A unified algorithm for fitting penalized models with high-dimensional data</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-09-18/</link>
      <pubDate>Fri, 18 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-09-18/</guid>
      <description>&lt;h4 id=&#34;date-2015-09-18&#34;&gt;Date: 2015-09-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the light of high-dimensional problems, research on the penalized model has received much interest. Correspondingly, several algorithms have been developed for solving penalized high-dimensional models. I will describe fast and efficient unified algorithms for computing the solution path for a collection of penalized models. In particular, we will look at an algorithm for solving L1-penalized learning problems and an algorithm for solving group-lasso learning problems. These algorithm take advantage of a majorization-minimization trick to make each update simple and efficient. The algorithms also enjoy a proven convergence property. To demonstrate the generality of these algorithms, I extend them to a class of elastic net penalized large margin classification methods and to elastic net penalized Cox proportional hazards models. These algorithms have been implemented in three R packages gglasso, gcdnet and fastcox, which are publicly available from the Comprehensive R Archive Network (CRAN) at &lt;a href=&#34;http://cran.r-project.org/web/packages&#34;&gt;http://cran.r-project.org/web/packages&lt;/a&gt;. On simulated and real data, our algorithms consistently outperform the existing software in speed for computing penalized models and often delivers better quality solutions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bias correction in multivariate extremes</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-09-11/</link>
      <pubDate>Fri, 11 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-09-11/</guid>
      <description>&lt;h4 id=&#34;date-2015-09-11&#34;&gt;Date: 2015-09-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The estimation of the extremal dependence structure of a multivariate extreme-value distribution is spoiled by the impact of the bias, which increases with the number of observations used for the estimation. Already known in the univariate setting, the bias correction procedure is studied in this talk under the multivariate framework. New families of estimators of the stable tail dependence function are obtained. They are asymptotically unbiased versions of the empirical estimator introduced by Huang (1992). Given that the new estimators have a regular behavior with respect to the number of observations, it is possible to deduce aggregated versions so that the choice of threshold is substantially simplified. An extensive simulation study is provided as well as an application on real data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some new classes of bivariate distributions based on conditional specification</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-05-14/</link>
      <pubDate>Thu, 14 May 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-05-14/</guid>
      <description>&lt;h4 id=&#34;date-2015-05-14&#34;&gt;Date: 2015-05-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A bivariate distribution can sometimes be characterized completely by properties of its conditional distributions. In this talk, we will discuss models of bivariate distributions whose conditionals are members of prescribed parametric families of distributions. Some relevant models with specified conditionals will be discussed, including the normal and lognormal cases, the skew-normal and other families of distributions. Finally, some conditionally specified densities will be shown to provide convenient flexible conjugate prior families in certain multiparameter Bayesian settings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Testing for network community structure</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-03-20/</link>
      <pubDate>Fri, 20 Mar 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-03-20/</guid>
      <description>&lt;h4 id=&#34;date-2015-03-20&#34;&gt;Date: 2015-03-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Networks provide a useful means to summarize sparse yet structured massive datasets, and so are an important aspect of the theory of big data. A key question in this setting is to test for the significance of community structure or what in social networks is termed homophily, the tendency of nodes to be connected based on similar characteristics. Network models where a single parameter per node governs the propensity of connection are popular in practice, because they are simple to understand and analyze. They frequently arise as null models to indicate a lack of community structure, since they cannot readily describe the division of a network into groups of nodes whose aggregate links behave in a block-like manner. Here we discuss asymptotic regimes under families of such models, and show their potential for enabling hypothesis tests in this setting. As an important special case, we treat network modularity, which summarizes the difference between observed and expected within-community edges under such null models, and which has seen much success in practical applications of large-scale network analysis. Our focus here is on statistical rather than algorithmic properties, however, in order to yield new insights into the canonical problem of testing for network community structure.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian approaches to causal inference: A lack-of-success story</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-03-13/</link>
      <pubDate>Fri, 13 Mar 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-03-13/</guid>
      <description>&lt;h4 id=&#34;date-2015-03-13&#34;&gt;Date: 2015-03-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Despite almost universal acceptance across most fields of statistics, Bayesian inferential methods have yet to breakthrough to widespread use in causal inference, despite Bayesian arguments being a core component of early developments in the field. Some quasi-Bayesian procedures have been proposed, but often these approaches rely on heuristic, sometimes flawed, arguments. In this talk I will discuss some formulations of classical causal inference problems from the perspective of standard Bayesian representations, and propose some inferential solutions. This is joint work with Olli Saarela, Dalla Lana School of Public Health, University of Toronto, Erica Moodie, Department of Epidemiology, Biostatistics and Occupational Health, McGill University, and Marina Klein, Division of Infectious Diseases, Faculty of Medicine, McGill University.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A novel statistical framework to characterize antigen-specific T-cell functional diversity in single-cell expression data</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-02-27/</link>
      <pubDate>Fri, 27 Feb 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-02-27/</guid>
      <description>&lt;h4 id=&#34;date-2015-02-27&#34;&gt;Date: 2015-02-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;I will talk about COMPASS, a new Bayesian hierarchical framework for characterizing functional differences in antigen-specific T cells by leveraging high-throughput, single-cell flow cytometry data. In particular, I will illustrate, using a variety of data sets, how COMPASS can reveal subtle and complex changes in antigen-specific T-cell activation profiles that correlate with biological endpoints. Applying COMPASS to data from the RV144 (“the Thai trial”) HIV clinical trial, it identified novel T-cell subsets that were inverse correlates of HIV infection risk. I also developed intuitive metrics for summarizing multivariate antigen-specific T-cell activation profiles for endpoints analysis. In addition, COMPASS identified correlates of latent infection in an immune study of Tuberculosis among South African adolescents. COMPASS is available as an R package and is sufficiently general that it can be adapted to new high-throughput data types, such as Mass Cytometry (CyTOF) and single-cell gene expressions, enabling interdisciplinary collaboration, which I will also highlight in my talk.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Comparison and assessment of particle diffusion models in biological fluids</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-02-20/</link>
      <pubDate>Fri, 20 Feb 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-02-20/</guid>
      <description>&lt;h4 id=&#34;date-2015-02-20&#34;&gt;Date: 2015-02-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Rapidly progressing particle tracking techniques have revealed that foreign particles in biological fluids exhibit rich and at times unexpected behavior, with important consequences for disease diagnosis and drug delivery. Yet, there remains a frustrating lack of coherence in the description of these particles&amp;rsquo; motion. Largely this is due to a reliance on functional statistics (e.g., mean-squared displacement) to perform model selection and assess goodness-of-fit. However, not only are such functional characteristics typically estimated with substantial variability, but also they may fail to distinguish between a number of stochastic processes &amp;mdash; each making fundamentally different predictions for relevant quantities of scientific interest. In this talk, I will describe a detailed Bayesian analysis of leading candidate models for subdiffusive particle trajectories in human pulmonary mucus. Efficient and scalable computational strategies will be proposed. Model selection will be achieved by way of intrinsic Bayes factors, which avoid both non-informative priors and &amp;ldquo;using the data twice&amp;rdquo;. Goodness-of-fit will be evaluated via second-order criteria along with exact model residuals. Our findings suggest that a simple model of fractional Brownian motion describes the data just as well as a first-principles physical model of visco-elastic subdiffusion.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tuning parameters in high-dimensional statistics</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-02-13/</link>
      <pubDate>Fri, 13 Feb 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-02-13/</guid>
      <description>&lt;h4 id=&#34;date-2015-02-13&#34;&gt;Date: 2015-02-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;High-dimensional statistics is the basis for analyzing large and complex data sets that are generated by cutting-edge technologies in genetics, neuroscience, astronomy, and many other fields. However, Lasso, Ridge Regression, Graphical Lasso, and other standard methods in high-dimensional statistics depend on tuning parameters that are difficult to calibrate in practice. In this talk, I present two novel approaches to overcome this difficulty. My first approach is based on a novel testing scheme that is inspired by Lepski’s idea for bandwidth selection in non-parametric statistics. This approach provides tuning parameter calibration for estimation and prediction with the Lasso and other standard methods and is to date the only way to ensure high performance, fast computations, and optimal finite sample guarantees. My second approach is based on the minimization of an objective function that avoids tuning parameters altogether. This approach provides accurate variable selection in regression settings and, additionally, opens up new possibilities for the estimation of gene regulation networks, microbial ecosystems, and many other network structures.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A fast unified algorithm for solving group Lasso penalized learning problems</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-02-05/</link>
      <pubDate>Thu, 05 Feb 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-02-05/</guid>
      <description>&lt;h4 id=&#34;date-2015-02-05&#34;&gt;Date: 2015-02-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1b39&#34;&gt;Location: BURN 1B39&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider a class of group-lasso learning problems where the objective function is the sum of an empirical loss and the group-lasso penalty. For a class of loss function satisfying a quadratic majorization condition, we derive a unified algorithm called groupwise-majorization-descent (GMD) for efficiently computing the solution paths of the corresponding group-lasso penalized learning problem. GMD allows for general design matrices, without requiring the predictors to be group-wise orthonormal. As illustration examples, we develop concrete algorithms for solving the group-lasso penalized least squares and several group-lasso penalized large margin classifiers. These group-lasso models have been implemented in an R package gglasso publicly available from the Comprehensive R Archive Network (CRAN) at &lt;a href=&#34;http://cran.r-project.org/web/packages/gglasso&#34;&gt;http://cran.r-project.org/web/packages/gglasso&lt;/a&gt;. On simulated and real data, gglasso consistently outperforms the existing software for computing the group-lasso that implements either the classical groupwise descent algorithm or Nesterov&amp;rsquo;s method. An application in risk segmentation of insurance business is illustrated by analysis of an auto insurance claim dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Joint analysis of multiple multi-state processes via copulas</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-02-02/</link>
      <pubDate>Mon, 02 Feb 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-02-02/</guid>
      <description>&lt;h4 id=&#34;date-2015-02-02&#34;&gt;Date: 2015-02-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1214&#34;&gt;Location: BURN 1214&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A copula-based model is described which enables joint analysis of multiple progressive multi-state processes. Unlike intensity-based or frailty-based approaches to joint modeling, the copula formulation proposed herein ensures that a wide range of marginal multi-state processes can be specified and the joint model will retain these marginal features. The copula formulation also facilitates a variety of approaches to estimation and inference including composite likelihood and two-stage estimation procedures. We consider processes with Markov margins in detail, which are often suitable when chronic diseases are progressive in nature. We give special attention to the setting in which individuals are examined intermittently and transition times are consequently interval-censored. Simulation studies give empirical insight into the different methods of analysis and an application involving progression in joint damage in psoriatic arthritis provides further illustration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Distributed estimation and inference for sparse regression</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-01-30/</link>
      <pubDate>Fri, 30 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-01-30/</guid>
      <description>&lt;h4 id=&#34;date-2015-01-30&#34;&gt;Date: 2015-01-30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We address two outstanding challenges in sparse regression: (i) computationally efficient estimation in distributed settings; (ii) valid inference for the selected coefficients. The main computational challenge in a distributed setting is harnessing the computational capabilities of all the machines while keeping communication costs low. We devise an approach that requires only a single round of communication among the machines. We show the approach recovers the convergence rate of the (centralized) lasso as long as each machine has access to an adequate number of samples. Turning to the second challenge, we devise an approach to post-selection inference by conditioning on the selected model. In a nutshell, our approach gives inferences with the same frequency interpretation as those given by data/sample splitting, but it is more broadly applicable and more powerful. The validity of our approach also does not depend on the correctness of the selected model, i.e., it gives valid inferences even when the selected model is incorrect.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simultaneous white noise models and shrinkage recovery of functional data</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-01-16/</link>
      <pubDate>Fri, 16 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-01-16/</guid>
      <description>&lt;h4 id=&#34;date-2015-01-16&#34;&gt;Date: 2015-01-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider the white noise representation of functional data taken as i.i.d. realizations of a Gaussian process. The main idea is to establish an asymptotic equivalence in Le Cam’s sense between an experiment which simultaneously describes these realizations and a collection of white noise models. In this context, we project onto an arbitrary basis and apply a novel variant of Stein-type estimation for optimal recovery of the realized trajectories. A key inequality is derived showing that the corresponding risks, conditioned on the underlying curves, are minimax optimal and can be made arbitrarily close to those that an oracle with knowledge of the process would attain. Empirical performance is illustrated through simulated and real data examples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mixtures of coalesced generalized hyperbolic distributions</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-01-13/</link>
      <pubDate>Tue, 13 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-01-13/</guid>
      <description>&lt;h4 id=&#34;date-2015-01-13&#34;&gt;Date: 2015-01-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A mixture of coalesced generalized hyperbolic distributions is developed by joining a finite mixture of generalized hyperbolic distributions with a mixture of multiple scaled generalized hyperbolic distributions. The result is a mixture of mixtures with shared model parameters and common mode. We begin by discussing the generalized hyperbolic distribution, which has the t, Gaussian and others as special cases. The generalized hyperbolic distribution can represented as a normal-variance mixture using a generalized inverse Gaussian distribution. This representation makes it a suitable candidate for the expectation-maximization algorithm. Secondly, we discuss the multiple scale generalized hyperbolic distribution which arises via implementation of a multi-dimensional weight function. A parameter estimation scheme is developed using the ever-expanding class of MM algorithms and the Bayesian information criterion is used for model selection. Special consideration is given to the contour shape. We use the coalesced distribution for clustering and compare them to finite mixtures of skew-t distributions using simulated and real data sets. Finally, the role of generalized hyperbolic mixtures within the wider model-based clustering, classification, and density estimation literature is discussed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Space-time data analysis: Out of the Hilbert box</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-01-9/</link>
      <pubDate>Fri, 09 Jan 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-01-9/</guid>
      <description>&lt;h4 id=&#34;date-2015-01-09&#34;&gt;Date: 2015-01-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Given the discouraging state of current efforts to curb global warming, we can imagine that we will soon turn our attention to mitigation. On a global scale, distressed populations will turn to national and international organizations for solutions to dramatic problems caused by climate change. These institutions in turn will mandate the collection of data on a scale and resolution that will present extraordinary statistical and computational challenges to those of us viewed as having the appropriate expertise. A review of the current state of our space-time data analysis machinery suggests that we have much to do. Most of current spatial modelling methodology is based on concepts translated from time series analysis, is heavily dependent on various kinds of stationarity assumptions, uses the Gaussian distribution to model data and depends on a priori coordinate systems that do not exist in nature. A way forward from this restrictive framework is proposed by modelling data over textured domains using layered coordinate systems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Testing for structured Normal means</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-12-12/</link>
      <pubDate>Fri, 12 Dec 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-12-12/</guid>
      <description>&lt;h4 id=&#34;date-2014-12-12&#34;&gt;Date: 2014-12-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We will discuss the detection of pattern in images and graphs from a high-dimensional Gaussian measurement. This problem is relevant to many applications including detecting anomalies in sensor and computer networks, large-scale surveillance, co-expressions in gene networks, disease outbreaks, etc. Beyond its wide applicability, structured Normal means detection serves as a case study in the difficulty of balancing computational complexity with statistical power. We will begin by discussing the detection of active rectangles in images and sensor grids. We will develop an adaptive scan test and determine its asymptotic distribution. We propose an approximate algorithm that runs in nearly linear time but achieves the same asymptotic distribution as the naive, quadratic run-time algorithm. We will move on to the more general problem of detecting a well-connected active subgraph within a graph in the Normal means context. Because the generalized likelihood ratio test is computationally infeasible, we propose approximate algorithms and study their statistical efficiency. One such algorithm that we develop is the graph Fourier scan statistic, whose statistical performance is characterized by the spectrum of the graph Laplacian. Another relaxation that we have developed is the Lovasz extended scan statistic (LESS), which is based on submodular optimization and the performance is described using electrical network theory. We also introduce the spanning tree wavelet basis over graphs, a localized basis that reflects the topology of the graph. For each of these tests we compare their statistical guarantees to an information theoretic lower bound.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Copula model selection: A statistical approach</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-12-05/</link>
      <pubDate>Fri, 05 Dec 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-12-05/</guid>
      <description>&lt;h4 id=&#34;date-2014-12-05&#34;&gt;Date: 2014-12-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Copula model selection is an important problem because similar but differing copula models can offer different conclusions surrounding the dependence structure of random variables. Chen &amp;amp; Fan (2005) proposed a model selection method involving a statistical hypothesis test. The hypothesis test attempts to take into account the randomness of the AIC and other likelihood-based model selection methods for finite samples. Performance of the test compared to the more common approach of AIC is illustrated in a series of simulations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model-based methods of classification with applications</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-11-28/</link>
      <pubDate>Fri, 28 Nov 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-11-28/</guid>
      <description>&lt;h4 id=&#34;date-2014-11-28&#34;&gt;Date: 2014-11-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Model-based clustering via finite mixture models is a popular clustering method for finding hidden structures in data. The model is often assumed to be a finite mixture of multivariate normal distributions; however, flexible extensions have been developed over recent years. This talk demonstrates some methods employed in unsupervised, semi-supervised, and supervised classification that include skew-normal and skew-t mixture models. Both real and simulated data sets are used to demonstrate the efficacy of these techniques.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Estimating by solving nonconvex programs: Statistical and computational guarantees</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-11-21/</link>
      <pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-11-21/</guid>
      <description>&lt;h4 id=&#34;date-2014-11-21&#34;&gt;Date: 2014-11-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Many statistical estimators are based on solving nonconvex programs. Although the practical performance of such methods is often excellent, the associated theory is frequently incomplete, due to the potential gaps between global and local optima. In this talk, we present theoretical results that apply to all local optima of various regularized M-estimators, where both loss and penalty functions are allowed to be nonconvex. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for error-in-variables linear models; regression in generalized linear models using nonconvex regularizers such as SCAD and MCP; and graph and inverse covariance matrix estimation. Under suitable regularity conditions, our theory guarantees that any local optimum of the composite objective function lies within statistical precision of the true parameter vector. This result closes the gap between theory and practice for these methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bridging the gap: A likelihood function approach for the analysis of ranking data</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-11-14/</link>
      <pubDate>Fri, 14 Nov 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-11-14/</guid>
      <description>&lt;h4 id=&#34;date-2014-11-14&#34;&gt;Date: 2014-11-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the parametric setting, the notion of a likelihood function forms the basis for the development of tests of hypotheses and estimation of parameters. Tests in connection with the analysis of variance stem entirely from considerations of the likelihood function. On the other hand, non- parametric procedures have generally been derived without any formal mechanism and are often the result of clever intuition. In this talk, we propose a more formal approach for deriving tests involving the use of ranks. Specifically, we define a likelihood function motivated by characteristics of the ranks of the data and demonstrate that this leads to well-known tests of hypotheses. We also point to various areas of further exploration.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian regression with B-splines under combinations of shape constraints and smoothness properties</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-11-07/</link>
      <pubDate>Fri, 07 Nov 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-11-07/</guid>
      <description>&lt;h4 id=&#34;date-2014-11-07&#34;&gt;Date: 2014-11-07&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We approach the problem of shape constrained regression from a Bayesian perspective. A B-spline basis is used to model the regression function. The smoothness of the regression function is controlled by the order of the B-splines and the shape is controlled by the shape of an associated control polygon. Controlling the shape of the control polygon reduces to some inequality constraints on the spline coefficients. Our approach enables us to take into account combinations of shape constraints and to localize each shape constraint on a given interval. The performances of our method is investigated through a simulation study. Applications to real data sets from the food industry and Global Warming are provided.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A copula-based model for risk aggregation</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-10-31/</link>
      <pubDate>Fri, 31 Oct 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-10-31/</guid>
      <description>&lt;h4 id=&#34;date-2014-10-31&#34;&gt;Date: 2014-10-31&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A flexible approach is proposed for risk aggregation. The model consists of a tree structure, bivariate copulas, and marginal distributions. The construction relies on a conditional independence assumption whose implications are studied. Selection the tree structure, estimation and model validation are illustrated using data from a Canadian property and casualty insurance company.&lt;/p&gt;&#xA;&lt;h2 id=&#34;speaker&#34;&gt;Speaker&lt;/h2&gt;&#xA;&lt;p&gt;Marie-Pier Côté is a PhD student in the Department of Mathematics and Statistics at McGill University.&lt;/p&gt;</description>
    </item>
    <item>
      <title>PREMIER: Probabilistic error-correction using Markov inference in error reads</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-10-24/</link>
      <pubDate>Fri, 24 Oct 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-10-24/</guid>
      <description>&lt;h4 id=&#34;date-2014-10-24&#34;&gt;Date: 2014-10-24&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Next generation sequencing (NGS) is a technology revolutionizing genetics and biology. Compared with the old Sanger sequencing method, the throughput is astounding and has fostered a slew of innovative sequencing applications.  Unfortunately, the error rates are also higher, complicating many downstream analyses.  For example, de novo assembly of genomes is less accurate and slower when reads include many errors.  We develop a probabilistic model for NGS reads that can detect and correct errors without a reference genome and while flexibly modeling and estimating the error properties of the sequencing machine.  It uses a penalized likelihood to enforce our prior belief that the kmer spectrum (collection of k-length strings observed in the reads) generated from a genome is sparse when k is sufficiently large.  The model formalizes core ideas that are used in many ad hoc algorithmic approaches to error correction.  We show our method can detect and remove more errors from sequencing reads than existing methods. Though our method carries a higher computational burden than the best algorithmic approaches, the probabilistic approach is extensible, flexible, and well-positioned to support downstream statistical analysis of the increasing volume of sequence data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Patient privacy, big data, and specimen pooling: Using an old tool for new challenges</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-10-17/</link>
      <pubDate>Fri, 17 Oct 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-10-17/</guid>
      <description>&lt;h4 id=&#34;date-2014-10-17&#34;&gt;Date: 2014-10-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the recent past, electronic health records and distributed data networks emerged as a viable resource for medical and scientific research. As the use of confidential patient information from such sources become more common, maintaining privacy of patients is of utmost importance. For a binary disease outcome of interest, we show that the techniques of specimen pooling could be applied for analysis of large and/or distributed data while respecting patient privacy. I will review the pooled analysis for a binary outcome and then show how it can be used for distributed data. Aggregate level data are passed from the nodes of the network to the analysis center and can be used very easily with logistic regression for estimation of disease odds ratio associated with a set of categorical or continuous covariates. Pooling approach allows for consistent estimation of the parameters of logistic regression that can include confounders. Additionally, since the individual covariate values can be accessed within a network, effect modifiers can be accommodated and consistently estimated. Since pooling effectively reduces the size of the dataset by creating pools or sets of individual, the resulting dataset can be analyzed much more quickly as compared to an original dataset that is too big as compared to computing environment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A margin-free clustering algorithm appropriate for dependent maxima in the domain of attraction of an extreme-value copula</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-10-10/</link>
      <pubDate>Fri, 10 Oct 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-10-10/</guid>
      <description>&lt;h4 id=&#34;date-2014-10-10&#34;&gt;Date: 2014-10-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Extracting relevant information in complex spatial-temporal data sets is of paramount importance in statistical climatology. This is especially true when identifying spatial dependencies between quantitative extremes like heavy rainfall. The paper of Bernard et al. (2013) develops a fast and simple clustering algorithm for finding spatial patterns appropriate for extremes. They develop their algorithm by adapting multivariate extreme-value theory to the context of spatial clustering. This is done by relating the variogram, a well-known distance used in geostatistics, to the extremal coefficient of a pair of joint maxima. This gives rise to a straightforward nonparametric estimator of this distance using the empirical distribution function. Their clustering approach is used to analyze weekly maxima of hourly precipitation recorded in France and a spatial pattern consistent with existing weather models arises. This applied talk is devoted to the validation and extension of this clustering approach. A simulation study using the multivariate logistic distribution as well as max-stable random fields shows that this approach provides accurate clustering when the maxima belong to an extreme-value distribution. Furthermore this clustering distance can be viewed as an average absolute rank difference, implying that it is appropriate for margin-free clustering of dependent variables. In particular it is appropriate for dependent maxima in the domain of attraction of an extreme-value copula.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical exploratory data analysis in the modern era</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-10-03/</link>
      <pubDate>Fri, 03 Oct 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-10-03/</guid>
      <description>&lt;h4 id=&#34;date-2014-10-03&#34;&gt;Date: 2014-10-03&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Major challenges arising from today&amp;rsquo;s &amp;ldquo;data deluge&amp;rdquo; include how to handle the commonly occurring situation of different types of variables (say, continuous and categorical) being simultaneously measured, as well as how to assess the accompanying flood of questions. Based on information theory, a bias-corrected mutual information (BCMI) measure of association that is valid and estimable between all basic types of variables has been proposed. It has the advantage of being able to identify non-linear as well as linear relationships. Based on the BCMI measure, a novel exploratory approach to finding associations in data sets having a large number of variables of different types has been developed. These associations can be used as a basis for downstream analyses such as finding clusters and networks. The application of this exploratory approach is very general. Comparisons also will be made with other measures. Illustrative examples include exploring relationships (i) in clinical and genomic (say, gene expression and genotypic) data, and (ii) between social, economic, health and political indicators from the World Health Organisation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Analysis of palliative care studies with joint models for quality-of-life measures and survival</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-09-26/</link>
      <pubDate>Fri, 26 Sep 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-09-26/</guid>
      <description>&lt;h4 id=&#34;date-2014-09-26&#34;&gt;Date: 2014-09-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In palliative care studies, the primary outcomes are often health related quality of life measures (HRLQ). Randomized trials and prospective cohorts typically recruit patients with advanced stage of disease and follow them until death or end of the study. An important feature of such studies is that, by design, some patients, but not all, are likely to die during the course of the study. This affects the interpretation of the conventional analysis of palliative care trials and suggests the need for specialized methods of analysis. We have developed a “terminal decline model” for palliative care trials that, by jointly modeling the time until death and the HRQL measures, leads to flexible interpretation and efficient analysis of the trial data (Li, Tosteson, Bakitas, STMED 2012).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Covariates missing by design</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-09-19/</link>
      <pubDate>Fri, 19 Sep 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-09-19/</guid>
      <description>&lt;h4 id=&#34;date-2014-09-19&#34;&gt;Date: 2014-09-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Incomplete data can arise in many different situations for many different reasons. Sometimes the data may be incomplete for reasons beyond the control of the experimenter. However, it is also possible that this missingness is part of the study design. By using a two-phase sampling approach where only a small sub-sample gives complete information, it is possible to greatly reduce the cost of a study and still obtain precise estimates. This talk will introduce the concepts of incomplete data and two-phase sampling designs and will discuss adaptive two-phase designs which exploit information from an internal pilot study to approximate the optimal sampling scheme for an analysis based on mean score estimating equations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hydrological applications with the functional data analysis framework</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-09-12/</link>
      <pubDate>Fri, 12 Sep 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-09-12/</guid>
      <description>&lt;h4 id=&#34;date-2014-09-12&#34;&gt;Date: 2014-09-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;River flows records are an essential data source for a variety of hydrological applications including the prevention of flood risks and as well as the planning and management of water resources. A hydrograph is a graphical representation of the temporal variation of flow over a period of time (continuously measured, usually over a year). A flood hydrograph is commonly characterized by a number of features, mainly its peak, volume and duration. Classical and recent multivariate approaches considered in hydrological applications treated these features jointly in order to take into account their dependence structure or their relationship. However, all these approaches are based on the analysis of a limited number of characteristics and do not make use of the full information provided by the hydrograph. Even though these approaches provided good results, they present some drawbacks and limitations. The objective of the present talk is to introduce a new framework for hydrological applications where data, such as hydrographs, are employed as continuous curves: functional data. In this context, the whole hydrograph is considered as one infinite-dimensional observation. This context contributes to addressing the problem of lack of data commonly encountered in hydrology. A number of functional data analysis tools and methods are presented and adapted.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some aspects of data analysis under confidentiality protection</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-04-04/</link>
      <pubDate>Fri, 04 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-04-04/</guid>
      <description>&lt;h4 id=&#34;date-2014-04-04&#34;&gt;Date: 2014-04-04&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statisticians working in most federal agencies are often faced with two conflicting objectives: (1) collect and publish useful datasets for designing public policies and building scientific theories, and (2) protect confidentiality of data respondents which is essential to uphold public trust, leading to better response rates and data accuracy. In this talk I will provide a survey of two statistical methods currently used at the U.S. Census Bureau: synthetic data and noise perturbed data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How much does the dependence structure matter?</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-03-28/</link>
      <pubDate>Fri, 28 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-03-28/</guid>
      <description>&lt;h4 id=&#34;date-2014-03-28&#34;&gt;Date: 2014-03-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, we will look at some classical problems from an anti-traditional perspective. We will consider two problems regarding a sequence of random variables with a given common marginal distribution. First, we will introduce the notion of extreme negative dependence (END), a new benchmark for negative dependence, which is comparable to comonotonicity and independence. Second, we will study the compatibility of the marginal distribution and the limiting distribution when the dependence structure in the sequence is allowed to vary among all possibilities. The results are somewhat simple, yet surprising. We will provide some interpretation and applications of the theoretical results in financial risk management, with the hope to deliver the following message: with the common marginal distribution known and dependence structure unknown, we know essentially nothing about the asymptotic shape of the sum of random variables.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mixed effects trees and forests for clustered data</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-03-14/</link>
      <pubDate>Fri, 14 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-03-14/</guid>
      <description>&lt;h4 id=&#34;date-2014-03-14&#34;&gt;Date: 2014-03-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, I will present extensions of tree-based and random forest methods for the case of clustered data. The proposed methods can handle unbalanced clusters, allows observations within clusters to be splitted, and can incorporate random effects and observation-level covariates. The basic tree-building algorithm for a continuous outcome is implemented using standard algorithms within the framework of the EM algorithm. The extension to other types of outcomes (e.g., binary, count) uses the penalized quasi-likelihood (PQL) method for the estimation and the EM algorithm for the computation. Simulation results show that the proposed methods provides substantial improvements over standard trees and forests when the random effects are non negligible. The use of the method will be illustrated with real data sets.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On the multivariate analysis of neural spike trains: Skellam process with resetting and its applications</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-02-21/</link>
      <pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-02-21/</guid>
      <description>&lt;h4 id=&#34;date-2014-02-21&#34;&gt;Date: 2014-02-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Nerve cells (a.k.a. neurons) communicate via electrochemical waves (action potentials), which are usually called spikes as they are very localized in time. A sequence of consecutive spikes from one neuron is called a spike train. The exact mechanism of information coding in spike trains is still an open problem; however, one popular approach is to model spikes as realizations of an inhomogeneous Poisson process. In this talk, the limitations of the Poisson model are highlighted , and the Skellam Process with Resetting (SPR) is introduced as an alternative model for the analysis of neural spike trains. SPR is biologically justified, and the parameter estimation algorithm developed for it is computationally efficient. To allow for the modelling of neural ensembles, this process is generalized to the multivariate case, where Multivariate Skellam Process with Resetting (MSPR), as well as the multivariate Skellam distribution are introduced. Simulation and real data studies confirm the promising results of the Skellam model in the statistical analysis of neural spike trains.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Divergence based inference for general estimating equations</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-02-14/</link>
      <pubDate>Fri, 14 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-02-14/</guid>
      <description>&lt;h4 id=&#34;date-2014-02-14&#34;&gt;Date: 2014-02-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Hellinger distance and its variants have long been used in the theory of robust statistics to develop inferential tools that are more robust than the maximum likelihood but as ecient as the MLE when the posited model holds. A key aspect of this alternative approach requires specication of a parametric family, which is usually not feasible in the context of problems involving complex data structures wherein estimating equations are typically used for inference. In this presentation, we describe how to extend the scope of divergence theory for inferential problems involving estimating equations and describe useful algorithms for their computation. Additionally, we theoretically study the robustness properties of the methods and establish the semi-parametric eciency of the new divergence based estimators under suitable technical conditions. Finally, we use the proposed methods to develop robust sure screening methods for ultra high dimensional problems. Theory of large deviations, convexity theory, and concentration inequalities play an essential role in the theoretical analysis and numerical development. Applications from equine parasitology, stochastic optimization, and antimicrobial resistance will be used to describe various aspects of the proposed methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical techniques for the normalization and segmentation of structural MRI</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-02-07/</link>
      <pubDate>Fri, 07 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-02-07/</guid>
      <description>&lt;h4 id=&#34;date-2014-02-07&#34;&gt;Date: 2014-02-07&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;While computed tomography and other imaging techniques are measured in absolute units with physical meaning, magnetic resonance images are expressed in arbitrary units that are difficult to interpret and differ between study visits and subjects. Much work in the image processing literature has centered on histogram matching and other histogram mapping techniques, but little focus has been on normalizing images to have biologically interpretable units. We explore this key goal for statistical analysis and the impact of normalization on cross-sectional and longitudinal segmentation of pathology.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An exchangeable Kendall&#39;s tau for clustered data</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-01-31/</link>
      <pubDate>Fri, 31 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-01-31/</guid>
      <description>&lt;h4 id=&#34;date-2014-01-31&#34;&gt;Date: 2014-01-31&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;I&amp;rsquo;ll introduce the exchangeable Kendall&amp;rsquo;s tau as a nonparametric intra class association measure in a clustered data frame and provide an estimator for this measure. The asymptotic properties of this estimator are investigated under a multivariate exchangeable cdf. Two applications of the proposed statistic are considered. The first is an estimator of the intraclass correlation coefficient for data drawn from an elliptical distribution. The second is a semi-parametric intraclass independence test based on the exchangeable Kendall&amp;rsquo;s tau.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An introduction to stochastic partial differential equations and intermittency</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-01-10/</link>
      <pubDate>Fri, 10 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-01-10/</guid>
      <description>&lt;h4 id=&#34;date-2014-01-10&#34;&gt;Date: 2014-01-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In a seminal article in 1944, Itô introduced the stochastic integral with respect to the Brownian motion, which turned out to be one of the most fruitful ideas in mathematics in the 20th century. This lead to the development of stochastic analysis, a field which includes the study of stochastic partial differential equations (SPDEs). One of the approaches for the study of SPDEs was initiated by Walsh (1986) and relies on the concept of random-field solution for equations perturbed by a space-time white noise (or Brownian sheet). This concept allows us to investigate the dynamical changes in the probabilistic behavior of the solution, simultaneously in time and space. These developments will be reviewed in the first part of the talk. The second part of the talk will be dedicated to some recent advances in this area, related to the existence of a random-field solution for some classical SPDEs (like the stochastic heat equation) perturbed by a &lt;code&gt;colored&#39;&#39; noise, which behaves in time like the fractional Brownian motion. When this solution exists, it exhibits a strong form of &lt;/code&gt;intermittency,&amp;rsquo;&amp;rsquo; a property which was originally introduced in the physics literature for describing random fields whose values develop very large peaks. This talk is based on some recent joint work with Daniel Conus (Lehigh University).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tail order and its applications</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-11-22/</link>
      <pubDate>Fri, 22 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-11-22/</guid>
      <description>&lt;h4 id=&#34;date-2013-11-22&#34;&gt;Date: 2013-11-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Tail order is a notion for quantifying the strength of dependence in the tail of a joint distribution. It can account for a wide range of dependence, ranging from tail positive dependence to tail negative dependence. We will introduce theory and applications of tail order. Conditions for tail orders of copula families will be discussed, and they are helpful in guiding us to find suitable copula families for statistical inference. As applications of tail order, regression analysis will be demonstrated, using appropriately constructed copulas, that can capture the unique tail dependence patterns appear in a medical expenditure panel survey data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Submodel selection and post estimation: Making sense or folly</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-11-15/</link>
      <pubDate>Fri, 15 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-11-15/</guid>
      <description>&lt;h4 id=&#34;date-2013-11-15&#34;&gt;Date: 2013-11-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, we consider estimation in generalized linear models when there are many potential predictors and some of them may not have influence on the response of interest. In the context of two competing models where one model includes all predictors and the other restricts variable coefficients to a candidate linear subspace based on subject matter or prior knowledge, we investigate the relative performances of Stein type shrinkage, pretest, and penalty estimators (L1GLM, adaptive L1GLM, and SCAD) with respect to the full model estimator. The asymptotic properties of the pretest and shrinkage estimators including the derivation of asymptotic distributional biases and risks are established. A Monte Carlo simulation study show that the mean squared error (MSE) of an adaptive shrinkage estimator is comparable to the MSE of the penalty estimators in many situations and in particular performs better than the penalty estimators when the model is sparse.  A real data set analysis is also presented to compare the suggested methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The inadequacy of the summed score (and how you can fix it!)</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-11-08/</link>
      <pubDate>Fri, 08 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-11-08/</guid>
      <description>&lt;h4 id=&#34;date-2013-11-08&#34;&gt;Date: 2013-11-08&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Health researchers often use patient and physician questionnaires to assess certain aspects of health status. Item Response Theory (IRT) provides a set of tools for examining the properties of the instrument and for estimation of the latent trait for each individual. In my research, I critically examine the usefulness of the summed score over items and an alternative weighted summed score (using weights computed from the IRT model) as an alternative to both the empirical Bayes estimator and maximum likelihood estimator for the Generalized Partial Credit Model. First, I will talk about two useful theoretical properties of the weighted summed score that I have proven as part of my work. Then I will relate the weighted summed score to other commonly used estimators of the latent trait. I will demonstrate the importance of these results in the context of both simulated and real data on the Center for Epidemiological Studies Depression Scale.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian latent variable modelling of longitudinal family data for genetic pleiotropy studies</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-11-01/</link>
      <pubDate>Fri, 01 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-11-01/</guid>
      <description>&lt;h4 id=&#34;date-2013-11-01&#34;&gt;Date: 2013-11-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Motivated by genetic association studies of pleiotropy, we propose  a Bayesian latent variable approach to jointly study multiple outcomes or phenotypes. The proposed method models both continuous and binary phenotypes, and it accounts for serial and familial correlations when longitudinal and pedigree data have been collected. We present a Bayesian estimation method for the model parameters and we discuss some of the model misspecification effects.  Central to the analysis is a novel MCMC algorithm that builds upon hierarchical centering and  parameter expansion techniques to efficiently sample the posterior distribution. We discuss phenotype and model selection, and we study the performance of two selection strategies based on Bayes factors and spike-and-slab priors.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Whole genome 3D architecture of chromatin and regulation</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-10-18/</link>
      <pubDate>Fri, 18 Oct 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-10-18/</guid>
      <description>&lt;h4 id=&#34;date-2013-10-18&#34;&gt;Date: 2013-10-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The expression of a gene is usually controlled by the regulatory elements in its promoter region. However, it has long been hypothesized that, in complex genomes, such as the human genome, a gene may be controlled by distant enhancers and repressors. A recent molecular technique, 3C (chromosome conformation capture), that uses formaldehyde cross-linking and locus-specific PCR, was able to detect physical contacts between distant genomic loci. Such communication is achieved through spatial organization (looping) of chromosomes to bring genes and their&#xA;regulatory elements into close proximity. Several adaptations of the 3C assay to study genomewide spatial interactions, including Hi-C and ChIA-PET, have been developed. The availability of such data makes it possible to reconstruct the underlying three-dimensional spatial chromatin structure. In this talk, I will first describe a Bayesian statistical model for building spatial estrogen receptor regulation focusing on reducing false positive interactions. A random effect model, PRAM, will then be presented to make inference on the locations of genomic loci in a 3D Euclidean space. Results from ChIA-PET and Hi-C data will be visualized to illustrate the regulation and spatial proximity of genomic loci that are far apart in their linear chromosomal locations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Some recent developments in likelihood-based small area estimation</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-10-04/</link>
      <pubDate>Fri, 04 Oct 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-10-04/</guid>
      <description>&lt;h4 id=&#34;date-2013-10-04&#34;&gt;Date: 2013-10-04&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Mixed models are commonly used for the analysis data in small area estimation. In particular, small area estimation has been extensively studied under linear mixed models. However, in practice there are many situations that we have counts or proportions in small area estimation; for example a (monthly) dataset on the number of incidences in small areas. Recently, small area estimation under the linear mixed model with penalized spline model, for xed part of the model, was studied. In this talk, small area estimation under generalized linear mixed models by combining time series and cross-sectional data with the extension of these models to include penalized spline regression models are proposed. A likelihood-based approach is used to predict small area parameters and also to provide prediction intervals. The performance of the proposed models and approach is evaluated through simulation studies and also by real datasets.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tests of independence for sparse contingency tables and beyond</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-09-20/</link>
      <pubDate>Fri, 20 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-09-20/</guid>
      <description>&lt;h4 id=&#34;date-2013-09-20&#34;&gt;Date: 2013-09-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, a new and consistent statistic is proposed to test whether two discrete random variables are independent. The test is based on a statistic of the Cramér–von Mises type  constructed from the so-called empirical checkerboard copula. The test can be used even for sparse contingency tables or tables whose dimension changes with the sample size. Because the limiting distribution of the test statistic is not tractable, a valid bootstrap procedure for the computation of p-values will be discussed. The new statistic is compared by a power study to standard procedures for testing independence, such as the Pearson’s Chi-Squared, the Likelihood Ratio, and the Zelterman statistics. The new test turns out to be considerably more powerful than all its competitors in all scenarios considered.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian nonparametric density estimation under length bias sampling</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-09-13/</link>
      <pubDate>Fri, 13 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-09-13/</guid>
      <description>&lt;h4 id=&#34;date-2013-09-13&#34;&gt;Date: 2013-09-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A new density estimation method in a Bayesian nonparametric framework is presented when recorded data are not coming directly from the distribution of interest, but from a length biased version. From a Bayesian perspective, efforts to computationally evaluate posterior quantities conditionally on length biased data were hindered by the inability to circumvent the problem of a normalizing constant. In this talk a novel Bayesian nonparametric approach to the length bias sampling problem is presented which circumvents the issue of the normalizing constant. Numerical illustrations as well as a real data example are presented and the estimator is compared against its frequentist counterpart, the kernel density estimator for indirect data.&amp;quot; This is joint work with: a) Spyridon J. Hatjispyros, University of the Aegean, Greece. b)Stephen G. Walker, University of Texas at Austin, U.S.A.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Éric Marchand: On improved predictive density estimation with parametric constraints</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-04-05/</link>
      <pubDate>Fri, 05 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-04-05/</guid>
      <description>&lt;h4 id=&#34;date-2013-04-05&#34;&gt;Date: 2013-04-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider the problem of predictive density estimation under Kullback-Leibler loss when the parameter space is restricted to a convex subset.   The principal situation analyzed relates to the estimation of an unknown predictive p-variate normal density based on an observation generated by another p-variate normal density.  The means of the densities are assumed to coincide, the covariance matrices are a known multiple of the identity matrix.   We obtain sharp results concerning plug-in estimators, we show that the best unrestricted invariant predictive density estimator is dominated by the Bayes estimator associated with a uniform prior on the restricted parameter space, and we obtain minimax results for cases where the parameter space is (i) a cone, and (ii) a ball.  A key feature, which we will describe, is a correspondence between the predictive density estimation problem with a collection of point estimation problems. Finally, if time permits, we describe recent work concerning : (i) non-normal models, and (ii) analysis relative to other loss functions such as reverse Kullback-Leibler and integrated L2.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Jiahua Chen: Quantile and quantile function estimations under density ratio model</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-03-15/</link>
      <pubDate>Fri, 15 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-03-15/</guid>
      <description>&lt;h4 id=&#34;date-2013-03-15&#34;&gt;Date: 2013-03-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Join work with Yukun Liu (East China Normal University)&lt;/p&gt;&#xA;&lt;p&gt;Population quantiles and their functions are important parameters in many applications. For example, the lower level quantiles often serve as crucial quality indices of forestry products and others. In the presence of several independent samples from populations satisfying density ratio model, we investigate the properties of the empirical likelihood (EL) based inferences of quantiles and their functions. In this paper, we first establish the consistency and asymptotic normality of the estimators of parameters and cumulative distributions. The induced EL quantile estimators are then shown to admit Bahadur representation. The results are used to construct asymptotically valid confidence intervals&#xA;for functions of quantiles. In addition, we rigorously prove that the EL quantiles based on all samples are more efficient than the empirical quantiles which can only utilize information from individual samples. Simulation study shows that the EL quantiles and their functions have superior performances both when the density ratio model assumption is satisfied and mildly violated. An application example is used to demonstrate the new methods and potential cost savings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Natalia Stepanova: On asymptotic efficiency of some nonparametric tests for testing multivariate independence</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-03-01/</link>
      <pubDate>Fri, 01 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-03-01/</guid>
      <description>&lt;h4 id=&#34;date-2013-03-01&#34;&gt;Date: 2013-03-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Some problems of statistics can be reduced to extremal problems of minimizing functionals of smooth functions defined on the cube $[0,1]^m$, $m\geq 2$. In this talk, we consider a class of  extremal problems that is closely connected to the problem of testing multivariate independence. By solving the extremal problem, we provide a unified approach to establishing weak convergence for a wide class&#xA;of empirical processes which emerge in connection with testing multivariate independence. The use of our result will be also illustrated by describing the domain of local asymptotic optimality of some nonparametric tests of independence.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Eric Cormier: Data Driven Nonparametric Inference for Bivariate Extreme-Value Copulas</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-02-15/</link>
      <pubDate>Fri, 15 Feb 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-02-15/</guid>
      <description>&lt;h4 id=&#34;date-2013-02-15&#34;&gt;Date: 2013-02-15&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;It is often crucial to know whether the dependence structure of a bivariate distribution belongs to the class of extreme-­‐value copulas. In this talk, I will describe a graphical tool that allows judgment regarding the existence of extreme-­‐value dependence. I will also present a data-­‐ driven nonparametric estimator of the Pickands dependence function. This estimator, which is constructed from constrained b-­‐splines, is intrinsic and differentiable, thereby enabling sampling from the fitted model. I will illustrate its properties via simulation. This will lead me to highlight some of the limitations associated with currently available tests of extremeness.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Celia Greenwood: Multiple testing and region-based tests of rare genetic variation</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-02-08/</link>
      <pubDate>Fri, 08 Feb 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-02-08/</guid>
      <description>&lt;h4 id=&#34;date-2013-02-08&#34;&gt;Date: 2013-02-08&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the context of univariate association tests between a trait of interest and common genetic variants (SNPs) across the whole genome, corrections for multiple testing have been well-studied. Due to the patterns of correlation (i.e. linkage disequilibrium), the number of independent tests remains close to 1 million, even when many more common genetic markers are available. With the advent of the DNA sequencing era, however, newly-identified genetic variants tend to be rare or even unique, and consequently single-variant tests of association have little power. As a result, region-based tests of association are being developed that examine associations between the trait and all the genetic variability in a small pre-defined region of the genome. However, coping with multiple testing in this situation has had little attention. I will discuss two aspects of multiple testing for region-based tests. First, I will describe a method for estimating the effective number of independent tests, and second, I will discuss an approach for controlling type I error that is based stratified false discovery rates, where strata are defined by external information such as genomic annotation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Daniela Witten: Structured learning of multiple Gaussian graphical models</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-02-01/</link>
      <pubDate>Fri, 01 Feb 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-02-01/</guid>
      <description>&lt;h4 id=&#34;date-2013-02-01&#34;&gt;Date: 2013-02-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;I will consider the task of estimating high-dimensional Gaussian graphical models (or networks) corresponding to a single set of features under several distinct conditions. In other words, I wish to estimate several distinct but related networks. I assume that most aspects of the networks are shared, but that there are some structured differences between them. The goal is to exploit the similarity among the networks in order to obtain more accurate estimates of each individual network, as well as to identify the differences between the networks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mylène Bédard: On the empirical efficiency of local MCMC algorithms with pools of proposals</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-01-25/</link>
      <pubDate>Fri, 25 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-01-25/</guid>
      <description>&lt;h4 id=&#34;date-2013-01-25&#34;&gt;Date: 2013-01-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In an attempt to improve on the Metropolis algorithm, various MCMC methods with auxiliary variables, such as the multiple-try and delayed rejection Metropolis algorithms, have been proposed. These methods generate several candidates in a single iteration; accordingly they are computationally more intensive than the Metropolis algorithm. It is usually difficult to provide a general estimate for the computational cost of a method without being overly conservative; potentially efficient methods could thus be overlooked by relying on such estimates. In this talk, we describe three algorithms with auxiliary variables - the multiple-try Metropolis (MTM) algorithm, the multiple-try Metropolis hit-and-run (MTM-HR) algorithm, and the delayed rejection Metropolis algorithm with antithetic proposals (DR-A) - and investigate the net performance of these algorithms in various contexts. To allow for a fair comparison, the study is carried under optimal mixing conditions for each of these algorithms. The DR-A algorithm, whose proposal scheme introduces correlation in the pool of candidates, seems particularly promising. The algorithms are used in the contexts of Bayesian logistic regressions and classical inference for a linear regression model. This talk is based on work in collaboration with M. Mireuta, E. Moulines, and R. Douc.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ana Best: Risk-set sampling, left truncation, and Bayesian methods in survival analysis</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-01-11/</link>
      <pubDate>Fri, 11 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-01-11/</guid>
      <description>&lt;h4 id=&#34;date-2013-01-11&#34;&gt;Date: 2013-01-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statisticians are often faced with budget concerns when conducting studies. The collection of some covariates, such as genetic data, is very expensive. Other covariates, such as detailed histories, might be difficult or time-consuming to measure. This helped bring about the invention of the nested case-control study, and its more generalized version, risk-set sampled survival analysis. The literature has a good discussion of the properties of risk-set sampling in standard right-censored survival data. My interest is in extending the methods of risk-set sampling to left-truncated survival data, which arise in prevalent longitudinal studies. Since prevalent studies are easier and cheaper to conduct than incident studies, this extension is extremely practical and relevant. I will introduce the partial likelihood in this scenario, and briefly discuss the asymptotic properties of my estimator. I will also introduce Bayesian methods for standard survival analysis, and discuss methods for analyzing risk-set-sampled survival data using Bayesian methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sample size and power determination for multiple comparison procedures aiming at rejecting at least r among m false hypotheses</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-12-07/</link>
      <pubDate>Fri, 07 Dec 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-12-07/</guid>
      <description>&lt;h4 id=&#34;date-2012-12-07&#34;&gt;Date: 2012-12-07&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Multiple testing problems arise in a variety of situations, notably in clinical trials with multiple endpoints. In such cases, it is often of interest to reject either all hypotheses or at least one of them. More generally, the question arises as to whether one can reject at least r out of m hypotheses. Statistical tools addressing this issue are rare in the literature. In this talk, I will recall well-known hypothesis testing concepts, both in a single- and in a multiple-hypothesis context. I will then present general power formulas for three important multiple comparison procedures: the Bonferroni and Hochberg procedures, as well as Holm’s sequential procedure. Next, I will describe an R package that we developed for sample size calculations in multiple endpoints trials where it is desired to reject at least r out of m hypotheses. This package covers the case where all the variables are continuous and four common variance-covariance patterns. I will show how to use this package to compute the sample size needed in a real-life application.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sharing confidential datasets using differential privacy</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-11-30/</link>
      <pubDate>Fri, 30 Nov 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-11-30/</guid>
      <description>&lt;h4 id=&#34;date-2012-11-30&#34;&gt;Date: 2012-11-30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;While statistical agencies would like to share their data with researchers, they must also protect the confidentiality of the data provided by their respondents. To satisfy these two conflicting objectives, agencies use various techniques to restrict and modify the data before publication. Most of these techniques however share a common flaw: their confidentiality protection can not be rigorously measured. In this talk, I will present the criterion of differential privacy, a rigorous measure of the protection offered by such methods. Designed to guarantee confidentiality even in a worst-case scenario, differential privacy protects the information of any individual in the database against an adversary with complete knowledge of the rest of the dataset. I will first give a brief overview of recent and current research on the topic of differential privacy. I will then focus on the publication of differentially-private synthetic contingency tables and present some of my results on the methods for the generation and proper analysis of such datasets.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Copula-based regression estimation and Inference</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-11-16/</link>
      <pubDate>Fri, 16 Nov 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-11-16/</guid>
      <description>&lt;h4 id=&#34;date-2012-11-16&#34;&gt;Date: 2012-11-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this paper we investigate a new approach of estimating a regression function based on copulas. The main idea behind this approach is to write the regression function in terms of a copula and marginal distributions. Once the copula and the marginal distributions are estimated we use the plug-in method to construct the new estimator. Because various methods are available in the literature for estimating both a copula and a distribution, this idea provides a rich and flexible alternative to many existing regression estimators. We provide some asymptotic results related to this copula-based regression modeling when the copula is estimated via profile likelihood and the marginals are estimated nonparametrically. We also study the finite sample performance of the estimator and illustrate its usefulness by analyzing data from air pollution studies.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The multidimensional edge: Seeking hidden risks</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-11-09/</link>
      <pubDate>Fri, 09 Nov 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-11-09/</guid>
      <description>&lt;h4 id=&#34;date-2012-11-09&#34;&gt;Date: 2012-11-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Assessing tail risks using the asymptotic models provided by multivariate extreme value theory has the danger that when asymptotic independence is present (as with the Gaussian copula model), the&#xA;asymptotic model provides estimates of probabilities of joint tail regions that are zero. In diverse applications such as finance, telecommunications, insurance and environmental science, it may be difficult to believe in the absence of risk contagion. This problem can be partly ameliorated by using hidden regular variation which assumes a lower order asymptotic behavior on a subcone of the state space and this theory can be made more flexible by extensions in the following directions: (i) higher dimensions than two; (ii) where the lower order variation on a subcone is of extreme value type different from regular variation; and (iii) where the concept is extended to searching for lower order behavior on the complement of the support of the limit measure of regular variation. We discuss some challenges and potential applications to this ongoing effort.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multivariate extremal dependence: Estimation with bias correction</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-11-02/</link>
      <pubDate>Fri, 02 Nov 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-11-02/</guid>
      <description>&lt;h4 id=&#34;date-2012-11-02&#34;&gt;Date: 2012-11-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Estimating extreme risks in a multivariate framework is highly connected with the estimation of the extremal dependence structure. This structure can be described via the stable tail dependence function L, for which several estimators have been introduced. Asymptotic normality is available for empirical estimates of L, with rate of convergence k^1/2, where k denotes the number of high order statistics used in the estimation. Choosing a higher k might be interesting for an improved accuracy of the estimation, but may lead to an increased asymptotic bias. We provide a bias correction procedure for the estimation of L. Combining estimators of L is done in such a way that the asymptotic bias term disappears. The new estimator of L is shown to allow more flexibility in the choice of k. Its asymptotic behavior is examined, and a simulation study is provided to assess its small sample behavior. This is a joint work with Cécile Mercadier (Université Lyon 1) and Laurens de Haan (Erasmus University Rotterdam).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simulation model calibration and prediction using outputs from multi-fidelity simulators</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-10-26/</link>
      <pubDate>Fri, 26 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-10-26/</guid>
      <description>&lt;h4 id=&#34;date-2012-10-26&#34;&gt;Date: 2012-10-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Computer simulators are used widely to describe physical processes in lieu of physical observations. In some cases, more than one computer code can be used to explore the same physical system - each with different degrees of fidelity. In this work, we combine field observations and model runs from deterministic multi-fidelity computer simulators to build a predictive model for the real process. The resulting model can be used to perform sensitivity analysis for the system and make predictions with associated measures of uncertainty. Our approach is Bayesian and will be illustrated through a simple example, as well as a real application in predictive science at the Center for Radiative Shock Hydrodynamics at the University of Michigan.&lt;/p&gt;</description>
    </item>
    <item>
      <title>	Modeling operational risk using a Bayesian approach to EVT</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-10-12/</link>
      <pubDate>Fri, 12 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-10-12/</guid>
      <description>&lt;h4 id=&#34;date-2012-10-12&#34;&gt;Date: 2012-10-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Extreme Value Theory has been widely used for assessing risk for highly unusual events, either by using block maxima or peaks over the threshold (POT) methods. However, one of the main drawbacks of the POT method is the choice of a threshold, which plays an important role in the estimation since the parameter estimates strongly depend on this value. Bayesian inference is an alternative to handle these difficulties; the threshold can be treated as another parameter in the estimation, avoiding the classical empirical approach. In addition, it is possible to incorporate internal and external observations in combination with expert opinion, providing a natural, probabilistic framework in which to evaluate risk models. In this talk, we analyze operational risk data using a mixture model which combines a parametric form for the center and a GPD for the tail of the distribution, using all observations for inference about the unknown parameters from both distributions, the threshold included. A Bayesian analysis is performed and inference is carried out through Markov Chain Monte Carlo (MCMC) methods in order to determine the minimum capital requirement for operational risk.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Markov switching regular vine copulas</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-10-05/</link>
      <pubDate>Fri, 05 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-10-05/</guid>
      <description>&lt;h4 id=&#34;date-2012-10-05&#34;&gt;Date: 2012-10-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Using only bivariate copulas as building blocks, regular vines(R-vines) constitute a flexible class of high-dimensional dependence models. In this talk we introduce a Markov switching R-vine copula model, combining the flexibility of general R-vine copulas with the possibility for dependence structures to change over time. Frequentist as well as Bayesian parameter estimation is discussed. Further, we apply the newly proposed model to examine the dependence of exchange rates as well as stock and stock index returns. We show that changes in dependence are usually closely interrelated with periods of market stress. In such times the Value at Risk of an asset portfolio is significantly underestimated when changes in the dependence structure are ignored.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The current state of Q-learning for personalized medicine</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-09-28/</link>
      <pubDate>Fri, 28 Sep 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-09-28/</guid>
      <description>&lt;h4 id=&#34;date-2012-09-28&#34;&gt;Date: 2012-09-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, I will provide an introduction to DTRs and an overview the state of the art (and science) of Q-learning, a popular tool in reinforcement learning. The use of Q-learning and its variance in randomized and non-randomized studies will be discussed, as well as issues concerning inference as the resulting estimators are not always regular. Current and future directions of interest will also be considered.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hypothesis testing in finite mixture models: from the likelihood ratio test to EM-test</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-04-05/</link>
      <pubDate>Thu, 05 Apr 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-04-05/</guid>
      <description>&lt;h4 id=&#34;date-2012-04-05&#34;&gt;Date: 2012-04-05&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the presence of heterogeneity, a mixture model is most natural to characterize the random behavior of the samples taken from such populations. Such strategy has been widely employed in applications ranging from genetics, information technology, marketing, to finance. Studying the mixing structure behind a random sample from the population allows us to infer the degree of heterogeneity with important implications in applications such as the presence of disease subgroups in genetics. The statistical problem is to test the hypotheses on the order of the finite mixture models. There has been continued interest in the limiting behavior of the likelihood ratio tests. The non-regularity of the finite mixture models has provided statisticians ample examples of unusual limiting distributions. Yet many of such results are not convenient for conducting hypothesis tests. Motivated at overcoming such difficulties, we have developed a number of strategies to obtain tests with high efficiency yet easy to use limiting distributions. The latest development is a class of EM-tests which are advantageous in many respects. Their limiting distributions are easier to derive mathematically, simple for implementation in data analysis and valid for more general class of mixture models without restrictions on the space of the mixing distribution. The simulation indicates the limiting distributions have good precision at approximating the finite sample distributions in the examples investigated.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A matching-based approach to assessing the surrogate value of a biomarker</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-03-30/</link>
      <pubDate>Fri, 30 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-03-30/</guid>
      <description>&lt;h4 id=&#34;date-2012-03-30&#34;&gt;Date: 2012-03-30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statisticians have developed a number of frameworks which can be used to assess the surrogate value of a biomarker, i.e. establish whether treatment effects on a biological quantity measured shortly after administration of treatment predict treatment effects on the clinical endpoint of interest. The most commonly applied of these frameworks is due to Prentice (1989), who proposed a set of criteria which a surrogate marker should satisfy. However, verifying these criteria using observed data can be challenging due to the presence of unmeasured simultaneous predictors (i.e. confounders) which influence both the potential surrogate and the outcome. In this work, we adapt a technique proposed by Rosenbaum (2002) for observational studies, in which observations are matched and the odds of treatment within each matched pair is bounded. This yields a straightforward and interpretable sensitivity analysis which can be performed particularly efficiently for certain types of test statistics. In this talk, I will introduce the surrogate endpoint problem, discuss the details of my proposed technique for assessing surrogate value, and illustrate with some simulated examples inspired by the problem of identifying immune surrogates in HIV vaccine trials.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Model selection principles in misspecified models</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-03-23/</link>
      <pubDate>Fri, 23 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-03-23/</guid>
      <description>&lt;h4 id=&#34;date-2012-03-23&#34;&gt;Date: 2012-03-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Model selection is of fundamental importance to high-dimensional modeling featured in many contemporary applications. Classical principles of model selection include the Bayesian principle and the Kullback-Leibler divergence principle, which lead to the Bayesian information criterion and Akaike information criterion, respectively, when models are correctly specified. Yet model misspecification is unavoidable in practice. We derive novel asymptotic expansions of the two well-known principles in misspecified generalized linear models, which give the generalized BIC (GBIC) and generalized AIC. A specific form of prior probabilities motivated by the Kullback-Leibler divergence principle leads to the generalized BIC with prior probability ($\mbox{GBIC}_p$), which can be naturally decomposed as the sum of  the negative maximum quasi-log-likelihood, and a penalty on model dimensionality, and a penalty on model misspecification directly. Numerical studies demonstrate the advantage of the new methods for model selection in both correctly specified and misspecified models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variable selection in longitudinal data with a change-point</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-03-16/</link>
      <pubDate>Fri, 16 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-03-16/</guid>
      <description>&lt;h4 id=&#34;date-2012-03-16&#34;&gt;Date: 2012-03-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Follow-up studies are frequently carried out to investigate the evolution of measurements through time, taken on a set of subjects. These measurements (responses) are bound to be influenced by subject specific covariates and if a regression model is used the data analyst is faced with the problem of selecting those covariates that “best explain” the data. For example, in a clinical trial, subjects may be monitored for a response following the administration of a treatment with a view of selecting the covariates that are best predictive of a treatment response. This variable selection setting is standard. However, more realistically, there will often be an unknown delay from the administration of a treatment before it has a measurable effect. This delay will not be directly observable since it is a property of the distribution of responses rather than of any particular trajectory of responses. Briefly, each subject will have an unobservable change-point. With a change-point component added, the variable selection problem necessitates the use of penalized likelihood methods. This is because the number of putative covariates for the responses, as well as the change-point distribution, could be large relative to the follow-up time and/or the number of subjects; variable selection in a change-point setting does not appear to have been studied in the literature. In this talk I will briefly introduce the multi-path change-point problem. I will show how variable selection for the covariates before the change, after the change, as well as for the change-point distribution, reduces to variable selection for a finite mixture of multivariate distributions. I will discuss the performance of my model selection methods using an example on cognitive decline in subjects with Alzheimer’s disease and through simulations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Estimating a variance-covariance surface for functional and longitudinal data</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-03-02/</link>
      <pubDate>Fri, 02 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-03-02/</guid>
      <description>&lt;h4 id=&#34;date-2012-03-02&#34;&gt;Date: 2012-03-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In functional data analysis, as in its multivariate counterpart, estimates of the bivariate covariance kernel σ(s,t ) and its inverse are useful for many things, and we need the inverse of a covariance matrix or kernel especially often.  However, the dimensionality of functional observations often exceeds the sample size available to estimate σ(s,t, and then the analogue S of the multivariate sample estimate is singular and non-invertible.   Even when this is not the case, the high dimensionality S often implies unacceptable sample variability and loss of degrees of freedom for model fitting.   The common practice of employing low-dimensional principal component approximations to σ(s,t) to achieve invertibility also raises serious issues.&lt;/p&gt;</description>
    </item>
    <item>
      <title>McGillivray: A penalized quasi-likelihood approach for estimating the number of states in a hidden Markov model | Best: Risk-set sampling and left truncation in survival analysis</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-02-17/</link>
      <pubDate>Fri, 17 Feb 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-02-17/</guid>
      <description>&lt;h4 id=&#34;date-2012-02-17&#34;&gt;Date: 2012-02-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;McGillivray&lt;/em&gt;: In statistical applications of hidden Markov models (HMMs), one may have no knowledge of the number of hidden states (or order) of the model needed to be able to accurately represent the underlying process of the data. The problem of estimating the number of hidden states of the HMM is thus brought to the forefront. In this talk, we present a penalized quasi-likelihood approach for order estimation in HMMs which makes use of the fact that the marginal distribution of the observations from a HMM is a finite mixture model. The method starts with a HMM with a large number of states and obtains a model of lower order by clustering and combining similar states of the model through two penalty functions. We assess the performance of the new method via extensive simulation studies for Normal and Poisson HMMs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Du: Simultaneous fixed and random effects selection in finite mixtures of linear mixed-effects models | Harel: Measuring fatigue in systemic sclerosis: a comparison of the SF-36 vitality subscale and FACIT fatigue scale using item response theory</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-02-03/</link>
      <pubDate>Fri, 03 Feb 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-02-03/</guid>
      <description>&lt;h4 id=&#34;date-2012-02-03&#34;&gt;Date: 2012-02-03&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Du&lt;/em&gt;: Linear mixed-effects (LME) models are frequently used for modeling longitudinal data. One complicating factor in the analysis of such data is that samples are sometimes obtained from a population with significant underlying heterogeneity, which would be hard to capture by a single LME model. Such problems may be addressed by a finite mixture of linear mixed-effects (FMLME) models, which segments the population into subpopulations and models each subpopulation by a distinct LME model. Often in the initial stage of a study, a large number of predictors are introduced. However, their associations to the response variable vary from one component to another of the FMLME model. To enhance predictability and to obtain a parsimonious model, it is of great practical interest to identify the important effects, both fixed and random, in the model. Traditional variable selection techniques such as stepwise deletion and subset selection are computationally expensive as the number of covariates and components in the mixture model increases. In this talk, we introduce a penalized likelihood approach and propose a nested EM algorithm for efficient numerical computations. Our estimators are shown to possess desirable properties such as consistency, sparsity and asymptotic normality. We illustrate the performance of our method through simulations and a systemic sclerosis data example.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Applying Kalman filtering to problems in causal inference</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-01-27/</link>
      <pubDate>Fri, 27 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-01-27/</guid>
      <description>&lt;h4 id=&#34;date-2012-01-27&#34;&gt;Date: 2012-01-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A common problem in observational studies is estimating the causal effect of time-varying treatment in the presence of a time varying confounder.  When random assignment of subjects to comparison groups is not possible, time-varying confounders can cause bias in estimating causal effects even after standard regression adjustment if past treatment history is a predictor of future confounders. To eliminate the bias of standard methods for estimating the causal effect of time varying treatment, Robins developed a number of innovative methods for discrete treatment levels, including G-computation, G-estimation, and marginal structural models (MSMs).   However, there does not currently exist straight-forward applications of G-Estimation and MSMs for continuous treatment.  In this talk, I will introduce an alternative approach to previous methods which utilize the Kalman filter. The key advantage to the Kalman filter approach is that the model easily accommodates continuous levels of treatment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A concave regularization technique for sparse mixture models</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-01-20/</link>
      <pubDate>Fri, 20 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-01-20/</guid>
      <description>&lt;h4 id=&#34;date-2012-01-20&#34;&gt;Date: 2012-01-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Latent variable mixture models are a powerful tool for exploring the structure in large datasets. A common challenge for interpreting such models is a desire to impose sparsity, the natural assumption that each data point only contains few latent features. Since mixture distributions are constrained in their L1 norm, typical sparsity techniques based on L1 regularization become toothless, and concave regularization becomes necessary. Unfortunately concave regularization typically results in EM algorithms that must perform problematic non-convex M-step optimization. In this work, we introduce a technique&#xA;for circumventing this difficulty, using the so-called Mountain Pass Theorem to provide easily verifiable conditions under which the M-step is well-behaved despite the lacking convexity. We also develop a correspondence between logarithmic regularization and what we term the pseudo-Dirichlet distribution, a generalization of the ordinary Dirichlet distribution well-suited for inducing sparsity. We demonstrate our approach on a text corpus, inferring a sparse topic mixture model for 2,406 weblogs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Path-dependent estimation of a distribution under generalized censoring</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-12-02/</link>
      <pubDate>Fri, 02 Dec 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-12-02/</guid>
      <description>&lt;h4 id=&#34;date-2011-12-02&#34;&gt;Date: 2011-12-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This talk focuses on the problem of the estimation of a distribution on an arbitrary complete separable metric space when the data points are subject to censoring by a general class of random sets. A path-dependent estimator for the distribution is proposed; among other properties, the estimator is sequential in the sense that it only uses data preceding any fixed point at which it is evaluated. If the censoring mechanism is totally ordered, the paths may be chosen in such a way that the estimate of the distribution defines a measure. In this case, we can prove a functional central limit theorem for the estimator when the underlying space is Euclidean. This is joint work with Gail Ivanoff (University of Ottawa)&lt;/p&gt;</description>
    </item>
    <item>
      <title>Estimation of the risk of a collision when using a cell phone while driving</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-11-25/</link>
      <pubDate>Fri, 25 Nov 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-11-25/</guid>
      <description>&lt;h4 id=&#34;date-2011-11-25&#34;&gt;Date: 2011-11-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The use of cell phone while driving raises the question of whether it is associated with an increased collision risk and if so, what is its magnitude. For policy decision making, it is important to rely on an accurate estimate of the real crash risk of cell phone use while driving. Three important epidemiological studies were published on the subject, two using the case-crossover approach  and one using a more conventional longitudinal cohort design. The methodology and results of these studies will be presented and discussed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Construction of bivariate distributions via principal components</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-11-18/</link>
      <pubDate>Fri, 18 Nov 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-11-18/</guid>
      <description>&lt;h4 id=&#34;date-2011-11-18&#34;&gt;Date: 2011-11-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The diagonal expansion of a bivariate distribution (Lancaster, 1958) has been used as a tool to construct bivariate distributions; this method has been generalized using principal dimensions of random variables (Cuadras 2002). Sufficient and necessary conditions are given for uniform, exponential, logistic and Pareto marginals in the one and two-dimensional case. The corresponding copulas are obtained.&lt;/p&gt;&#xA;&lt;h2 id=&#34;speaker&#34;&gt;Speaker&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Amparo Casanova&lt;/em&gt; is an Assistant Professor at the Dalla Lana School of Public Health, Division of Biostatistics, University of Toronto.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Bayesian method of parametric inference for diffusion processes</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-11-04/</link>
      <pubDate>Fri, 04 Nov 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-11-04/</guid>
      <description>&lt;h4 id=&#34;date-2011-11-04&#34;&gt;Date: 2011-11-04&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Diffusion processes have been used to model a multitude of continuous-time phenomena in Engineering and the Natural Sciences, and as in this case, the volatility of financial assets.  However, parametric inference has long been complicated by an intractable likelihood function.  For many models the most effective solution involves a large amount of missing data for which the typical Gibbs sampler can be arbitrarily slow.  On the other hand, joint parameter and missing data proposals can lead to a radical improvement, but their acceptance rate tends to scale exponentially with the number of observations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum likelihood estimation in network models</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-11-03/</link>
      <pubDate>Thu, 03 Nov 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-11-03/</guid>
      <description>&lt;h4 id=&#34;date-2011-11-03&#34;&gt;Date: 2011-11-03&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1600-1700&#34;&gt;Time: 16:00-17:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This talk is concerned with maximum likelihood estimation (MLE) in exponential statistical models for networks (random graphs) and, in particular, with the beta model, a simple model for undirected graphs in which the degree sequence is the minimal sufficient statistic. The speaker will present necessary and sufficient conditions for the existence of the MLE of the beta model parameters that are based on a geometric object known as the polytope of degree sequences. Using this result, it is possible to characterize in a combinatorial fashion sample points leading to a non-existent MLE and non-estimability of the probability parameters under a non-existent MLE. The speaker will further indicate some conditions guaranteeing that the MLE exists with probability tending to 1 as the number of nodes increases. Much of this analysis applies also to other well-known models for networks, such as the Rasch model, the Bradley-Terry model and the more general p1 model of Holland and Leinhardt. These results are in fact instantiations of rather general geometric properties of exponential families with polyhedral support that will be illustrated with a simple exponential random graph model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simulated method of moments estimation for copula-based multivariate models</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-10-28/</link>
      <pubDate>Fri, 28 Oct 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-10-28/</guid>
      <description>&lt;h4 id=&#34;date-2011-10-28&#34;&gt;Date: 2011-10-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1500-1600&#34;&gt;Time: 15:00-16:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This paper considers the estimation of the parameters of a copula via a simulated method of moments type approach. This approach is attractive when the likelihood of the copula model is not known in closed form, or when the researcher has a set of dependence measures or other functionals of the copula, such as pricing errors, that are of particular interest. The proposed approach naturally also nests method of moments and generalized method of moments estimators. Combining existing results on simulation based estimation with recent results from empirical copula process theory, we show the consistency and asymptotic normality of the proposed estimator, and obtain a simple test of over-identifying restrictions as a goodness-of-fit test. The results apply to both iid and time series data. We analyze the finite-sample behavior of these estimators in an extensive simulation study. We apply the model to a group of seven financial stock returns and find evidence of statistically significant tail dependence, and that the dependence between these assets is stronger in crashes than booms.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian modelling of GWAS data using linear mixed models</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-10-21/</link>
      <pubDate>Fri, 21 Oct 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-10-21/</guid>
      <description>&lt;h4 id=&#34;date-2011-10-21&#34;&gt;Date: 2011-10-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Genome-wide association studies (GWAS) are used to identify physical positions (loci) on the genome where genetic variation is causally associated with a phenotype of interest at the population level. Typical studies are based on the measurement of several hundred thousand single nucleotide polymorphism (SNP) variants spread across the genome, in a few thousand individuals. The resulting datasets are large and require computationally efficient methods of statistical analysis.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Nonexchangeability and radial asymmetry identification via bivariate quantiles, with financial applications</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-10-07/</link>
      <pubDate>Fri, 07 Oct 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-10-07/</guid>
      <description>&lt;h4 id=&#34;date-2011-10-07&#34;&gt;Date: 2011-10-07&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, the following topics will be discussed: A class of bivariate probability integral transforms and Kendall distribution; bivariate quantile curves, central and lateral regions; non-exchangeability and radial asymmetry identification; new measures of nonexchangeability and radial asymmetry; financial applications and a few open problems (joint work with Flavio Ferreira).&lt;/p&gt;&#xA;&lt;h2 id=&#34;speaker&#34;&gt;Speaker&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Nikolai Kolev&lt;/em&gt; is a Professor of Statistics at the University of Sao Paulo, Brazil.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data sketching for cardinality and entropy estimation?</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-09-30/</link>
      <pubDate>Fri, 30 Sep 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-09-30/</guid>
      <description>&lt;h4 id=&#34;date-2011-09-30&#34;&gt;Date: 2011-09-30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Streaming data is ubiquitous in a wide range of areas from engineering and information technology, finance, and commerce, to atmospheric physics, and earth sciences. The online approximation of properties of data streams is of great interest, but this approximation process is hindered by the sheer size of the data and the speed at which it is generated. Data stream algorithms typically allow only one pass over the data, and maintain sub-linear representations of the data from which target properties can be inferred with high efficiency.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is singular learning theory?</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-09-23/</link>
      <pubDate>Fri, 23 Sep 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-09-23/</guid>
      <description>&lt;h4 id=&#34;date-2011-09-23&#34;&gt;Date: 2011-09-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, we give a basic introduction to Sumio Watanabe&amp;rsquo;s&#xA;Singular Learning Theory, as outlined in his book &amp;ldquo;Algebraic Geometry&#xA;and Statistical Learning Theory&amp;rdquo;. Watanabe&amp;rsquo;s key insight to studying&#xA;singular models was to use a deep result in algebraic geometry known&#xA;as Hironaka&amp;rsquo;s Resolution of Singularities. This result allows him to&#xA;reparametrize the model in a normal form so that central limit&#xA;theorems can be applied. In the second half of the talk, we discuss&#xA;new algebraic methods where we define fiber ideals for discrete/Gaussian models. We show that the key to understanding the singular model lies in monomializing its fiber ideal.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Inference and model selection for pair-copula constructions</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-09-16/</link>
      <pubDate>Fri, 16 Sep 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-09-16/</guid>
      <description>&lt;h4 id=&#34;date-2011-09-16&#34;&gt;Date: 2011-09-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Pair-copula constructions (PCCs) provide an elegant way to construct highly flexible multivariate distributions. However, for convenience of inference, pair-copulas are often assumed to depend on the conditioning variables only indirectly. In this talk, I will show how nonparametric smoothing techniques can be used to avoid this assumption. Model selection for PCCs will also be addressed within the proposed method.&lt;/p&gt;&#xA;&lt;h2 id=&#34;speaker&#34;&gt;Speaker&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Elif F. Acar&lt;/em&gt; is a Postdoctoral Fellow in the Department of Mathematics and Statistics at McGill University. She holds a Ph.D. in Statistics from the University of Toronto.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Precision estimation for stereological volumes</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-08-31/</link>
      <pubDate>Wed, 31 Aug 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-08-31/</guid>
      <description>&lt;h4 id=&#34;date-2011-08-31&#34;&gt;Date: 2011-08-31&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Volume estimators based on Cavalieri’s principle are widely used in the bio-&#xA;sciences. For example in neuroscience, where volumetric measurements of brain&#xA;structures are of interest, systematic samples of serial sections are obtained by&#xA;magnetic resonance imaging or by a physical cutting procedure. The volume v is&#xA;then estimated by ˆv, which is the sum over the areas of the structure of interest in&#xA;the section planes multiplied by the width of the sections, t &amp;gt; 0.&#xA;Assessing the precision of such volume estimates is a question of great practical&#xA;importance, but statistically a challenging task due to the strong spatial dependence&#xA;of the data and typically small sample sizes. In this talk, an overview of classical&#xA;and new approaches to this problem will be presented. A special focus will be given&#xA;to some recent advances on distribution estimators and confidence intervals for ˆv;&#xA;see Hall and Ziegel (2011).&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
