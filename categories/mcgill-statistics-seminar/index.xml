<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mcgill Statistics Seminar on McGill Statistics Seminars</title>
    <link>/categories/mcgill-statistics-seminar/</link>
    <description>Recent content in Mcgill Statistics Seminar on McGill Statistics Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/mcgill-statistics-seminar/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Magic Cross-Validation Theory for Large-Margin Classification</title>
      <link>/post/2019winter/2019-01-11/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019winter/2019-01-11/</guid>
      <description>Date: 2019-01-11 Time: 15:30-16:30 Location: BURN 1104 Abstract: Cross-validation (CV) is perhaps the most widely used tool for tuning supervised machine learning algorithms in order to achieve better generalization error rate. In this paper, we focus on leave-one-out cross-validation (LOOCV) for the support vector machine (SVM) and related algorithms. We first address two wide-spreading misconceptions on LOOCV. We show that LOOCV, ten-fold, and five-fold CV are actually well-matched in estimating the generalization error, and the computation speed of LOOCV is not necessarily slower than that of ten-fold and five-fold CV.</description>
    </item>
    
    <item>
      <title>p-values vs Bayes factors: Is there a compromise?</title>
      <link>/post/2018fall/2018-11-23/</link>
      <pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-11-23/</guid>
      <description>Date: 2018-11-23 Time: 15:30-16:30 Location: BURN 1104 Abstract: This is not a research talk. Rather, the goal is to address the topic of the talk title through a 2017 multi-authored paper published in Nature Human Behaviour. The Nature article proposes that the standard cut-off significance level of .05 should be replaced by a cut-off level of .005 when new discoveries are being claimed. The authors attribute the high proportion of irreducible results in the literature that accompany claimed new discoveries, in part, to the low-bar cut-off of .</description>
    </item>
    
    <item>
      <title>Estimation of the Median Residual Lifetime Function for Length-Biased Failure Time Data</title>
      <link>/post/2018fall/2018-11-16/</link>
      <pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-11-16/</guid>
      <description>Date: 2018-11-16 Time: 15:30-16:30 Location: BURN 1104 Abstract: The median residual lifetime function is a statistical quantity which describes the future point in time at which the probability of current survival has dropped by 50%. In deriving an estimator for the median residual lifetime function for length-biased data, the added features of left-truncation and right-censoring must be taken into account.
In this talk, we give a brief description of length-biased failure time data and show that by using a particular non-parametric estimator for the survival function that it is possible to derive the asymptotically most-efficient non-parametric estimator for the median residual lifetime function.</description>
    </item>
    
    <item>
      <title>Density estimation of mixtures of Gaussians and Ising models</title>
      <link>/post/2018fall/2018-11-09/</link>
      <pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-11-09/</guid>
      <description>Date: 2018-11-09 Time: 15:30-16:30 Location: BURN 1104 Abstract: Density estimation lies at the intersection of statistics, theoretical computer science, and machine learning. We review some old and new results on the sample complexities (also known as minimax convergence rates) of estimating densities of high-dimensional distributions, in particular mixtures of Gaussians and Ising models.
Based on joint work with Hassan Ashtiani, Shai Ben-David, Luc Devroye, Nick Harvey, Christopher Liaw, Yani Plan, and Tommy Reddad.</description>
    </item>
    
    <item>
      <title>Terrorists never congregate in even numbers (and other strange results in fragmentation-coalescence)</title>
      <link>/post/2018fall/2018-11-02/</link>
      <pubDate>Fri, 02 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-11-02/</guid>
      <description>Date: 2018-11-02 Time: 15:30-16:30 Location: BURN 1104 Abstract: The rigorous mathematical treatment of random fragmentation-coalescent models in the literature is difficult to find, and perhaps for good reason. We examine two different types of random fragmentation-coalescent models which produce somewhat unexpected results.
The first concerns an agent-based model in which, with a rate that depends on the configuration of the system, agents coalesce into clusters that also fragment into their individual constituent membership.</description>
    </item>
    
    <item>
      <title>Object Oriented Data Analysis with Application to Neuroimaging Studies</title>
      <link>/post/2018fall/2018-10-26/</link>
      <pubDate>Fri, 26 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-10-26/</guid>
      <description>Date: 2018-10-26 Time: 15:30-16:30 Location: BURN 1104 Abstract: In this talk, I will first briefly introduce my research on object oriented data analysis with application to neuroimaging studies. I will then talk about a detailed example on imaging genetics. In this project, we develop a high-dimensional matrix linear regression model to correlate 2D imaging responses with high-dimensional genetic covariates. We propose a fast and efficient screening procedure based on the spectral norm to deal with the case that the dimension of scalar covariates is much larger than the sample size.</description>
    </item>
    
    <item>
      <title>Multilevel clustering and optimal transport</title>
      <link>/post/2018fall/2018-10-19/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-10-19/</guid>
      <description>Date: 2018-10-19 Time: 15:30-16:30 Location: BURN 1104 Abstract: Optimal transport plays an increasingly relevant and useful role in the theory and application of mixture model based clustering and inference. In this talk I will describe some recent progress in characterizing the convergence behavior of mixing distributions when one fits a mixture model to the data. This theory hinges on the relationship between the space of mixture densities, which is endowed with variational or Hellinger distance, and the space of mixing measures endowed with optimal transport distance metrics.</description>
    </item>
    
    <item>
      <title>Dimension Reduction for Causal Inference</title>
      <link>/post/2018fall/2018-10-05/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-10-05/</guid>
      <description>Date: 2018-10-05 Time: 15:30-16:30 Location: BURN 1104 Abstract: In this talk, we discuss how sufficient dimension reduction can be used to aid causal inference. We propose a new matching approach based on the reduced covariates obtained from sufficient dimension reduction. Compared with the original covariates and the propensity scores, which are commonly used for matching in the literature, the reduced covariates are estimable nonparametrically and are effective in imputing the missing potential outcomes.</description>
    </item>
    
    <item>
      <title>Selective inference for dynamic treatment regimes via the LASSO</title>
      <link>/post/2018fall/2018-09-28/</link>
      <pubDate>Fri, 28 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-09-28/</guid>
      <description>Date: 2018-09-28 Time: 15:30-16:30 Location: BURN 1205 Abstract: Constructing an optimal dynamic treatment regime become complex when there are large number of prognostic factors, such as patient’s genetic information, demographic characteristics, medical history over time. Existing methods only focus on selecting the important variables for the decision-making process and fall short in providing inference for the selected model. We fill this gap by leveraging the conditional selective inference methodology. We show that the proposed method is asymptotically valid given certain rate assumptions in semiparametric regression.</description>
    </item>
    
    <item>
      <title>Possession Sketches: Mapping NBA Strategies</title>
      <link>/post/2018fall/2018-09-21/</link>
      <pubDate>Fri, 21 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-09-21/</guid>
      <description>Date: 2018-09-21 Time: 09:30-10:15 Location: Bronfman Building 001 Abstract: We present Possession Sketches, a new machine learning method for organizing and exploring a database of basketball player-tracks. Our method organizes basketball possessions by offensive structure. We first develop a model for populating a dictionary of short, repeated, and spatially registered actions. Each action corresponds to an interpretable type of player movement. We examine statistical patterns in these actions, and show how they can be used to describe individual player behavior.</description>
    </item>
    
    <item>
      <title>Quantile LASSO in Nonparametric Models with Changepoints Under Optional Shape Constraints</title>
      <link>/post/2018fall/2018-09-14/</link>
      <pubDate>Fri, 14 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-09-14/</guid>
      <description>Date: 2018-09-14 Time: 15:30-16:30 Location: BURN 1104 Abstract: Nonparametric models are popular modeling tools because of their natural overall flexibility. In our approach, we apply nonparametric techniques for panel data structures with changepoints and optional shape constraints and the estimation is performed in a fully data driven manner by utilizing atomic pursuit methods – LASSO regularization techniques in particular. However, in order to obtain robust estimates and, also, to have a more complex insight into the underlying data structure, we target conditional quantiles rather then the conditional mean only.</description>
    </item>
    
    <item>
      <title>Association Measures for Clustered Competing Risks Data</title>
      <link>/post/2018fall/2018-09-07/</link>
      <pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-09-07/</guid>
      <description>Date: 2018-09-07 Time: 15:30-16:30 Location: BURN 1104 Abstract: In this work, we propose a semiparametric model for multivariate clustered competing risks data when the cause-specific failure times and the occurrence of competing risk events among subjects within the same cluster are of interest. The cause-specific hazard functions are assumed to follow Cox proportional hazard models, and the associations between failure times given the same or different cause events and the associations between occurrences of competing risk events within the same cluster are investigated through copula models.</description>
    </item>
    
    <item>
      <title>Methodological challenges in using point-prevalence versus cohort data in risk factor analyses of hospital-acquired infections</title>
      <link>/post/2018winter/2018-04-27/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-04-27/</guid>
      <description>Date: 2018-04-27 Time: 15:30-16:30 Location: BURN 1205 Abstract: To explore the impact of length-biased sampling on the evaluation of risk factors of nosocomial infections in point-prevalence studies. We used cohort data with full information including the exact date of the nosocomial infection and mimicked an artificial one-day prevalence study by picking a sample from this cohort study. Based on the cohort data, we studied the underlying multi-state model which accounts for nosocomial infection as an intermediate and discharge/death as competing events.</description>
    </item>
    
    <item>
      <title>Kernel Nonparametric Overlap-based Syncytial Clustering</title>
      <link>/post/2018winter/2018-04-20/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-04-20/</guid>
      <description>Date: 2018-04-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Standard clustering algorithms can find regular-structured clusters such as ellipsoidally- or spherically-dispersed groups, but are more challenged with groups lacking formal structure or definition. Syncytial clustering is the name that we introduce for methods that merge groups obtained from standard clustering algorithms in order to reveal complex group structure in the data. Here, we develop a distribution-free fully-automated syncytial algorithm that can be used with the computationally efficient k-means or other algorithms.</description>
    </item>
    
    <item>
      <title>Empirical likelihood and robust regression in diffusion tensor imaging data analysis</title>
      <link>/post/2018winter/2018-04-06/</link>
      <pubDate>Fri, 06 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-04-06/</guid>
      <description>Date: 2018-04-06 Time: 15:30-16:30 Location: BURN 1205 Abstract: With modern technology development, functional responses are observed frequently in various scientific fields including neuroimaging data analysis. Empirical likelihood as a nonparametric data-driven technique has become an important statistical inference methodology. In this paper, motivated by diffusion tensor imaging (DTI) data we propose three generalized empirical likelihood-based methods that accommodate within-curve dependence on the varying coefficient model with functional responses and embed a robust regression idea.</description>
    </item>
    
    <item>
      <title>Some development on dynamic computer experiments</title>
      <link>/post/2018winter/2018-03-23/</link>
      <pubDate>Fri, 23 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-03-23/</guid>
      <description>Date: 2018-03-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: Computer experiments refer to the study of real systems using complex simulation models. They have been widely used as efficient, economical alternatives to physical experiments. Computer experiments with time series outputs are called dynamic computer experiments. In this talk, we consider two problems of such experiments: emulation of large-scale dynamic computer experiments and inverse problem. For the first problem, we proposed a computationally efficient modelling approach which sequentially finds a set of local design points based on a new criterion specifically designed for emulating dynamic computer simulators.</description>
    </item>
    
    <item>
      <title>Statistical Genomics for Understanding Complex Traits</title>
      <link>/post/2018winter/2018-03-16/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-03-16/</guid>
      <description>Date: 2018-03-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: Over the last decade, advances in measurement technologies has enabled researchers to generate multiple types of high-dimensional &amp;ldquo;omics&amp;rdquo; datasets for large cohorts. These data provide an opportunity to derive a mechanistic understanding of human complex traits. However, inferring meaningful biological relationships from these data is challenging due to high-dimensionality , noise, and abundance of confounding factors. In this talk, I&amp;rsquo;ll describe statistical approaches for robust analysis of genomic data from large population studies, with a focus on 1) understanding the nature of confounding and approaches for addressing them and 2) understanding the genomic correlates of aging and dementia.</description>
    </item>
    
    <item>
      <title>Sparse Penalized Quantile Regression: Method, Theory, and Algorithm</title>
      <link>/post/2018winter/2018-02-23/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-02-23/</guid>
      <description>Date: 2018-02-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: Sparse penalized quantile regression is a useful tool for variable selection, robust estimation, and heteroscedasticity detection in high-dimensional data analysis. We discuss the variable selection and estimation properties of the lasso and folded concave penalized quantile regression via non-asymptotic arguments. We also consider consistent parameter tuning therein. The computational issue of the sparse penalized quantile regression has not yet been fully resolved in the literature, due to non-smoothness of the quantile regression loss function.</description>
    </item>
    
    <item>
      <title>Methodological considerations for the analysis of relative treatment effects in multi-drug-resistant tuberculosis from fused observational studies</title>
      <link>/post/2018winter/2018-02-09/</link>
      <pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-02-09/</guid>
      <description>Date: 2018-02-09 Time: 15:30-16:30 Location: BURN 1205 Abstract: Multi-drug-resistant tuberculosis (MDR-TB) is defined as strains of tuberculosis that do not respond to at least the two most used anti-TB drugs. After diagnosis, the intensive treatment phase for MDR-TB involves taking several alternative antibiotics concurrently. The Collaborative Group for Meta-analysis of Individual Patient Data in MDR-TB has assembled a large, fused dataset of over 30 observational studies comparing the effectiveness of 15 antibiotics.</description>
    </item>
    
    <item>
      <title>A new approach to model financial data: The Factorial Hidden Markov Volatility Model</title>
      <link>/post/2018winter/2018-02-02/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-02-02/</guid>
      <description>Date: 2018-02-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: A new process, the factorial hidden Markov volatility (FHMV) model, is proposed to model financial returns or realized variances. This process is constructed based on a factorial hidden Markov model structure and corresponds to a parsimoniously parametrized hidden Markov model that includes thousands of volatility states. The transition probability matrix of the underlying Markov chain is structured so that the multiplicity of its second largest eigenvalue can be greater than one.</description>
    </item>
    
    <item>
      <title>Generalized Sparse Additive Models</title>
      <link>/post/2018winter/2018-01-19/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-01-19/</guid>
      <description>Date: 2018-01-19 Time: 15:30-16:30 Location: BURN 1205 Abstract: I will present a unified approach to the estimation of generalized sparse additive models in high dimensional regression problems. Our approach is based on combining structure-inducing and sparsity penalties in a single regression problem. It allows for the use of a large family of structure-inducing penalties: Those characterized by semi-norm constraints. This includes finite dimensional linear subspaces, sobolev and holder classes, classes with bounded total variation, among others.</description>
    </item>
    
    <item>
      <title>Modelling RNA stability for decoding the regulatory programs that drive human diseases</title>
      <link>/post/2018winter/2018-01-12/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-01-12/</guid>
      <description>Date: 2018-01-12 Time: 15:30-16:30 Location: BURN 1205 Abstract: The key determinant of the identity and behaviour of the cell is gene regulation, i.e. which genes are active and which genes are inactive in a particular cell. One of the least understood aspects of gene regulation is RNA stability: genes produce RNA molecules to carry their genetic information – the more stable these RNA molecules are, the longer they can function within the cell, and the less stable they are, the more rapidly they are removed from the pool of active molecules.</description>
    </item>
    
    <item>
      <title>Fisher’s method revisited: set-based genetic association and interaction studies</title>
      <link>/post/2017fall/2017-12-01/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-12-01/</guid>
      <description>Date: 2017-12-01 Time: 15:30-16:30 Location: BURN 1205 Abstract: Fisher’s method, also known as Fisher’s combined probability test, is commonly used in meta-analyses to combine p-values from the same test applied to K independent samples to evaluate a common null hypothesis. Here we propose to use it to combine p-values from different tests applied to the same sample in two settings: when jointly analyzing multiple genetic variants in set-based genetic association studies, or when jointly capturing main and interaction effects in the presence of missing one of the interacting variables.</description>
    </item>
    
    <item>
      <title>A log-linear time algorithm for constrained changepoint detection</title>
      <link>/post/2017fall/2017-11-17/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-11-17/</guid>
      <description>Date: 2017-11-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: Changepoint detection is a central problem in time series and genomic data. For some applications, it is natural to impose constraints on the directions of changes. One example is ChIP-seq data, for which adding an up-down constraint improves peak detection accuracy, but makes the optimization problem more complicated. In this talk I will explain how a recently proposed functional pruning algorithm can be generalized to solve such constrained changepoint detection problems.</description>
    </item>
    
    <item>
      <title>PAC-Bayesian Generalizations Bounds for Deep Neural Networks</title>
      <link>/post/2017fall/2017-11-10/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-11-10/</guid>
      <description>Date: 2017-11-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this &amp;ldquo;deep learning&amp;rdquo; regime.</description>
    </item>
    
    <item>
      <title>How to do statistics</title>
      <link>/post/2017fall/2017-11-03/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-11-03/</guid>
      <description>Date: 2017-11-03 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, I will outline how to do (Bayesian) statistics. I will focus particularly on the things that need to be done before you see data, including prior specification and checking that your inference algorithm actually works.
Speaker Daniel Simpson is an Assistant Professor in the Department of Statistical Sciences, University of Toronto</description>
    </item>
    
    <item>
      <title>Penalized robust regression estimation with applications to proteomics</title>
      <link>/post/2017fall/2017-10-27/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-10-27/</guid>
      <description>Date: 2017-10-27 Time: 15:30-16:30 Location: BURN 1205 Abstract: In many current applications, scientists can easily measure a very large number of variables (for example, hundreds of protein levels), some of which are expected be useful to explain or predict a specific response variable of interest. These potential explanatory variables are most likely to contain redundant or irrelevant information, and in many cases, their quality and reliability may be suspect. We developed two penalized robust regression estimators that can be used to identify a useful subset of explanatory variables to predict the response, while protecting the resulting estimator against possible aberrant observations in the data set.</description>
    </item>
    
    <item>
      <title>Statistical optimization and nonasymptotic robustness</title>
      <link>/post/2017fall/2017-10-20/</link>
      <pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-10-20/</guid>
      <description>Date: 2017-10-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Statistical optimization has generated quite some interest recently. It refers to the case where hidden and local convexity can be discovered in most cases for nonconvex problems, making polynomial algorithms possible. It relies on a careful analysis of the geometry near global optima. In this talk, I will explore this issue by focusing on sparse regression problems in high dimensions. A computational framework named iterative local adaptive majorize-minimization (I-LAMM) will be proposed to simultaneously control algorithmic complexity and statistical error.</description>
    </item>
    
    <item>
      <title>Quantifying spatial flood risks: A comparative study of max-stable models</title>
      <link>/post/2017fall/2017-10-13/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-10-13/</guid>
      <description>Date: 2017-10-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: In various applications, evaluating spatial risks (such as floods, heatwaves or storms) is a key problem. The aim of this talk is to make use of extreme value theory and max-stable processes to provide quantitative answers to this issue. A review of the literature will be provided, as well as a wide comparative study based on a simulation design mimicking daily rainfall in France.</description>
    </item>
    
    <item>
      <title>BET on independence</title>
      <link>/post/2017fall/2017-09-22/</link>
      <pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-09-22/</guid>
      <description>Date: 2017-09-22 Time: 14:00-15:00 Location: BRONF179 Abstract: We study the problem of nonparametric dependence detection. Many existing methods suffer severe power loss due to non-uniform consistency, which we illustrate with a paradox. To avoid such power loss, we approach the nonparametric test of independence through the new framework of binary expansion statistics (BEStat) and binary expansion testing (BET), which examine dependence through a filtration induced by marginal binary expansions. Through a novel decomposition of the likelihood of contingency tables whose sizes are powers of 2, we show that the interactions of binary variables in the filtration are complete sufficient statistics for dependence.</description>
    </item>
    
    <item>
      <title>Our quest for robust time series forecasting at scale</title>
      <link>/post/2017fall/2017-09-15/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-09-15/</guid>
      <description>Date: 2017-09-15 Time: 15:30-16:30 Location: BURN 1205 Abstract: The demand for time series forecasting at Google has grown rapidly along with the company since its founding. Initially, the various business and engineering needs led to a multitude of forecasting approaches, most reliant on direct analyst support. The volume and variety of the approaches, and in some cases their inconsistency, called out for an attempt to unify, automate, and extend forecasting methods, and to distribute the results via tools that could be deployed reliably across the company.</description>
    </item>
    
    <item>
      <title>Genomics like it&#39;s 1960: Inferring human history</title>
      <link>/post/2017fall/2017-09-08/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-09-08/</guid>
      <description>Date: 2017-09-08 Time: 15:30-16:30 Location: BURN 1205 Abstract: A central goal of population genetics is the inference of the biological, evolutionary and demographic forces that shaped human diversity. Large-scale sequencing experiments provide fantastic opportunities to learn about human history and biology if we can overcome computational and statistical challenges. I will discuss how simple mid-century statistical approaches, such as the jackknife and Kolmogorov equations, can be combined in unexpected ways to solve partial differential equations, optimize genomic study design, and learn about the spread of modern humans since our common African origins.</description>
    </item>
    
    <item>
      <title>Distributed kernel regression for large-scale data</title>
      <link>/post/2017winter/2017-03-31/</link>
      <pubDate>Fri, 31 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-03-31/</guid>
      <description>Date: 2017-03-31 Time: 15:30-16:30 Location: BURN 1205 Abstract: In modern scientific research, massive datasets with huge numbers of observations are frequently encountered. To facilitate the computational process, a divide-and-conquer scheme is often used for the analysis of big data. In such a strategy, a full dataset is first split into several manageable segments; the final output is then aggregated from the individual outputs of the segments. Despite its popularity in practice, it remains largely unknown that whether such a distributive strategy provides valid theoretical inferences to the original data; if so, how efficient does it work?</description>
    </item>
    
    <item>
      <title>Bayesian sample size determination for clinical trials</title>
      <link>/post/2017winter/2017-03-24/</link>
      <pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-03-24/</guid>
      <description>Date: 2017-03-24 Time: 15:30-16:30 Location: BURN 1205 Abstract: Sample size determination problem is an important task in the planning of clinical trials. The problem may be formulated formally in statistical terms. The most frequently used methods are based on the required size, and power of the trial for a specified treatment effect. In contrast to the Bayesian decision-theoretic approach, there is no explicit balancing of the cost of a possible increase in the size of the trial against the benefit of the more accurate information which it would give.</description>
    </item>
    
    <item>
      <title>High-throughput single-cell biology: The challenges and opportunities for machine learning scientists</title>
      <link>/post/2017winter/2017-03-10/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-03-10/</guid>
      <description>Date: 2017-03-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: The immune system does a lot more than killing “foreign” invaders. It’s a powerful sensory system that can detect stress levels, infections, wounds, and even cancer tumors. However, due to the complex interplay between different cell types and signaling pathways, the amount of data produced to characterize all different aspects of the immune system (tens of thousands of genes measured and hundreds of millions of cells, just from a single patient) completely overwhelms existing bioinformatics tools.</description>
    </item>
    
    <item>
      <title>The first pillar of statistical wisdom</title>
      <link>/post/2017winter/2017-02-24/</link>
      <pubDate>Fri, 24 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-02-24/</guid>
      <description>Date: 2017-02-24 Time: 15:30-16:30 Location: BURN 1205 Abstract: This talk will provide an introduction to the first of the pillars in Stephen Stigler&amp;rsquo;s 2016 book The Seven Pillars of Statistical Wisdom, namely “Aggregation.” It will focus on early instances of the sample mean in scientific work, on the early error distributions, and on how their “centres” were fitted.
Speaker James A. Hanley is a Professor in the Department of Epidemiology, Biostatistics and Occupational Health, at McGill University.</description>
    </item>
    
    <item>
      <title>Building end-to-end dialogue systems using deep neural architectures</title>
      <link>/post/2017winter/2017-02-17/</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-02-17/</guid>
      <description>Date: 2017-02-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: The ability for a computer to converse in a natural and coherent manner with a human has long been held as one of the important steps towards solving artificial intelligence. In this talk I will present recent results on building dialogue systems from large corpuses using deep neural architectures. I will highlight several challenges related to data acquisition, algorithmic development, and performance evaluation.</description>
    </item>
    
    <item>
      <title>Sparse envelope model: Efficient estimation and response variable selection in multivariate linear regression</title>
      <link>/post/2017winter/2017-02-10/</link>
      <pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-02-10/</guid>
      <description>Date: 2017-02-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: The envelope model is a method for efficient estimation in multivariate linear regression. In this article, we propose the sparse envelope model, which is motivated by applications where some response variables are invariant to changes of the predictors and have zero regression coefficients. The envelope estimator is consistent but not sparse, and in many situations it is important to identify the response variables for which the regression coefficients are zero.</description>
    </item>
    
    <item>
      <title>MM algorithms for variance component models</title>
      <link>/post/2017winter/2017-02-03/</link>
      <pubDate>Fri, 03 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-02-03/</guid>
      <description>Date: 2017-02-03 Time: 15:30-16:30 Location: BURN 1205 Abstract: Variance components estimation and mixed model analysis are central themes in statistics with applications in numerous scientific disciplines. Despite the best efforts of generations of statisticians and numerical analysts, maximum likelihood estimation and restricted maximum likelihood estimation of variance component models remain numerically challenging. In this talk, we present a novel iterative algorithm for variance components estimation based on the minorization-maximization (MM) principle.</description>
    </item>
    
    <item>
      <title>Order selection in multidimensional finite mixture models</title>
      <link>/post/2017winter/2017-01-20/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-01-20/</guid>
      <description>Date: 2017-01-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Finite mixture models provide a natural framework for analyzing data from heterogeneous populations. In practice, however, the number of hidden subpopulations in the data may be unknown. The problem of estimating the order of a mixture model, namely the number of subpopulations, is thus crucial for many applications. In this talk, we present a new penalized likelihood solution to this problem, which is applicable to models with a multidimensional parameter space.</description>
    </item>
    
    <item>
      <title>(Sparse) exchangeable graphs</title>
      <link>/post/2017winter/2017-01-13/</link>
      <pubDate>Fri, 13 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-01-13/</guid>
      <description>Date: 2017-01-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: Many popular statistical models for network valued datasets fall under the remit of the graphon framework, which (implicitly) assumes the networks are densely connected. However, this assumption rarely holds for the real-world networks of practical interest. We introduce a new class of models for random graphs that generalises the dense graphon models to the sparse graph regime, and we argue that this meets many of the desiderata one would demand of a model to serve as the foundation for a statistical analysis of real-world networks.</description>
    </item>
    
    <item>
      <title>Modeling dependence in bivariate multi-state processes: A frailty approach</title>
      <link>/post/2016fall/2016-12-02/</link>
      <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-12-02/</guid>
      <description>Date: 2016-12-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: The aim of this talk is to present a statistical framework for the analysis of dependent bivariate multistate processes, allowing one to study the dependence both across subjects in a pair and among individual-specific events. As for the latter, copula- based models are employed, whereas dependence between multi-state models can be accomplished by means of frailties. The well known Marshall-Olkin Bivariate Exponential Distribution (MOBVE) is considered for the joint distribution of frailties.</description>
    </item>
    
    <item>
      <title>Spatio-temporal models for skewed processes</title>
      <link>/post/2016fall/2016-11-25/</link>
      <pubDate>Fri, 25 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-25/</guid>
      <description>Date: 2016-11-25 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the analysis of most spatio-temporal processes in environmental studies, observations present skewed distributions. Usually, a single transformation of the data is used to approximate normality, and stationary Gaussian processes are assumed to model the transformed data. The choice of transformation is key for spatial interpolation and temporal prediction. We propose a spatio-temporal model for skewed data that does not require the use of data transformation.</description>
    </item>
    
    <item>
      <title>Progress in theoretical understanding of deep learning</title>
      <link>/post/2016fall/2016-11-18/</link>
      <pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-18/</guid>
      <description>Date: 2016-11-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: Deep learning has arisen around 2006 as a renewal of neural networks research allowing such models to have more layers. Theoretical investigations have shown that functions obtained as deep compositions of simpler functions (which includes both deep and recurrent nets) can express highly varying functions (with many ups and downs and different input regions that can be distinguished) much more efficiently (with fewer parameters) than otherwise, under a prior which seems to work well for artificial intelligence tasks.</description>
    </item>
    
    <item>
      <title>Tyler&#39;s M-estimator: Subspace recovery and high-dimensional regime</title>
      <link>/post/2016fall/2016-11-11/</link>
      <pubDate>Fri, 11 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-11/</guid>
      <description>Date: 2016-11-11 Time: 15:30-16:30 Location: BURN 1205 Abstract: Given a data set, Tyler&amp;rsquo;s M-estimator is a widely used covariance matrix estimator with robustness to outliers or heavy-tailed distribution. We will discuss two recent results of this estimator. First, we show that when a certain percentage of the data points are sampled from a low-dimensional subspace, Tyler&amp;rsquo;s M-estimator can be used to recover the subspace exactly. Second, in the high-dimensional regime that the number of samples n and the dimension p both go to infinity, p/n converges to a constant y between 0 and 1, and when the data samples are identically and independently generated from the Gaussian distribution N(0,I), we showed that the difference between the sample covariance matrix and a scaled version of Tyler&amp;rsquo;s M-estimator tends to zero in spectral norm, and the empirical spectral densities of both estimators converge to the Marcenko-Pastur distribution.</description>
    </item>
    
    <item>
      <title>Lawlor: Time-varying mixtures of Markov chains: An application to traffic modeling Piché: Bayesian nonparametric modeling of heterogeneous groups of censored data</title>
      <link>/post/2016fall/2016-11-04/</link>
      <pubDate>Fri, 04 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-04/</guid>
      <description>Date: 2016-11-04 Time: 15:30-16:30 Location: BURN 1205 Abstract: Piché: Analysis of survival data arising from different groups, whereby the data in each group is scarce, but abundant overall, is a common issue in applied statistics. Bayesian nonparametrics are tools of choice to handle such datasets given their ability to share information across groups. In this presentation, we will compare three popular Bayesian nonparametric methods on the modeling of survival functions coming from related heterogeneous groups.</description>
    </item>
    
    <item>
      <title>First talk: Bootstrap in practice | Second talk: Statistics and Big Data at Google</title>
      <link>/post/2016fall/2016-11-02/</link>
      <pubDate>Wed, 02 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-02/</guid>
      <description>Date: 2016-11-02 Time: 15:00-16:00 17:35-18:25 Location: 1st: BURN 306 2nd: ADAMS AUD Abstract: First talk: This talk focuses on three practical aspects of resampling: communication, accuracy, and software. I&amp;rsquo;ll introduce the bootstrap and permutation tests, and discussed how they may be used to help clients understand statistical results. I&amp;rsquo;ll talk about accuracy &amp;ndash; there are dramatic differences in how accurate different bootstrap methods are. Surprisingly, the most common bootstrap methods are less accurate than classical methods for small samples, and more accurate for larger samples.</description>
    </item>
    
    <item>
      <title>Statistical analysis of two-level hierarchical clustered data</title>
      <link>/post/2016fall/2016-10-21/</link>
      <pubDate>Fri, 21 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-10-21/</guid>
      <description>Date: 2016-10-21 Time: 15:30-16:30 Location: BURN 1205 Abstract: Multi-level hierarchical clustered data are commonly seen in financial and biostatistics applications. In this talk, we introduce several modeling strategies for describing the dependent relationships for members within a cluster or between different clusters (in the same or different levels). In particular we will apply the hierarchical Kendall copula, first proposed by Brechmann (2014), to model two-level hierarchical clustered survival data. This approach provides a clever way of dimension reduction in modeling complicated multivariate data.</description>
    </item>
    
    <item>
      <title>A Bayesian finite mixture of bivariate regressions model for causal mediation analyses</title>
      <link>/post/2016fall/2016-10-14/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-10-14/</guid>
      <description>Date: 2016-10-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: Building on the work of Schwartz, Gelfand and Miranda (Statistics in Medicine (2010); 29(16), 1710-23), we propose a Bayesian finite mixture of bivariate regressions model for causal mediation analyses. Using an identifiability condition within each component of the mixture, we express the natural direct and indirect effects of the exposure on the outcome as functions of the component-specific regression coefficients. On the basis of simulated data, we examine the behaviour of the model for estimating these effects in situations where the associations between exposure, mediator and outcome are confounded, or not.</description>
    </item>
    
    <item>
      <title>Cellular tree classifiers</title>
      <link>/post/2016fall/2016-10-07/</link>
      <pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-10-07/</guid>
      <description>Date: 2016-10-07 Time: 15:30-16:30 Location: BURN 1205 Abstract: Suppose that binary classification is done by a tree method in which the leaves of a tree correspond to a partition of d-space. Within a partition, a majority vote is used. Suppose furthermore that this tree must be constructed recursively by implementing just two functions, so that the construction can be carried out in parallel by using &amp;ldquo;cells&amp;rdquo;: first of all, given input data, a cell must decide whether it will become a leaf or internal node in the tree.</description>
    </item>
    
    <item>
      <title>CoCoLasso for high-dimensional error-in-variables regression</title>
      <link>/post/2016fall/2016-09-30/</link>
      <pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-09-30/</guid>
      <description>Date: 2016-09-30 Time: 15:30-16:30 Location: BURN 1205 Abstract: Much theoretical and applied work has been devoted to high-dimensional regression with clean data. However, we often face corrupted data in many applications where missing data and measurement errors cannot be ignored. Loh and Wainwright (2012) proposed a non-convex modification of the Lasso for doing high-dimensional regression with noisy and missing data. It is generally agreed that the virtues of convexity contribute fundamentally the success and popularity of the Lasso.</description>
    </item>
    
    <item>
      <title>Stein estimation of the intensity parameter of a stationary spatial Poisson point process</title>
      <link>/post/2016fall/2016-09-23/</link>
      <pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-09-23/</guid>
      <description>Date: 2016-09-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: We revisit the problem of estimating the intensity parameter of a homogeneous Poisson point process observed in a bounded window of $R^d$ making use of a (now) old idea going back to James and Stein. For this, we prove an integration by parts formula for functionals defined on the Poisson space. This formula extends the one obtained by Privault and Réveillac (Statistical inference for Stochastic Processes, 2009) in the one-dimensional case and is well-suited to a notion of derivative of Poisson functionals which satisfy the chain rule.</description>
    </item>
    
    <item>
      <title>Two-set canonical variate model in multiple populations with invariant loadings</title>
      <link>/post/2016fall/2016-09-09/</link>
      <pubDate>Fri, 09 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-09-09/</guid>
      <description>Date: 2016-09-09 Time: 15:30-16:30 Location: BURN 1205 Abstract: Goria and Flury (Definition 2.1, 1996) proposed the two-set canonical variate model (referred to as the CV-2 model hereafter) and its extension in multiple populations with invariant weight coefficients (Definition 2.2). The equality constraints imposed on the weight coefficients are in line with the approach to interpreting the canonical variates (i.e., the linear combinations of original variables) advocated by Harris (1975, 1989), Rencher (1988, 1992), and Rencher and Christensen (2003).</description>
    </item>
    
    <item>
      <title>Multivariate tests of associations based on univariate tests</title>
      <link>/post/2016winter/2016-04-08/</link>
      <pubDate>Fri, 08 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-04-08/</guid>
      <description>Date: 2016-04-08 Time: 15:30-16:30 Location: BURN 1205 Abstract: For testing two random vectors for independence, we consider testing whether the distance of one vector from an arbitrary center point is independent from the distance of the other vector from an arbitrary center point by a univariate test. We provide conditions under which it is enough to have a consistent univariate test of independence on the distances to guarantee that the power to detect dependence between the random vectors increases to one, as the sample size increases.</description>
    </item>
    
    <item>
      <title>Asymptotic behavior of binned kernel density estimators for locally non-stationary random fields</title>
      <link>/post/2016winter/2016-04-01/</link>
      <pubDate>Fri, 01 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-04-01/</guid>
      <description>Date: 2016-04-01 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, I will describe the finite- and large-sample behavior of binned kernel density estimators for dependent and locally non-stationary random fields converging to stationary random fields. In addition to looking at the bias and asymptotic normality of the estimators, I will present results from a simulation study which shows that the kernel density estimator and the binned kernel density estimator have the same behavior and both estimate accurately the true density when the number of fields increases.</description>
    </item>
    
    <item>
      <title>Robust minimax shrinkage estimation of location vectors under concave loss</title>
      <link>/post/2016winter/2016-03-18/</link>
      <pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-03-18/</guid>
      <description>Date: 2016-03-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: We consider the problem of estimating the mean vector, q, of a multivariate spherically symmetric distribution under a loss function which is a concave function of squared error. In particular we find conditions on the shrinkage factor under which Stein-type shrinkage estimators dominate the usual minimax best equivariant estimator. In problems where the scale is known, minimax shrinkage factors which generally depend on both the loss and the sampling distribution are found.</description>
    </item>
    
    <item>
      <title>Nonparametric graphical models: Foundation and trends</title>
      <link>/post/2016winter/2016-03-11/</link>
      <pubDate>Fri, 11 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-03-11/</guid>
      <description>Date: 2016-03-11 Time: 15:30-16:30 Location: BURN 1205 Abstract: We consider the problem of learning the structure of a non-Gaussian graphical model. We introduce two strategies for constructing tractable nonparametric graphical model families. One approach is through semiparametric extension of the Gaussian or exponential family graphical models that allows arbitrary graphs. Another approach is to restrict the family of allowed graphs to be acyclic, enabling the use of fully nonparametric density estimation in high dimensions.</description>
    </item>
    
    <item>
      <title>Aggregation methods for portfolios of dependent risks with Archimedean copulas</title>
      <link>/post/2016winter/2016-02-26/</link>
      <pubDate>Fri, 26 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-02-26/</guid>
      <description>Date: 2016-02-26 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we will consider a portfolio of dependent risks represented by a vector of dependent random variables whose joint cumulative distribution function (CDF) is defined with an Archimedean copula. Archimedean copulas are very popular and their extensions, nested Archimedean copulas, are well suited for vectors of random vectors in high dimension. I will describe a simple approach which makes it possible to compute the CDF of the sum or a variety of other functions of those random variables.</description>
    </item>
    
    <item>
      <title>An introduction to statistical lattice models and observables</title>
      <link>/post/2016winter/2016-02-19/</link>
      <pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-02-19/</guid>
      <description>Date: 2016-02-19 Time: 15:30-16:30 Location: BURN 1205 Abstract: The study of convergence of random walks to well defined curves is founded in the fields of complex analysis, probability theory, physics and combinatorics. The foundations of this subject were motivated by physicists interested in the properties of one-dimensional models that represented some form of physical phenomenon. By taking physical models and generalizing them into abstract mathematical terms, macroscopic properties about the model could be determined from the microscopic level.</description>
    </item>
    
    <item>
      <title>The Bayesian causal effect estimation algorithm</title>
      <link>/post/2016winter/2016-02-05/</link>
      <pubDate>Fri, 05 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-02-05/</guid>
      <description>Date: 2016-02-05 Time: 15:30-16:30 Location: BURN 1214 Abstract: Estimating causal exposure effects in observational studies ideally requires the analyst to have a vast knowledge of the domain of application. Investigators often bypass difficulties related to the identification and selection of confounders through the use of fully adjusted outcome regression models. However, since such models likely contain more covariates than required, the variance of the regression coefficient for exposure may be unnecessarily large.</description>
    </item>
    
    <item>
      <title>Estimating high-dimensional networks with hubs with an application to microbiome data</title>
      <link>/post/2016winter/2016-01-29/</link>
      <pubDate>Fri, 29 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-01-29/</guid>
      <description>Date: 2016-01-29 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we investigate the problem of estimating high-dimensional networks in which there are a few highly connected “hub&amp;rdquo; nodes. Methods based on L1-regularization have been widely used for performing sparse selection in the graphical modelling context. However, the L1 penalty penalizes each edge equally and independently of each other without taking into account any structural information. We introduce a new method for estimating undirected graphical models with hubs, called the hubs weighted graphical lasso (HWGL).</description>
    </item>
    
    <item>
      <title>Robust estimation in the presence of influential units in surveys</title>
      <link>/post/2016winter/2016-01-22/</link>
      <pubDate>Fri, 22 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-01-22/</guid>
      <description>Date: 2016-01-22 Time: 15:30-16:30 Location: BURN 1205 Abstract: Influential units are those which make classical estimators (e.g., the Horvitz-Thompson estimator or calibration estimators) very unstable. The problem of influential units is particularly important in business surveys, which collect economic variables, whose distribution are highly skewed (heavy right tail). In this talk, we will attempt to answer the following questions:
(1) What is an influential value in surveys? (2) How measure the influence of unit?</description>
    </item>
    
    <item>
      <title>Prevalent cohort studies: Length-biased sampling with right censoring</title>
      <link>/post/2015fall/2015-11-13/</link>
      <pubDate>Fri, 13 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-11-13/</guid>
      <description>Date: 2015-11-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: Logistic or other constraints often preclude the possibility of conducting incident cohort studies. A feasible alternative in such cases is to conduct a cross-sectional prevalent cohort study for which we recruit prevalent cases, i.e., subjects who have already experienced the initiating event, say the onset of a disease. When the interest lies in estimating the lifespan between the initiating event and a terminating event, say death for instance, such subjects may be followed prospectively until the terminating event or loss to follow-up, whichever happens first.</description>
    </item>
    
    <item>
      <title>Bayesian analysis of non-identifiable models, with an example from epidemiology and biostatistics</title>
      <link>/post/2015fall/2015-11-06/</link>
      <pubDate>Fri, 06 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-11-06/</guid>
      <description>Date: 2015-11-06 Time: 15:30-16:30 Location: BURN 1205 Abstract: Most regression models in biostatistics assume identifiability, which means that each point in the parameter space corresponds to a unique likelihood function for the observable data. Recently there has been interest in Bayesian inference for non-identifiable models, which can better represent uncertainty in some contexts. One example is in the field of epidemiology, where the investigator is concerned with bias due to unmeasured confounders (omitted variables).</description>
    </item>
    
    <item>
      <title>Robust mixture regression and outlier detection via penalized likelihood</title>
      <link>/post/2015fall/2015-10-23/</link>
      <pubDate>Fri, 23 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-10-23/</guid>
      <description>Date: 2015-10-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: Finite mixture regression models have been widely used for modeling mixed regression relationships arising from a clustered and thus heterogenous population. The classical normal mixture model, despite of its simplicity and wide applicability, may fail dramatically in the presence of severe outliers. We propose a robust mixture regression approach based on a sparse, case-specific, and scale-dependent mean-shift parameterization, for simultaneously conducting outlier detection and robust parameter estimation.</description>
    </item>
    
    <item>
      <title>Estimating high-dimensional multi-layered networks through penalized maximum likelihood</title>
      <link>/post/2015fall/2015-10-16/</link>
      <pubDate>Fri, 16 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-10-16/</guid>
      <description>Date: 2015-10-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: Gaussian graphical models represent a good tool for capturing interactions between nodes represent the underlying random variables. However, in many applications in biology one is interested in modeling associations both between, as well as within molecular compartments (e.g., interactions between genes and proteins/metabolites). To this end, inferring multi-layered network structures from high-dimensional data provides insight into understanding the conditional relationships among nodes within layers, after adjusting for and quantifying the effects of nodes from other layers.</description>
    </item>
    
    <item>
      <title>Parameter estimation of partial differential equations over irregular domains</title>
      <link>/post/2015fall/2015-10-09/</link>
      <pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-10-09/</guid>
      <description>Date: 2015-10-09 Time: 15:30-16:30 Location: BURN 1205 Abstract: Spatio-temporal data are abundant in many scientific fields; examples include daily satellite images of the earth, hourly temperature readings from multiple weather stations, and the spread of an infectious disease over a particular region. In many instances the spatio-temporal data are accompanied by mathematical models expressed in terms of partial differential equations (PDEs). These PDEs determine the theoretical aspects of the behavior of the physical, chemical or biological phenomena considered.</description>
    </item>
    
    <item>
      <title>Estimating covariance matrices of intermediate size</title>
      <link>/post/2015fall/2015-10-02/</link>
      <pubDate>Fri, 02 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-10-02/</guid>
      <description>Date: 2015-10-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: In finance, the covariance matrix of many assets is a key component of financial portfolio optimization and is usually estimated from historical data. Much research in the past decade has focused on improving estimation by studying the asymptotics of large covariance matrices in the so-called high-dimensional regime, where the dimension p grows at the same pace as the sample size n, and this approach has been very successful.</description>
    </item>
    
    <item>
      <title>Topics in statistical inference for the semiparametric elliptical copula model</title>
      <link>/post/2015fall/2015-09-25/</link>
      <pubDate>Fri, 25 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-09-25/</guid>
      <description>Date: 2015-09-25 Time: 15:30-16:30 Location: BURN 1205 Abstract: This talk addresses aspects of the statistical inference problem for the semiparametric elliptical copula model. The semiparametric elliptical copula model is the family of distributions whose dependence structures are specified by parametric elliptical copulas but whose marginal distributions are left unspecified. An elliptical copula is uniquely characterized by a characteristic generator and a copula correlation matrix Sigma. In the first part of this talk, I will consider the estimation of Sigma.</description>
    </item>
    
    <item>
      <title>A unified algorithm for fitting penalized models with high-dimensional data</title>
      <link>/post/2015fall/2015-09-18/</link>
      <pubDate>Fri, 18 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-09-18/</guid>
      <description>Date: 2015-09-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the light of high-dimensional problems, research on the penalized model has received much interest. Correspondingly, several algorithms have been developed for solving penalized high-dimensional models. I will describe fast and efficient unified algorithms for computing the solution path for a collection of penalized models. In particular, we will look at an algorithm for solving L1-penalized learning problems and an algorithm for solving group-lasso learning problems.</description>
    </item>
    
    <item>
      <title>Bias correction in multivariate extremes</title>
      <link>/post/2015fall/2015-09-11/</link>
      <pubDate>Fri, 11 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-09-11/</guid>
      <description>Date: 2015-09-11 Time: 15:30-16:30 Location: BURN 1205 Abstract: The estimation of the extremal dependence structure of a multivariate extreme-value distribution is spoiled by the impact of the bias, which increases with the number of observations used for the estimation. Already known in the univariate setting, the bias correction procedure is studied in this talk under the multivariate framework. New families of estimators of the stable tail dependence function are obtained.</description>
    </item>
    
    <item>
      <title>Some new classes of bivariate distributions based on conditional specification</title>
      <link>/post/2015winter/2015-05-14/</link>
      <pubDate>Thu, 14 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-05-14/</guid>
      <description>Date: 2015-05-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: A bivariate distribution can sometimes be characterized completely by properties of its conditional distributions. In this talk, we will discuss models of bivariate distributions whose conditionals are members of prescribed parametric families of distributions. Some relevant models with specified conditionals will be discussed, including the normal and lognormal cases, the skew-normal and other families of distributions. Finally, some conditionally specified densities will be shown to provide convenient flexible conjugate prior families in certain multiparameter Bayesian settings.</description>
    </item>
    
    <item>
      <title>Testing for network community structure</title>
      <link>/post/2015winter/2015-03-20/</link>
      <pubDate>Fri, 20 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-03-20/</guid>
      <description>Date: 2015-03-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Networks provide a useful means to summarize sparse yet structured massive datasets, and so are an important aspect of the theory of big data. A key question in this setting is to test for the significance of community structure or what in social networks is termed homophily, the tendency of nodes to be connected based on similar characteristics. Network models where a single parameter per node governs the propensity of connection are popular in practice, because they are simple to understand and analyze.</description>
    </item>
    
    <item>
      <title>Bayesian approaches to causal inference: A lack-of-success story</title>
      <link>/post/2015winter/2015-03-13/</link>
      <pubDate>Fri, 13 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-03-13/</guid>
      <description>Date: 2015-03-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: Despite almost universal acceptance across most fields of statistics, Bayesian inferential methods have yet to breakthrough to widespread use in causal inference, despite Bayesian arguments being a core component of early developments in the field. Some quasi-Bayesian procedures have been proposed, but often these approaches rely on heuristic, sometimes flawed, arguments. In this talk I will discuss some formulations of classical causal inference problems from the perspective of standard Bayesian representations, and propose some inferential solutions.</description>
    </item>
    
    <item>
      <title>A novel statistical framework to characterize antigen-specific T-cell functional diversity in single-cell expression data</title>
      <link>/post/2015winter/2015-02-27/</link>
      <pubDate>Fri, 27 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-27/</guid>
      <description>Date: 2015-02-27 Time: 15:30-16:30 Location: BURN 1205 Abstract: I will talk about COMPASS, a new Bayesian hierarchical framework for characterizing functional differences in antigen-specific T cells by leveraging high-throughput, single-cell flow cytometry data. In particular, I will illustrate, using a variety of data sets, how COMPASS can reveal subtle and complex changes in antigen-specific T-cell activation profiles that correlate with biological endpoints. Applying COMPASS to data from the RV144 (“the Thai trial”) HIV clinical trial, it identified novel T-cell subsets that were inverse correlates of HIV infection risk.</description>
    </item>
    
    <item>
      <title>Comparison and assessment of particle diffusion models in biological fluids</title>
      <link>/post/2015winter/2015-02-20/</link>
      <pubDate>Fri, 20 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-20/</guid>
      <description>Date: 2015-02-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Rapidly progressing particle tracking techniques have revealed that foreign particles in biological fluids exhibit rich and at times unexpected behavior, with important consequences for disease diagnosis and drug delivery. Yet, there remains a frustrating lack of coherence in the description of these particles&amp;rsquo; motion. Largely this is due to a reliance on functional statistics (e.g., mean-squared displacement) to perform model selection and assess goodness-of-fit.</description>
    </item>
    
    <item>
      <title>Tuning parameters in high-dimensional statistics</title>
      <link>/post/2015winter/2015-02-13/</link>
      <pubDate>Fri, 13 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-13/</guid>
      <description>Date: 2015-02-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: High-dimensional statistics is the basis for analyzing large and complex data sets that are generated by cutting-edge technologies in genetics, neuroscience, astronomy, and many other fields. However, Lasso, Ridge Regression, Graphical Lasso, and other standard methods in high-dimensional statistics depend on tuning parameters that are difficult to calibrate in practice. In this talk, I present two novel approaches to overcome this difficulty.</description>
    </item>
    
    <item>
      <title>A fast unified algorithm for solving group Lasso penalized learning problems</title>
      <link>/post/2015winter/2015-02-05/</link>
      <pubDate>Thu, 05 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-05/</guid>
      <description>Date: 2015-02-05 Time: 15:30-16:30 Location: BURN 1B39 Abstract: We consider a class of group-lasso learning problems where the objective function is the sum of an empirical loss and the group-lasso penalty. For a class of loss function satisfying a quadratic majorization condition, we derive a unified algorithm called groupwise-majorization-descent (GMD) for efficiently computing the solution paths of the corresponding group-lasso penalized learning problem. GMD allows for general design matrices, without requiring the predictors to be group-wise orthonormal.</description>
    </item>
    
    <item>
      <title>Joint analysis of multiple multi-state processes via copulas</title>
      <link>/post/2015winter/2015-02-02/</link>
      <pubDate>Mon, 02 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-02/</guid>
      <description>Date: 2015-02-02 Time: 15:30-16:30 Location: BURN 1214 Abstract: A copula-based model is described which enables joint analysis of multiple progressive multi-state processes. Unlike intensity-based or frailty-based approaches to joint modeling, the copula formulation proposed herein ensures that a wide range of marginal multi-state processes can be specified and the joint model will retain these marginal features. The copula formulation also facilitates a variety of approaches to estimation and inference including composite likelihood and two-stage estimation procedures.</description>
    </item>
    
    <item>
      <title>Distributed estimation and inference for sparse regression</title>
      <link>/post/2015winter/2015-01-30/</link>
      <pubDate>Fri, 30 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-01-30/</guid>
      <description>Date: 2015-01-30 Time: 15:30-16:30 Location: BURN 1205 Abstract: We address two outstanding challenges in sparse regression: (i) computationally efficient estimation in distributed settings; (ii) valid inference for the selected coefficients. The main computational challenge in a distributed setting is harnessing the computational capabilities of all the machines while keeping communication costs low. We devise an approach that requires only a single round of communication among the machines. We show the approach recovers the convergence rate of the (centralized) lasso as long as each machine has access to an adequate number of samples.</description>
    </item>
    
    <item>
      <title>Simultaneous white noise models and shrinkage recovery of functional data</title>
      <link>/post/2015winter/2015-01-16/</link>
      <pubDate>Fri, 16 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-01-16/</guid>
      <description>Date: 2015-01-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: We consider the white noise representation of functional data taken as i.i.d. realizations of a Gaussian process. The main idea is to establish an asymptotic equivalence in Le Cam’s sense between an experiment which simultaneously describes these realizations and a collection of white noise models. In this context, we project onto an arbitrary basis and apply a novel variant of Stein-type estimation for optimal recovery of the realized trajectories.</description>
    </item>
    
    <item>
      <title>Mixtures of coalesced generalized hyperbolic distributions</title>
      <link>/post/2015winter/2015-01-13/</link>
      <pubDate>Tue, 13 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-01-13/</guid>
      <description>Date: 2015-01-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: A mixture of coalesced generalized hyperbolic distributions is developed by joining a finite mixture of generalized hyperbolic distributions with a mixture of multiple scaled generalized hyperbolic distributions. The result is a mixture of mixtures with shared model parameters and common mode. We begin by discussing the generalized hyperbolic distribution, which has the t, Gaussian and others as special cases. The generalized hyperbolic distribution can represented as a normal-variance mixture using a generalized inverse Gaussian distribution.</description>
    </item>
    
    <item>
      <title>Space-time data analysis: Out of the Hilbert box</title>
      <link>/post/2015winter/2015-01-9/</link>
      <pubDate>Fri, 09 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-01-9/</guid>
      <description>Date: 2015-01-09 Time: 15:30-16:30 Location: BURN 1205 Abstract: Given the discouraging state of current efforts to curb global warming, we can imagine that we will soon turn our attention to mitigation. On a global scale, distressed populations will turn to national and international organizations for solutions to dramatic problems caused by climate change. These institutions in turn will mandate the collection of data on a scale and resolution that will present extraordinary statistical and computational challenges to those of us viewed as having the appropriate expertise.</description>
    </item>
    
    <item>
      <title>Testing for structured Normal means</title>
      <link>/post/2014fall/2014-12-12/</link>
      <pubDate>Fri, 12 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-12-12/</guid>
      <description>Date: 2014-12-12 Time: 15:30-16:30 Location: BURN 1205 Abstract: We will discuss the detection of pattern in images and graphs from a high-dimensional Gaussian measurement. This problem is relevant to many applications including detecting anomalies in sensor and computer networks, large-scale surveillance, co-expressions in gene networks, disease outbreaks, etc. Beyond its wide applicability, structured Normal means detection serves as a case study in the difficulty of balancing computational complexity with statistical power.</description>
    </item>
    
    <item>
      <title>Copula model selection: A statistical approach</title>
      <link>/post/2014fall/2014-12-05/</link>
      <pubDate>Fri, 05 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-12-05/</guid>
      <description>Date: 2014-12-05 Time: 15:30-16:30 Location: BURN 1205 Abstract: Copula model selection is an important problem because similar but differing copula models can offer different conclusions surrounding the dependence structure of random variables. Chen &amp;amp; Fan (2005) proposed a model selection method involving a statistical hypothesis test. The hypothesis test attempts to take into account the randomness of the AIC and other likelihood-based model selection methods for finite samples. Performance of the test compared to the more common approach of AIC is illustrated in a series of simulations.</description>
    </item>
    
    <item>
      <title>Model-based methods of classification with applications</title>
      <link>/post/2014fall/2014-11-28/</link>
      <pubDate>Fri, 28 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-11-28/</guid>
      <description>Date: 2014-11-28 Time: 15:30-16:30 Location: BURN 1205 Abstract: Model-based clustering via finite mixture models is a popular clustering method for finding hidden structures in data. The model is often assumed to be a finite mixture of multivariate normal distributions; however, flexible extensions have been developed over recent years. This talk demonstrates some methods employed in unsupervised, semi-supervised, and supervised classification that include skew-normal and skew-t mixture models. Both real and simulated data sets are used to demonstrate the efficacy of these techniques.</description>
    </item>
    
    <item>
      <title>Estimating by solving nonconvex programs: Statistical and computational guarantees</title>
      <link>/post/2014fall/2014-11-21/</link>
      <pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-11-21/</guid>
      <description>Date: 2014-11-21 Time: 15:30-16:30 Location: BURN 1205 Abstract: Many statistical estimators are based on solving nonconvex programs. Although the practical performance of such methods is often excellent, the associated theory is frequently incomplete, due to the potential gaps between global and local optima. In this talk, we present theoretical results that apply to all local optima of various regularized M-estimators, where both loss and penalty functions are allowed to be nonconvex.</description>
    </item>
    
    <item>
      <title>Bridging the gap: A likelihood function approach for the analysis of ranking data</title>
      <link>/post/2014fall/2014-11-14/</link>
      <pubDate>Fri, 14 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-11-14/</guid>
      <description>Date: 2014-11-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the parametric setting, the notion of a likelihood function forms the basis for the development of tests of hypotheses and estimation of parameters. Tests in connection with the analysis of variance stem entirely from considerations of the likelihood function. On the other hand, non- parametric procedures have generally been derived without any formal mechanism and are often the result of clever intuition.</description>
    </item>
    
    <item>
      <title>Bayesian regression with B-splines under combinations of shape constraints and smoothness properties</title>
      <link>/post/2014fall/2014-11-07/</link>
      <pubDate>Fri, 07 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-11-07/</guid>
      <description>Date: 2014-11-07 Time: 15:30-16:30 Location: BURN 1205 Abstract: We approach the problem of shape constrained regression from a Bayesian perspective. A B-spline basis is used to model the regression function. The smoothness of the regression function is controlled by the order of the B-splines and the shape is controlled by the shape of an associated control polygon. Controlling the shape of the control polygon reduces to some inequality constraints on the spline coefficients.</description>
    </item>
    
    <item>
      <title>A copula-based model for risk aggregation</title>
      <link>/post/2014fall/2014-10-31/</link>
      <pubDate>Fri, 31 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-31/</guid>
      <description>Date: 2014-10-31 Time: 15:30-16:30 Location: BURN 1205 Abstract: A flexible approach is proposed for risk aggregation. The model consists of a tree structure, bivariate copulas, and marginal distributions. The construction relies on a conditional independence assumption whose implications are studied. Selection the tree structure, estimation and model validation are illustrated using data from a Canadian property and casualty insurance company.
Speaker Marie-Pier Côté is a PhD student in the Department of Mathematics and Statistics at McGill University.</description>
    </item>
    
    <item>
      <title>PREMIER: Probabilistic error-correction using Markov inference in error reads</title>
      <link>/post/2014fall/2014-10-24/</link>
      <pubDate>Fri, 24 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-24/</guid>
      <description>Date: 2014-10-24 Time: 15:30-16:30 Location: BURN 1205 Abstract: Next generation sequencing (NGS) is a technology revolutionizing genetics and biology. Compared with the old Sanger sequencing method, the throughput is astounding and has fostered a slew of innovative sequencing applications. Unfortunately, the error rates are also higher, complicating many downstream analyses. For example, de novo assembly of genomes is less accurate and slower when reads include many errors. We develop a probabilistic model for NGS reads that can detect and correct errors without a reference genome and while flexibly modeling and estimating the error properties of the sequencing machine.</description>
    </item>
    
    <item>
      <title>Patient privacy, big data, and specimen pooling: Using an old tool for new challenges</title>
      <link>/post/2014fall/2014-10-17/</link>
      <pubDate>Fri, 17 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-17/</guid>
      <description>Date: 2014-10-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the recent past, electronic health records and distributed data networks emerged as a viable resource for medical and scientific research. As the use of confidential patient information from such sources become more common, maintaining privacy of patients is of utmost importance. For a binary disease outcome of interest, we show that the techniques of specimen pooling could be applied for analysis of large and/or distributed data while respecting patient privacy.</description>
    </item>
    
    <item>
      <title>A margin-free clustering algorithm appropriate for dependent maxima in the domain of attraction of an extreme-value copula</title>
      <link>/post/2014fall/2014-10-10/</link>
      <pubDate>Fri, 10 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-10/</guid>
      <description>Date: 2014-10-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: Extracting relevant information in complex spatial-temporal data sets is of paramount importance in statistical climatology. This is especially true when identifying spatial dependencies between quantitative extremes like heavy rainfall. The paper of Bernard et al. (2013) develops a fast and simple clustering algorithm for finding spatial patterns appropriate for extremes. They develop their algorithm by adapting multivariate extreme-value theory to the context of spatial clustering.</description>
    </item>
    
    <item>
      <title>Statistical exploratory data analysis in the modern era</title>
      <link>/post/2014fall/2014-10-03/</link>
      <pubDate>Fri, 03 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-03/</guid>
      <description>Date: 2014-10-03 Time: 15:30-16:30 Location: BURN 1205 Abstract: Major challenges arising from today&amp;rsquo;s &amp;ldquo;data deluge&amp;rdquo; include how to handle the commonly occurring situation of different types of variables (say, continuous and categorical) being simultaneously measured, as well as how to assess the accompanying flood of questions. Based on information theory, a bias-corrected mutual information (BCMI) measure of association that is valid and estimable between all basic types of variables has been proposed.</description>
    </item>
    
    <item>
      <title>Analysis of palliative care studies with joint models for quality-of-life measures and survival</title>
      <link>/post/2014fall/2014-09-26/</link>
      <pubDate>Fri, 26 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-09-26/</guid>
      <description>Date: 2014-09-26 Time: 15:30-16:30 Location: BURN 1205 Abstract: In palliative care studies, the primary outcomes are often health related quality of life measures (HRLQ). Randomized trials and prospective cohorts typically recruit patients with advanced stage of disease and follow them until death or end of the study. An important feature of such studies is that, by design, some patients, but not all, are likely to die during the course of the study.</description>
    </item>
    
    <item>
      <title>Covariates missing by design</title>
      <link>/post/2014fall/2014-09-19/</link>
      <pubDate>Fri, 19 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-09-19/</guid>
      <description>Date: 2014-09-19 Time: 15:30-16:30 Location: BURN 1205 Abstract: Incomplete data can arise in many different situations for many different reasons. Sometimes the data may be incomplete for reasons beyond the control of the experimenter. However, it is also possible that this missingness is part of the study design. By using a two-phase sampling approach where only a small sub-sample gives complete information, it is possible to greatly reduce the cost of a study and still obtain precise estimates.</description>
    </item>
    
    <item>
      <title>Hydrological applications with the functional data analysis framework</title>
      <link>/post/2014fall/2014-09-12/</link>
      <pubDate>Fri, 12 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-09-12/</guid>
      <description>Date: 2014-09-12 Time: 15:30-16:30 Location: BURN 1205 Abstract: River flows records are an essential data source for a variety of hydrological applications including the prevention of flood risks and as well as the planning and management of water resources. A hydrograph is a graphical representation of the temporal variation of flow over a period of time (continuously measured, usually over a year). A flood hydrograph is commonly characterized by a number of features, mainly its peak, volume and duration.</description>
    </item>
    
    <item>
      <title>Some aspects of data analysis under confidentiality protection</title>
      <link>/post/2014winter/2014-04-04/</link>
      <pubDate>Fri, 04 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-04-04/</guid>
      <description>Date: 2014-04-04 Time: 15:30-16:30 Location: BURN 1205 Abstract: Statisticians working in most federal agencies are often faced with two conflicting objectives: (1) collect and publish useful datasets for designing public policies and building scientific theories, and (2) protect confidentiality of data respondents which is essential to uphold public trust, leading to better response rates and data accuracy. In this talk I will provide a survey of two statistical methods currently used at the U.</description>
    </item>
    
    <item>
      <title>How much does the dependence structure matter?</title>
      <link>/post/2014winter/2014-03-28/</link>
      <pubDate>Fri, 28 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-03-28/</guid>
      <description>Date: 2014-03-28 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we will look at some classical problems from an anti-traditional perspective. We will consider two problems regarding a sequence of random variables with a given common marginal distribution. First, we will introduce the notion of extreme negative dependence (END), a new benchmark for negative dependence, which is comparable to comonotonicity and independence. Second, we will study the compatibility of the marginal distribution and the limiting distribution when the dependence structure in the sequence is allowed to vary among all possibilities.</description>
    </item>
    
    <item>
      <title>Mixed effects trees and forests for clustered data</title>
      <link>/post/2014winter/2014-03-14/</link>
      <pubDate>Fri, 14 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-03-14/</guid>
      <description>Date: 2014-03-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, I will present extensions of tree-based and random forest methods for the case of clustered data. The proposed methods can handle unbalanced clusters, allows observations within clusters to be splitted, and can incorporate random effects and observation-level covariates. The basic tree-building algorithm for a continuous outcome is implemented using standard algorithms within the framework of the EM algorithm. The extension to other types of outcomes (e.</description>
    </item>
    
    <item>
      <title>On the multivariate analysis of neural spike trains: Skellam process with resetting and its applications</title>
      <link>/post/2014winter/2014-02-21/</link>
      <pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-02-21/</guid>
      <description>Date: 2014-02-21 Time: 15:30-16:30 Location: BURN 1205 Abstract: Nerve cells (a.k.a. neurons) communicate via electrochemical waves (action potentials), which are usually called spikes as they are very localized in time. A sequence of consecutive spikes from one neuron is called a spike train. The exact mechanism of information coding in spike trains is still an open problem; however, one popular approach is to model spikes as realizations of an inhomogeneous Poisson process.</description>
    </item>
    
    <item>
      <title>Divergence based inference for general estimating equations</title>
      <link>/post/2014winter/2014-02-14/</link>
      <pubDate>Fri, 14 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-02-14/</guid>
      <description>Date: 2014-02-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: Hellinger distance and its variants have long been used in the theory of robust statistics to develop inferential tools that are more robust than the maximum likelihood but as ecient as the MLE when the posited model holds. A key aspect of this alternative approach requires specication of a parametric family, which is usually not feasible in the context of problems involving complex data structures wherein estimating equations are typically used for inference.</description>
    </item>
    
    <item>
      <title>Statistical techniques for the normalization and segmentation of structural MRI</title>
      <link>/post/2014winter/2014-02-07/</link>
      <pubDate>Fri, 07 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-02-07/</guid>
      <description>Date: 2014-02-07 Time: 15:30-16:30 Location: BURN 1205 Abstract: While computed tomography and other imaging techniques are measured in absolute units with physical meaning, magnetic resonance images are expressed in arbitrary units that are difficult to interpret and differ between study visits and subjects. Much work in the image processing literature has centered on histogram matching and other histogram mapping techniques, but little focus has been on normalizing images to have biologically interpretable units.</description>
    </item>
    
    <item>
      <title>An exchangeable Kendall&#39;s tau for clustered data</title>
      <link>/post/2014winter/2014-01-31/</link>
      <pubDate>Fri, 31 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-01-31/</guid>
      <description>Date: 2014-01-31 Time: 15:30-16:30 Location: BURN 1205 Abstract: I&amp;rsquo;ll introduce the exchangeable Kendall&amp;rsquo;s tau as a nonparametric intra class association measure in a clustered data frame and provide an estimator for this measure. The asymptotic properties of this estimator are investigated under a multivariate exchangeable cdf. Two applications of the proposed statistic are considered. The first is an estimator of the intraclass correlation coefficient for data drawn from an elliptical distribution.</description>
    </item>
    
    <item>
      <title>An introduction to stochastic partial differential equations and intermittency</title>
      <link>/post/2014winter/2014-01-10/</link>
      <pubDate>Fri, 10 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-01-10/</guid>
      <description>Date: 2014-01-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: In a seminal article in 1944, Itô introduced the stochastic integral with respect to the Brownian motion, which turned out to be one of the most fruitful ideas in mathematics in the 20th century. This lead to the development of stochastic analysis, a field which includes the study of stochastic partial differential equations (SPDEs). One of the approaches for the study of SPDEs was initiated by Walsh (1986) and relies on the concept of random-field solution for equations perturbed by a space-time white noise (or Brownian sheet).</description>
    </item>
    
    <item>
      <title>Tail order and its applications</title>
      <link>/post/2013fall/2013-11-22/</link>
      <pubDate>Fri, 22 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-11-22/</guid>
      <description>Date: 2013-11-22 Time: 15:30-16:30 Location: BURN 1205 Abstract: Tail order is a notion for quantifying the strength of dependence in the tail of a joint distribution. It can account for a wide range of dependence, ranging from tail positive dependence to tail negative dependence. We will introduce theory and applications of tail order. Conditions for tail orders of copula families will be discussed, and they are helpful in guiding us to find suitable copula families for statistical inference.</description>
    </item>
    
    <item>
      <title>Submodel selection and post estimation: Making sense or folly</title>
      <link>/post/2013fall/2013-11-15/</link>
      <pubDate>Fri, 15 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-11-15/</guid>
      <description>Date: 2013-11-15 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we consider estimation in generalized linear models when there are many potential predictors and some of them may not have influence on the response of interest. In the context of two competing models where one model includes all predictors and the other restricts variable coefficients to a candidate linear subspace based on subject matter or prior knowledge, we investigate the relative performances of Stein type shrinkage, pretest, and penalty estimators (L1GLM, adaptive L1GLM, and SCAD) with respect to the full model estimator.</description>
    </item>
    
    <item>
      <title>The inadequacy of the summed score (and how you can fix it!)</title>
      <link>/post/2013fall/2013-11-08/</link>
      <pubDate>Fri, 08 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-11-08/</guid>
      <description>Date: 2013-11-08 Time: 15:30-16:30 Location: BURN 1205 Abstract: Health researchers often use patient and physician questionnaires to assess certain aspects of health status. Item Response Theory (IRT) provides a set of tools for examining the properties of the instrument and for estimation of the latent trait for each individual. In my research, I critically examine the usefulness of the summed score over items and an alternative weighted summed score (using weights computed from the IRT model) as an alternative to both the empirical Bayes estimator and maximum likelihood estimator for the Generalized Partial Credit Model.</description>
    </item>
    
    <item>
      <title>Bayesian latent variable modelling of longitudinal family data for genetic pleiotropy studies</title>
      <link>/post/2013fall/2013-11-01/</link>
      <pubDate>Fri, 01 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-11-01/</guid>
      <description>Date: 2013-11-01 Time: 15:30-16:30 Location: BURN 1205 Abstract: Motivated by genetic association studies of pleiotropy, we propose a Bayesian latent variable approach to jointly study multiple outcomes or phenotypes. The proposed method models both continuous and binary phenotypes, and it accounts for serial and familial correlations when longitudinal and pedigree data have been collected. We present a Bayesian estimation method for the model parameters and we discuss some of the model misspecification effects.</description>
    </item>
    
    <item>
      <title>Whole genome 3D architecture of chromatin and regulation</title>
      <link>/post/2013fall/2013-10-18/</link>
      <pubDate>Fri, 18 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-10-18/</guid>
      <description>Date: 2013-10-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: The expression of a gene is usually controlled by the regulatory elements in its promoter region. However, it has long been hypothesized that, in complex genomes, such as the human genome, a gene may be controlled by distant enhancers and repressors. A recent molecular technique, 3C (chromosome conformation capture), that uses formaldehyde cross-linking and locus-specific PCR, was able to detect physical contacts between distant genomic loci.</description>
    </item>
    
    <item>
      <title>Some recent developments in likelihood-based small area estimation</title>
      <link>/post/2013fall/2013-10-04/</link>
      <pubDate>Fri, 04 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-10-04/</guid>
      <description>Date: 2013-10-04 Time: 15:30-16:30 Location: BURN 1205 Abstract: Mixed models are commonly used for the analysis data in small area estimation. In particular, small area estimation has been extensively studied under linear mixed models. However, in practice there are many situations that we have counts or proportions in small area estimation; for example a (monthly) dataset on the number of incidences in small areas. Recently, small area estimation under the linear mixed model with penalized spline model, for xed part of the model, was studied.</description>
    </item>
    
    <item>
      <title>Tests of independence for sparse contingency tables and beyond</title>
      <link>/post/2013fall/2013-09-20/</link>
      <pubDate>Fri, 20 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-09-20/</guid>
      <description>Date: 2013-09-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, a new and consistent statistic is proposed to test whether two discrete random variables are independent. The test is based on a statistic of the Cramér–von Mises type constructed from the so-called empirical checkerboard copula. The test can be used even for sparse contingency tables or tables whose dimension changes with the sample size. Because the limiting distribution of the test statistic is not tractable, a valid bootstrap procedure for the computation of p-values will be discussed.</description>
    </item>
    
    <item>
      <title>Bayesian nonparametric density estimation under length bias sampling</title>
      <link>/post/2013fall/2013-09-13/</link>
      <pubDate>Fri, 13 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-09-13/</guid>
      <description>Date: 2013-09-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: A new density estimation method in a Bayesian nonparametric framework is presented when recorded data are not coming directly from the distribution of interest, but from a length biased version. From a Bayesian perspective, efforts to computationally evaluate posterior quantities conditionally on length biased data were hindered by the inability to circumvent the problem of a normalizing constant. In this talk a novel Bayesian nonparametric approach to the length bias sampling problem is presented which circumvents the issue of the normalizing constant.</description>
    </item>
    
    <item>
      <title>Éric Marchand: On improved predictive density estimation with parametric constraints</title>
      <link>/post/2013winter/2013-04-05/</link>
      <pubDate>Fri, 05 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-04-05/</guid>
      <description>Date: 2013-04-05 Time: 14:30-15:30 Location: BURN 1205 Abstract: We consider the problem of predictive density estimation under Kullback-Leibler loss when the parameter space is restricted to a convex subset. The principal situation analyzed relates to the estimation of an unknown predictive p-variate normal density based on an observation generated by another p-variate normal density. The means of the densities are assumed to coincide, the covariance matrices are a known multiple of the identity matrix.</description>
    </item>
    
    <item>
      <title>Jiahua Chen: Quantile and quantile function estimations under density ratio model</title>
      <link>/post/2013winter/2013-03-15/</link>
      <pubDate>Fri, 15 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-03-15/</guid>
      <description>Date: 2013-03-15 Time: 14:30-15:30 Location: BURN 1205 Abstract: Join work with Yukun Liu (East China Normal University)
Population quantiles and their functions are important parameters in many applications. For example, the lower level quantiles often serve as crucial quality indices of forestry products and others. In the presence of several independent samples from populations satisfying density ratio model, we investigate the properties of the empirical likelihood (EL) based inferences of quantiles and their functions.</description>
    </item>
    
    <item>
      <title>Natalia Stepanova: On asymptotic efficiency of some nonparametric tests for testing multivariate independence</title>
      <link>/post/2013winter/2013-03-01/</link>
      <pubDate>Fri, 01 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-03-01/</guid>
      <description>Date: 2013-03-01 Time: 14:30-15:30 Location: BURN 1205 Abstract: Some problems of statistics can be reduced to extremal problems of minimizing functionals of smooth functions defined on the cube $[0,1]^m$, $m\geq 2$. In this talk, we consider a class of extremal problems that is closely connected to the problem of testing multivariate independence. By solving the extremal problem, we provide a unified approach to establishing weak convergence for a wide class of empirical processes which emerge in connection with testing multivariate independence.</description>
    </item>
    
    <item>
      <title>Eric Cormier: Data Driven Nonparametric Inference for Bivariate Extreme-Value Copulas</title>
      <link>/post/2013winter/2013-02-15/</link>
      <pubDate>Fri, 15 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-02-15/</guid>
      <description>Date: 2013-02-15 Time: 14:30-15:30 Location: BURN 1205 Abstract: It is often crucial to know whether the dependence structure of a bivariate distribution belongs to the class of extreme-­‐value copulas. In this talk, I will describe a graphical tool that allows judgment regarding the existence of extreme-­‐value dependence. I will also present a data-­‐ driven nonparametric estimator of the Pickands dependence function. This estimator, which is constructed from constrained b-­‐splines, is intrinsic and differentiable, thereby enabling sampling from the fitted model.</description>
    </item>
    
    <item>
      <title>Celia Greenwood: Multiple testing and region-based tests of rare genetic variation</title>
      <link>/post/2013winter/2013-02-08/</link>
      <pubDate>Fri, 08 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-02-08/</guid>
      <description>Date: 2013-02-08 Time: 14:30-15:30 Location: BURN 1205 Abstract: In the context of univariate association tests between a trait of interest and common genetic variants (SNPs) across the whole genome, corrections for multiple testing have been well-studied. Due to the patterns of correlation (i.e. linkage disequilibrium), the number of independent tests remains close to 1 million, even when many more common genetic markers are available. With the advent of the DNA sequencing era, however, newly-identified genetic variants tend to be rare or even unique, and consequently single-variant tests of association have little power.</description>
    </item>
    
    <item>
      <title>Daniela Witten: Structured learning of multiple Gaussian graphical models</title>
      <link>/post/2013winter/2013-02-01/</link>
      <pubDate>Fri, 01 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-02-01/</guid>
      <description>Date: 2013-02-01 Time: 14:30-15:30 Location: BURN 1205 Abstract: I will consider the task of estimating high-dimensional Gaussian graphical models (or networks) corresponding to a single set of features under several distinct conditions. In other words, I wish to estimate several distinct but related networks. I assume that most aspects of the networks are shared, but that there are some structured differences between them. The goal is to exploit the similarity among the networks in order to obtain more accurate estimates of each individual network, as well as to identify the differences between the networks.</description>
    </item>
    
    <item>
      <title>Mylène Bédard: On the empirical efficiency of local MCMC algorithms with pools of proposals</title>
      <link>/post/2013winter/2013-01-25/</link>
      <pubDate>Fri, 25 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-01-25/</guid>
      <description>Date: 2013-01-25 Time: 14:30-15:30 Location: BURN 1205 Abstract: In an attempt to improve on the Metropolis algorithm, various MCMC methods with auxiliary variables, such as the multiple-try and delayed rejection Metropolis algorithms, have been proposed. These methods generate several candidates in a single iteration; accordingly they are computationally more intensive than the Metropolis algorithm. It is usually difficult to provide a general estimate for the computational cost of a method without being overly conservative; potentially efficient methods could thus be overlooked by relying on such estimates.</description>
    </item>
    
    <item>
      <title>Ana Best: Risk-set sampling, left truncation, and Bayesian methods in survival analysis</title>
      <link>/post/2013winter/2013-01-11/</link>
      <pubDate>Fri, 11 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-01-11/</guid>
      <description>Date: 2013-01-11 Time: 14:30-15:30 Location: BURN 1205 Abstract: Statisticians are often faced with budget concerns when conducting studies. The collection of some covariates, such as genetic data, is very expensive. Other covariates, such as detailed histories, might be difficult or time-consuming to measure. This helped bring about the invention of the nested case-control study, and its more generalized version, risk-set sampled survival analysis. The literature has a good discussion of the properties of risk-set sampling in standard right-censored survival data.</description>
    </item>
    
    <item>
      <title>Sample size and power determination for multiple comparison procedures aiming at rejecting at least r among m false hypotheses</title>
      <link>/post/2012fall/2012-12-07/</link>
      <pubDate>Fri, 07 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-12-07/</guid>
      <description>Date: 2012-12-07 Time: 14:30-15:30 Location: BURN 1205 Abstract: Multiple testing problems arise in a variety of situations, notably in clinical trials with multiple endpoints. In such cases, it is often of interest to reject either all hypotheses or at least one of them. More generally, the question arises as to whether one can reject at least r out of m hypotheses. Statistical tools addressing this issue are rare in the literature.</description>
    </item>
    
    <item>
      <title>Sharing confidential datasets using differential privacy</title>
      <link>/post/2012fall/2012-11-30/</link>
      <pubDate>Fri, 30 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-11-30/</guid>
      <description>Date: 2012-11-30 Time: 14:30-15:30 Location: BURN 1205 Abstract: While statistical agencies would like to share their data with researchers, they must also protect the confidentiality of the data provided by their respondents. To satisfy these two conflicting objectives, agencies use various techniques to restrict and modify the data before publication. Most of these techniques however share a common flaw: their confidentiality protection can not be rigorously measured. In this talk, I will present the criterion of differential privacy, a rigorous measure of the protection offered by such methods.</description>
    </item>
    
    <item>
      <title>Copula-based regression estimation and Inference</title>
      <link>/post/2012fall/2012-11-16/</link>
      <pubDate>Fri, 16 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-11-16/</guid>
      <description>Date: 2012-11-16 Time: 14:30-15:30 Location: BURN 1205 Abstract: In this paper we investigate a new approach of estimating a regression function based on copulas. The main idea behind this approach is to write the regression function in terms of a copula and marginal distributions. Once the copula and the marginal distributions are estimated we use the plug-in method to construct the new estimator. Because various methods are available in the literature for estimating both a copula and a distribution, this idea provides a rich and flexible alternative to many existing regression estimators.</description>
    </item>
    
    <item>
      <title>The multidimensional edge: Seeking hidden risks</title>
      <link>/post/2012fall/2012-11-09/</link>
      <pubDate>Fri, 09 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-11-09/</guid>
      <description>Date: 2012-11-09 Time: 14:30-15:30 Location: BURN 1205 Abstract: Assessing tail risks using the asymptotic models provided by multivariate extreme value theory has the danger that when asymptotic independence is present (as with the Gaussian copula model), the asymptotic model provides estimates of probabilities of joint tail regions that are zero. In diverse applications such as finance, telecommunications, insurance and environmental science, it may be difficult to believe in the absence of risk contagion.</description>
    </item>
    
    <item>
      <title>Multivariate extremal dependence: Estimation with bias correction</title>
      <link>/post/2012fall/2012-11-02/</link>
      <pubDate>Fri, 02 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-11-02/</guid>
      <description>Date: 2012-11-02 Time: 14:30-15:30 Location: BURN 1205 Abstract: Estimating extreme risks in a multivariate framework is highly connected with the estimation of the extremal dependence structure. This structure can be described via the stable tail dependence function L, for which several estimators have been introduced. Asymptotic normality is available for empirical estimates of L, with rate of convergence k^1&amp;frasl;2, where k denotes the number of high order statistics used in the estimation.</description>
    </item>
    
    <item>
      <title>Simulation model calibration and prediction using outputs from multi-fidelity simulators</title>
      <link>/post/2012fall/2012-10-26/</link>
      <pubDate>Fri, 26 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-10-26/</guid>
      <description>Date: 2012-10-26 Time: 14:30-15:30 Location: BURN 1205 Abstract: Computer simulators are used widely to describe physical processes in lieu of physical observations. In some cases, more than one computer code can be used to explore the same physical system - each with different degrees of fidelity. In this work, we combine field observations and model runs from deterministic multi-fidelity computer simulators to build a predictive model for the real process.</description>
    </item>
    
    <item>
      <title>	Modeling operational risk using a Bayesian approach to EVT</title>
      <link>/post/2012fall/2012-10-12/</link>
      <pubDate>Fri, 12 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-10-12/</guid>
      <description>Date: 2012-10-12 Time: 14:30-15:30 Location: BURN 1205 Abstract: Extreme Value Theory has been widely used for assessing risk for highly unusual events, either by using block maxima or peaks over the threshold (POT) methods. However, one of the main drawbacks of the POT method is the choice of a threshold, which plays an important role in the estimation since the parameter estimates strongly depend on this value. Bayesian inference is an alternative to handle these difficulties; the threshold can be treated as another parameter in the estimation, avoiding the classical empirical approach.</description>
    </item>
    
    <item>
      <title>Markov switching regular vine copulas</title>
      <link>/post/2012fall/2012-10-05/</link>
      <pubDate>Fri, 05 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-10-05/</guid>
      <description>Date: 2012-10-05 Time: 14:30-15:30 Location: BURN 1205 Abstract: Using only bivariate copulas as building blocks, regular vines(R-vines) constitute a flexible class of high-dimensional dependence models. In this talk we introduce a Markov switching R-vine copula model, combining the flexibility of general R-vine copulas with the possibility for dependence structures to change over time. Frequentist as well as Bayesian parameter estimation is discussed. Further, we apply the newly proposed model to examine the dependence of exchange rates as well as stock and stock index returns.</description>
    </item>
    
    <item>
      <title>The current state of Q-learning for personalized medicine</title>
      <link>/post/2012fall/2012-09-28/</link>
      <pubDate>Fri, 28 Sep 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-09-28/</guid>
      <description>Date: 2012-09-28 Time: 14:30-15:30 Location: BURN 1205 Abstract: In this talk, I will provide an introduction to DTRs and an overview the state of the art (and science) of Q-learning, a popular tool in reinforcement learning. The use of Q-learning and its variance in randomized and non-randomized studies will be discussed, as well as issues concerning inference as the resulting estimators are not always regular. Current and future directions of interest will also be considered.</description>
    </item>
    
    <item>
      <title>Hypothesis testing in finite mixture models: from the likelihood ratio test to EM-test</title>
      <link>/post/2012winter/2012-04-05/</link>
      <pubDate>Thu, 05 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-04-05/</guid>
      <description>Date: 2012-04-05 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the presence of heterogeneity, a mixture model is most natural to characterize the random behavior of the samples taken from such populations. Such strategy has been widely employed in applications ranging from genetics, information technology, marketing, to finance. Studying the mixing structure behind a random sample from the population allows us to infer the degree of heterogeneity with important implications in applications such as the presence of disease subgroups in genetics.</description>
    </item>
    
    <item>
      <title>A matching-based approach to assessing the surrogate value of a biomarker</title>
      <link>/post/2012winter/2012-03-30/</link>
      <pubDate>Fri, 30 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-30/</guid>
      <description>Date: 2012-03-30 Time: 15:30-16:30 Location: BURN 1205 Abstract: Statisticians have developed a number of frameworks which can be used to assess the surrogate value of a biomarker, i.e. establish whether treatment effects on a biological quantity measured shortly after administration of treatment predict treatment effects on the clinical endpoint of interest. The most commonly applied of these frameworks is due to Prentice (1989), who proposed a set of criteria which a surrogate marker should satisfy.</description>
    </item>
    
    <item>
      <title>Model selection principles in misspecified models</title>
      <link>/post/2012winter/2012-03-23/</link>
      <pubDate>Fri, 23 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-23/</guid>
      <description>Date: 2012-03-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: Model selection is of fundamental importance to high-dimensional modeling featured in many contemporary applications. Classical principles of model selection include the Bayesian principle and the Kullback-Leibler divergence principle, which lead to the Bayesian information criterion and Akaike information criterion, respectively, when models are correctly specified. Yet model misspecification is unavoidable in practice. We derive novel asymptotic expansions of the two well-known principles in misspecified generalized linear models, which give the generalized BIC (GBIC) and generalized AIC.</description>
    </item>
    
    <item>
      <title>Variable selection in longitudinal data with a change-point</title>
      <link>/post/2012winter/2012-03-16/</link>
      <pubDate>Fri, 16 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-16/</guid>
      <description>Date: 2012-03-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: Follow-up studies are frequently carried out to investigate the evolution of measurements through time, taken on a set of subjects. These measurements (responses) are bound to be influenced by subject specific covariates and if a regression model is used the data analyst is faced with the problem of selecting those covariates that “best explain” the data. For example, in a clinical trial, subjects may be monitored for a response following the administration of a treatment with a view of selecting the covariates that are best predictive of a treatment response.</description>
    </item>
    
    <item>
      <title>Estimating a variance-covariance surface for functional and longitudinal data</title>
      <link>/post/2012winter/2012-03-02/</link>
      <pubDate>Fri, 02 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-02/</guid>
      <description>Date: 2012-03-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: In functional data analysis, as in its multivariate counterpart, estimates of the bivariate covariance kernel σ(s,t ) and its inverse are useful for many things, and we need the inverse of a covariance matrix or kernel especially often. However, the dimensionality of functional observations often exceeds the sample size available to estimate σ(s,t, and then the analogue S of the multivariate sample estimate is singular and non-invertible.</description>
    </item>
    
    <item>
      <title>McGillivray: A penalized quasi-likelihood approach for estimating the number of states in a hidden Markov model | Best: Risk-set sampling and left truncation in survival analysis</title>
      <link>/post/2012winter/2012-02-17/</link>
      <pubDate>Fri, 17 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-02-17/</guid>
      <description>Date: 2012-02-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: McGillivray: In statistical applications of hidden Markov models (HMMs), one may have no knowledge of the number of hidden states (or order) of the model needed to be able to accurately represent the underlying process of the data. The problem of estimating the number of hidden states of the HMM is thus brought to the forefront. In this talk, we present a penalized quasi-likelihood approach for order estimation in HMMs which makes use of the fact that the marginal distribution of the observations from a HMM is a finite mixture model.</description>
    </item>
    
    <item>
      <title>Du: Simultaneous fixed and random effects selection in finite mixtures of linear mixed-effects models | Harel: Measuring fatigue in systemic sclerosis: a comparison of the SF-36 vitality subscale and FACIT fatigue scale using item response theory</title>
      <link>/post/2012winter/2012-02-03/</link>
      <pubDate>Fri, 03 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-02-03/</guid>
      <description>Date: 2012-02-03 Time: 15:30-16:30 Location: BURN 1205 Abstract: Du: Linear mixed-effects (LME) models are frequently used for modeling longitudinal data. One complicating factor in the analysis of such data is that samples are sometimes obtained from a population with significant underlying heterogeneity, which would be hard to capture by a single LME model. Such problems may be addressed by a finite mixture of linear mixed-effects (FMLME) models, which segments the population into subpopulations and models each subpopulation by a distinct LME model.</description>
    </item>
    
    <item>
      <title>Applying Kalman filtering to problems in causal inference</title>
      <link>/post/2012winter/2012-01-27/</link>
      <pubDate>Fri, 27 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-01-27/</guid>
      <description>Date: 2012-01-27 Time: 15:30-16:30 Location: BURN 1205 Abstract: A common problem in observational studies is estimating the causal effect of time-varying treatment in the presence of a time varying confounder. When random assignment of subjects to comparison groups is not possible, time-varying confounders can cause bias in estimating causal effects even after standard regression adjustment if past treatment history is a predictor of future confounders. To eliminate the bias of standard methods for estimating the causal effect of time varying treatment, Robins developed a number of innovative methods for discrete treatment levels, including G-computation, G-estimation, and marginal structural models (MSMs).</description>
    </item>
    
    <item>
      <title>A concave regularization technique for sparse mixture models</title>
      <link>/post/2012winter/2012-01-20/</link>
      <pubDate>Fri, 20 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-01-20/</guid>
      <description>Date: 2012-01-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Latent variable mixture models are a powerful tool for exploring the structure in large datasets. A common challenge for interpreting such models is a desire to impose sparsity, the natural assumption that each data point only contains few latent features. Since mixture distributions are constrained in their L1 norm, typical sparsity techniques based on L1 regularization become toothless, and concave regularization becomes necessary.</description>
    </item>
    
    <item>
      <title>Path-dependent estimation of a distribution under generalized censoring</title>
      <link>/post/2011fall/2011-12-02/</link>
      <pubDate>Fri, 02 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-12-02/</guid>
      <description>Date: 2011-12-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: This talk focuses on the problem of the estimation of a distribution on an arbitrary complete separable metric space when the data points are subject to censoring by a general class of random sets. A path-dependent estimator for the distribution is proposed; among other properties, the estimator is sequential in the sense that it only uses data preceding any fixed point at which it is evaluated.</description>
    </item>
    
    <item>
      <title>Estimation of the risk of a collision when using a cell phone while driving</title>
      <link>/post/2011fall/2011-11-25/</link>
      <pubDate>Fri, 25 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-11-25/</guid>
      <description>Date: 2011-11-25 Time: 15:30-16:30 Location: BURN 1205 Abstract: The use of cell phone while driving raises the question of whether it is associated with an increased collision risk and if so, what is its magnitude. For policy decision making, it is important to rely on an accurate estimate of the real crash risk of cell phone use while driving. Three important epidemiological studies were published on the subject, two using the case-crossover approach and one using a more conventional longitudinal cohort design.</description>
    </item>
    
    <item>
      <title>Construction of bivariate distributions via principal components</title>
      <link>/post/2011fall/2011-11-18/</link>
      <pubDate>Fri, 18 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-11-18/</guid>
      <description>Date: 2011-11-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: The diagonal expansion of a bivariate distribution (Lancaster, 1958) has been used as a tool to construct bivariate distributions; this method has been generalized using principal dimensions of random variables (Cuadras 2002). Sufficient and necessary conditions are given for uniform, exponential, logistic and Pareto marginals in the one and two-dimensional case. The corresponding copulas are obtained.
Speaker Amparo Casanova is an Assistant Professor at the Dalla Lana School of Public Health, Division of Biostatistics, University of Toronto.</description>
    </item>
    
    <item>
      <title>A Bayesian method of parametric inference for diffusion processes</title>
      <link>/post/2011fall/2011-11-04/</link>
      <pubDate>Fri, 04 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-11-04/</guid>
      <description>Date: 2011-11-04 Time: 15:30-16:30 Location: BURN 1205 Abstract: Diffusion processes have been used to model a multitude of continuous-time phenomena in Engineering and the Natural Sciences, and as in this case, the volatility of financial assets. However, parametric inference has long been complicated by an intractable likelihood function. For many models the most effective solution involves a large amount of missing data for which the typical Gibbs sampler can be arbitrarily slow.</description>
    </item>
    
    <item>
      <title>Maximum likelihood estimation in network models</title>
      <link>/post/2011fall/2011-11-03/</link>
      <pubDate>Thu, 03 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-11-03/</guid>
      <description>Date: 2011-11-03 Time: 16:00-17:00 Location: BURN 1205 Abstract: This talk is concerned with maximum likelihood estimation (MLE) in exponential statistical models for networks (random graphs) and, in particular, with the beta model, a simple model for undirected graphs in which the degree sequence is the minimal sufficient statistic. The speaker will present necessary and sufficient conditions for the existence of the MLE of the beta model parameters that are based on a geometric object known as the polytope of degree sequences.</description>
    </item>
    
    <item>
      <title>Simulated method of moments estimation for copula-based multivariate models</title>
      <link>/post/2011fall/2011-10-28/</link>
      <pubDate>Fri, 28 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-10-28/</guid>
      <description>Date: 2011-10-28 Time: 15:00-16:00 Location: BURN 1205 Abstract: This paper considers the estimation of the parameters of a copula via a simulated method of moments type approach. This approach is attractive when the likelihood of the copula model is not known in closed form, or when the researcher has a set of dependence measures or other functionals of the copula, such as pricing errors, that are of particular interest. The proposed approach naturally also nests method of moments and generalized method of moments estimators.</description>
    </item>
    
    <item>
      <title>Bayesian modelling of GWAS data using linear mixed models</title>
      <link>/post/2011fall/2011-10-21/</link>
      <pubDate>Fri, 21 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-10-21/</guid>
      <description>Date: 2011-10-21 Time: 15:30-16:30 Location: BURN 1205 Abstract: Genome-wide association studies (GWAS) are used to identify physical positions (loci) on the genome where genetic variation is causally associated with a phenotype of interest at the population level. Typical studies are based on the measurement of several hundred thousand single nucleotide polymorphism (SNP) variants spread across the genome, in a few thousand individuals. The resulting datasets are large and require computationally efficient methods of statistical analysis.</description>
    </item>
    
    <item>
      <title>Nonexchangeability and radial asymmetry identification via bivariate quantiles, with financial applications</title>
      <link>/post/2011fall/2011-10-07/</link>
      <pubDate>Fri, 07 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-10-07/</guid>
      <description>Date: 2011-10-07 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, the following topics will be discussed: A class of bivariate probability integral transforms and Kendall distribution; bivariate quantile curves, central and lateral regions; non-exchangeability and radial asymmetry identification; new measures of nonexchangeability and radial asymmetry; financial applications and a few open problems (joint work with Flavio Ferreira).
Speaker Nikolai Kolev is a Professor of Statistics at the University of Sao Paulo, Brazil.</description>
    </item>
    
    <item>
      <title>Data sketching for cardinality and entropy estimation?</title>
      <link>/post/2011fall/2011-09-30/</link>
      <pubDate>Fri, 30 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-09-30/</guid>
      <description>Date: 2011-09-30 Time: 15:30-16:30 Location: BURN 1205 Abstract: Streaming data is ubiquitous in a wide range of areas from engineering and information technology, finance, and commerce, to atmospheric physics, and earth sciences. The online approximation of properties of data streams is of great interest, but this approximation process is hindered by the sheer size of the data and the speed at which it is generated. Data stream algorithms typically allow only one pass over the data, and maintain sub-linear representations of the data from which target properties can be inferred with high efficiency.</description>
    </item>
    
    <item>
      <title>What is singular learning theory?</title>
      <link>/post/2011fall/2011-09-23/</link>
      <pubDate>Fri, 23 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-09-23/</guid>
      <description>Date: 2011-09-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we give a basic introduction to Sumio Watanabe&amp;rsquo;s Singular Learning Theory, as outlined in his book &amp;ldquo;Algebraic Geometry and Statistical Learning Theory&amp;rdquo;. Watanabe&amp;rsquo;s key insight to studying singular models was to use a deep result in algebraic geometry known as Hironaka&amp;rsquo;s Resolution of Singularities. This result allows him to reparametrize the model in a normal form so that central limit theorems can be applied.</description>
    </item>
    
    <item>
      <title>Inference and model selection for pair-copula constructions</title>
      <link>/post/2011fall/2011-09-16/</link>
      <pubDate>Fri, 16 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-09-16/</guid>
      <description>Date: 2011-09-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: Pair-copula constructions (PCCs) provide an elegant way to construct highly flexible multivariate distributions. However, for convenience of inference, pair-copulas are often assumed to depend on the conditioning variables only indirectly. In this talk, I will show how nonparametric smoothing techniques can be used to avoid this assumption. Model selection for PCCs will also be addressed within the proposed method.
Speaker Elif F.</description>
    </item>
    
    <item>
      <title>Precision estimation for stereological volumes</title>
      <link>/post/2011fall/2011-08-31/</link>
      <pubDate>Wed, 31 Aug 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-08-31/</guid>
      <description>Date: 2011-08-31 Time: 15:30-16:30 Location: BURN 1205 Abstract: Volume estimators based on Cavalieri’s principle are widely used in the bio- sciences. For example in neuroscience, where volumetric measurements of brain structures are of interest, systematic samples of serial sections are obtained by magnetic resonance imaging or by a physical cutting procedure. The volume v is then estimated by ˆv, which is the sum over the areas of the structure of interest in the section planes multiplied by the width of the sections, t &amp;gt; 0.</description>
    </item>
    
  </channel>
</rss>