<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CRM-Colloquium on McGill Statistics Seminars</title>
    <link>https://mcgillstat.github.io/categories/crm-colloquium/</link>
    <description>Recent content in CRM-Colloquium on McGill Statistics Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://mcgillstat.github.io/categories/crm-colloquium/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Structure learning for extremal graphical models</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-02-18/</link>
      <pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2022winter/2022-02-18/</guid>
      <description>Date: 2022-02-18 Time: 15:30-16:30 (Montreal time) https://umontreal.zoom.us/j/85105423917?pwd=enM3MGpFNkZKU2daMjRITmo0N0JUUT09 Meeting ID: 851 0542 3917 Passcode: 403790 Abstract: Extremal graphical models are sparse statistical models for multivariate extreme events. The underlying graph encodes conditional independencies and enables a visual interpretation of the complex extremal dependence structure. For the important case of tree models, we provide a data-driven methodology for learning the graphical structure. We show that sample versions of the extremal correlation and a new summary statistic, which we call the extremal variogram, can be used as weights for a minimum spanning tree to consistently recover the true underlying tree.</description>
    </item>
    
    <item>
      <title>Risk assessment, heavy tails, and asymmetric least squares techniques</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-01-28/</link>
      <pubDate>Fri, 28 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2022winter/2022-01-28/</guid>
      <description>Date: 2022-01-28 Time: 15:30-16:30 (Montreal time) https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09 Meeting ID: 939 8331 3215 Passcode: 096952 Abstract: Statistical risk assessment, in particular in finance and insurance, requires estimating simple indicators to summarize the risk incurred in a given situation. Of most interest is to infer extreme levels of risk so as to be able to manage high-impact rare events such as extreme climate episodes or stock market crashes. A standard procedure in this context, whether in the academic, industrial or regulatory circles, is to estimate a well-chosen single quantile (or Value-at-Risk).</description>
    </item>
    
    <item>
      <title>Adventures with Partial Identifications in Studies of Marked Individuals</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-11-26/</link>
      <pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2021fall/2021-11-26/</guid>
      <description>Date: 2021-11-26 Time: 15:30-16:30 (Montreal time) Zoom Link Meeting ID: 939 8331 3215 Passcode: 096952 Abstract: Monitoring marked individuals is a common strategy in studies of wild animals (referred to as mark-recapture or capture-recapture experiments) and hard to track human populations (referred to as multi-list methods or multiple-systems estimation). A standard assumption of these techniques is that individuals can be identified uniquely and without error, but this can be violated in many ways.</description>
    </item>
    
    <item>
      <title>Opinionated practices for teaching reproducibility: motivation, guided instruction and practice</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-10-29/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2021fall/2021-10-29/</guid>
      <description>Date: 2021-10-29 Time: 15:30-16:30 (Montreal time) Zoom Link Meeting ID: 939 8331 3215 Passcode: 096952 Abstract: In the data science courses at the University of British Columbia, we define data science as the study, development and practice of reproducible and auditable processes to obtain insight from data. While reproducibility is core to our definition, most data science learners enter the field with other aspects of data science in mind, for example predictive modelling, which is often one of the most interesting topic to novices.</description>
    </item>
    
    <item>
      <title>Deep down, everyone wants to be causal</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-09-24/</link>
      <pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2021fall/2021-09-24/</guid>
      <description>Date: 2021-09-24 Time: 15:00-16:00 (Montreal time) https://mcgill.zoom.us/j/9791073141 Meeting ID: 979 107 3141 Abstract: In the data science courses at the University of British Columbia, we define data science as the study, development and practice of reproducible and auditable processes to obtain insight from data. While reproducibility is core to our definition, most data science learners enter the field with other aspects of data science in mind, for example predictive modelling, which is often one of the most interesting topic to novices.</description>
    </item>
    
    <item>
      <title>Nonparametric Tests for Informative Selection in Complex Surveys</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-03-12/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2021winter/2021-03-12/</guid>
      <description>Date: 2021-03-12 Time: 15:30-16:30 (Montreal time) Zoom Link Meeting ID: 939 8331 3215 Passcode: 096952 Abstract: Informative selection, in which the distribution of response variables given that they are sampled is different from their distribution in the population, is pervasive in complex surveys. Failing to take such informativeness into account can produce severe inferential errors, including biased and inconsistent estimation of population parameters. While several parametric procedures exist to test for informative selection, these methods are limited in scope and their parametric assumptions are difficult to assess.</description>
    </item>
    
    <item>
      <title>Spatio-temporal methods for estimating subsurface ocean thermal response to tropical cyclones</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-02-12/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2021winter/2021-02-12/</guid>
      <description>Date: 2021-02-12 Time: 15:30-16:30 (Montreal time) Zoom Link Meeting ID: 939 8331 3215 Passcode: 096952 Abstract: Tropical cyclones (TCs), driven by heat exchange between the air and sea, pose a substantial risk to many communities around the world. Accurate characterization of the subsurface ocean thermal response to TC passage is crucial for accurate TC intensity forecasts and for understanding the role TCs play in the global climate system, yet that characterization is complicated by the high-noise ocean environment, correlations inherent in spatio-temporal data, relative scarcity of in situ observations and the entanglement of the TC-induced signal with seasonal signals.</description>
    </item>
    
    <item>
      <title>Small Area Estimation in Low- and Middle-Income Countries</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-01-29/</link>
      <pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2021winter/2021-01-29/</guid>
      <description>Date: 2021-01-29 Time: 15:30-16:30 (Montreal time) Zoom Link Meeting ID: 939 8331 3215 Passcode: 096952 Abstract: The under-five mortality rate (U5MR) is a key barometer of the health of a nation. Unfortunately, many people living in low- and middle-income countries are not covered by civil registration systems. This makes estimation of the U5MR, particularly at the subnational level, difficult. In this talk, I will describe models that have been developed to produce the official United Nations (UN) subnational U5MR estimates in 22 countries.</description>
    </item>
    
    <item>
      <title>Approximate Cross-Validation for Large Data and High Dimensions</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-11-13/</link>
      <pubDate>Fri, 13 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2020fall/2020-11-13/</guid>
      <description>Date: 2020-11-13 Time: 15:30-16:30 Zoom Link Abstract: The error or variability of statistical and machine learning algorithms is often assessed by repeatedly re-fitting a model with different weighted versions of the observed data. The ubiquitous tools of cross-validation (CV) and the bootstrap are examples of this technique. These methods are powerful in large part due to their model agnosticism but can be slow to run on modern, large data sets due to the need to repeatedly re-fit the model.</description>
    </item>
    
    <item>
      <title>Data Science, Classification, Clustering and Three-Way Data</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-02/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-02/</guid>
      <description>Date: 2020-10-02 Time: 15:30-16:30 Zoom Link Meeting ID: 939 8331 3215 Passcode: 096952 Abstract: Data science is discussed along with some historical perspective. Selected problems in classification are considered, either via specific datasets or general problem types. In each case, the problem is introduced before one or more potential solutions are discussed and applied. The problems discussed include data with outliers, longitudinal data, and three-way data. The proposed approaches are generally mixture model-based.</description>
    </item>
    
    <item>
      <title>Machine Learning for Causal Inference</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-09-11/</link>
      <pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2020fall/2020-09-11/</guid>
      <description>Date: 2020-09-11 Time: 16:00-17:00 Zoom Link Meeting ID: 965 2536 7383 Passcode: 421254 Abstract: Given advances in machine learning over the past decades, it is now possible to accurately solve difficult non-parametric prediction problems in a way that is routine and reproducible. In this talk, I’ll discuss how machine learning tools can be rigorously integrated into observational study analyses, and how they interact with classical statistical ideas around randomization, semiparametric modeling, double robustness, etc.</description>
    </item>
    
    <item>
      <title>Neyman-Pearson classification: parametrics and sample size requirement</title>
      <link>https://mcgillstat.github.io/post/2020winter/2020-02-28/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2020winter/2020-02-28/</guid>
      <description>Date: 2020-02-28 Time: 15:30-16:30 Location: BURNSIDE 1104 Abstract: The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level alpha. This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong, Feng and Li (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.</description>
    </item>
    
    <item>
      <title>Formulation and solution of stochastic inverse problems for science and engineering models</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-11-22/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2019fall/2019-11-22/</guid>
      <description>Date: 2019-11-22 Time: 16:00-17:00 Location: Pavillon Kennedy, PK-5115, UQAM Abstract: The stochastic inverse problem of determining probability structures on input parameters for a physics model corresponding to a given probability structure on the output of the model forms the core of scientific inference and engineering design. We describe a formulation and solution method for stochastic inverse problems that is based on functional analysis, differential geometry, and probability/measure theory. This approach yields a computationally tractable problem while avoiding alterations of the model like regularization and ad hoc assumptions about the probability structures.</description>
    </item>
    
    <item>
      <title>General Bayesian Modeling</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-11-01/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2019fall/2019-11-01/</guid>
      <description>Date: 2019-11-01 Time: 16:00-17:00 Location: BURN 1104 Abstract: The work is motivated by the inflexibility of Bayesian modeling; in that only parameters of probability models are required to be connected with data. The idea is to generalize this by allowing arbitrary unknowns to be connected with data via loss functions. An updating process is then detailed which can be viewed as arising in at least a couple of ways - one being purely axiomatically driven.</description>
    </item>
    
    <item>
      <title>Network models, sampling, and symmetry properties</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-02-01/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2019winter/2019-02-01/</guid>
      <description>Date: 2019-02-01 Time: 15:30-16:30 Location: BURN 1205 Abstract: A recent body of work, by myself and many others, aims to develop a statistical theory of network data for problems a single network is observed. Of the models studied in this area, graphon models are probably most widely known in statistics. I will explain the relationship between three aspects of this work: (1) Specific models, such as graphon models, graphex models, and edge-exchangeable graphs.</description>
    </item>
    
    <item>
      <title>The Law of Large Populations: The return of the long-ignored N and how it can affect our 2020 vision</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-02-16/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2018winter/2018-02-16/</guid>
      <description>Date: 2018-02-16 Time: 15:30-16:30 Location: McGill University, OTTO MAASS 217 Abstract: For over a century now, we statisticians have successfully convinced ourselves and almost everyone else, that in statistical inference the size of the population N can be ignored, especially when it is large. Instead, we focused on the size of the sample, n, the key driving force for both the Law of Large Numbers and the Central Limit Theorem. We were thus taught that the statistical error (standard error) goes down with n typically at the rate of 1/√n.</description>
    </item>
    
    <item>
      <title>150 years (and more) of data analysis in Canada</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-11-24/</link>
      <pubDate>Fri, 24 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2017fall/2017-11-24/</guid>
      <description>Date: 2017-11-24 Time: 15:30-16:30 Location: LEA 232 Abstract: As Canada celebrates its 150th anniversary, it may be good to reflect on the past and future of data analysis and statistics in this country. In this talk, I will review the Victorian Statistics Movement and its effect in Canada, data analysis by a Montréal physician in the 1850s, a controversy over data analysis in the 1850s and 60s centred in Montréal, John A.</description>
    </item>
    
    <item>
      <title>McNeil: Spectral backtests of forecast distributions with application to risk management | Jasiulis-Goldyn: Asymptotic properties and renewal theory for Kendall random walks</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-09-29/</link>
      <pubDate>Fri, 29 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2017fall/2017-09-29/</guid>
      <description>Date: 2017-09-29 Time: 14:30-16:30 Location: BURN 1205 Abstract: McNeil: In this talk we study a class of backtests for forecast distributions in which the test statistic is a spectral transformation that weights exceedance events by a function of the modelled probability level. The choice of the kernel function makes explicit the user’s priorities for model performance. The class of spectral backtests includes tests of unconditional coverage and tests of conditional coverage.</description>
    </item>
    
    <item>
      <title>Instrumental Variable Regression with Survival Outcomes</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-04-06/</link>
      <pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2017winter/2017-04-06/</guid>
      <description>Date: 2017-04-06 Time: 15:30-16:30 Location: Universite Laval, Pavillon Vachon, Salle 3840 Abstract: Instrumental variable (IV) methods are popular in non-experimental studies to estimate the causal effects of medical interventions or exposures. These approaches allow for the consistent estimation of such effects even if important confounding factors are unobserved. Despite the increasing use of these methods, there have been few extensions of IV methods to censored data regression problems. We discuss challenges in applying IV structural equational modelling techniques to the proportional hazards model and suggest alternative modelling frameworks.</description>
    </item>
    
    <item>
      <title>Inference in dynamical systems</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-03-17/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2017winter/2017-03-17/</guid>
      <description>Date: 2017-03-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: We consider the asymptotic consistency of maximum likelihood parameter estimation for dynamical systems observed with noise. Under suitable conditions on the dynamical systems and the observations, we show that maximum likelihood parameter estimation is consistent. Furthermore, we show how some well-studied properties of dynamical systems imply the general statistical properties related to maximum likelihood estimation. Finally, we exhibit classical families of dynamical systems for which maximum likelihood estimation is consistent.</description>
    </item>
    
    <item>
      <title>High-dimensional changepoint estimation via sparse projection</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-12-01/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2016fall/2016-12-01/</guid>
      <description>Date: 2016-12-01 Time: 15:30-16:30 Location: BURN 708 Abstract: Changepoints are a very common feature of Big Data that arrive in the form of a data stream. We study high-dimensional time series in which, at certain time points, the mean structure changes in a sparse subset of the coordinates. The challenge is to borrow strength across the coordinates in order to detect smaller changes than could be observed in any individual component series.</description>
    </item>
    
    <item>
      <title>Efficient tests of covariate effects in two-phase failure time studies</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-10-28/</link>
      <pubDate>Fri, 28 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2016fall/2016-10-28/</guid>
      <description>Date: 2016-10-28 Time: 15:30-16:30 Location: BURN 1205 Abstract: Two-phase studies are frequently used when observations on certain variables are expensive or difficult to obtain. One such situation is when a cohort exists for which certain variables have been measured (phase 1 data); then, a sub-sample of individuals is selected, and additional data are collected on them (phase 2). Efficiency for tests and estimators can be increased by basing the selection of phase 2 individuals on data collected at phase 1.</description>
    </item>
    
    <item>
      <title>Statistical inference for fractional diffusion processes</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-09-16/</link>
      <pubDate>Fri, 16 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2016fall/2016-09-16/</guid>
      <description>Date: 2016-09-16 Time: 16:00-17:00 Location: LB-921.04, Library Building, Concordia Univ. Abstract: There are some time series which exhibit long-range dependence as noticed by Hurst in his investigations of river water levels along Nile river. Long-range dependence is connected with the concept of self-similarity in that increments of a self-similar process with stationary increments exhibit long-range dependence under some conditions. Fractional Brownian motion is an example of such a process. We discuss statistical inference for stochastic processes modeled by stochastic differential equations driven by a fractional Brownian motion.</description>
    </item>
    
    <item>
      <title>Ridges and valleys in the high excursion sets of Gaussian random fields</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-03-10/</link>
      <pubDate>Thu, 10 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2016winter/2016-03-10/</guid>
      <description>Date: 2016-03-10 Time: 15:30-16:30 Location: MAASS 217, McGill Abstract: It is well known that normal random variables do not like taking large values. Therefore, a continuous Gaussian random field on a compact set does not like exceeding a large level. If it does exceed a large level at some point, it tends to go back below the level a short distance away from that point. One, therefore, does not expect the excursion set above a high for such a field to possess any interesting structure.</description>
    </item>
    
    <item>
      <title>Causal discovery with confidence using invariance principles</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-12-10/</link>
      <pubDate>Thu, 10 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2015fall/2015-12-10/</guid>
      <description>Date: 2015-12-10 Time: 15:30-16:30 Location: UdeM, Pav. Roger-Gaudry, salle S-116 Abstract: What is interesting about causal inference? One of the most compelling aspects is that any prediction under a causal model is valid in environments that are possibly very different to the environment used for inference. For example, variables can be actively changed and predictions will still be valid and useful. This invariance is very useful but still leaves open the difficult question of inference.</description>
    </item>
    
    <item>
      <title>Inference regarding within-family association in disease onset times under biased sampling schemes</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-11-26/</link>
      <pubDate>Thu, 26 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2015fall/2015-11-26/</guid>
      <description>Date: 2015-11-26 Time: 15:30-16:30 Location: BURN 306 Abstract: In preliminary studies of the genetic basis for chronic conditions, interest routinely lies in the within-family dependence in disease status. When probands are selected from disease registries and their respective families are recruited, a variety of ascertainment bias-corrected methods of inference are available which are typically based on models for correlated binary data. This approach ignores the age that family members are at the time of assessment.</description>
    </item>
    
    <item>
      <title>A knockoff filter for controlling the false discovery rate</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-10-30/</link>
      <pubDate>Fri, 30 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2015fall/2015-10-30/</guid>
      <description>Date: 2015-10-30 Time: 16:00-17:00 Location: Salle 1360, Pavillon André-Aisenstadt, Université de Montréa Abstract: The big data era has created a new scientific paradigm: collect data first, ask questions later. Imagine that we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR) - the expected fraction of false discoveries among all discoveries - is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable.</description>
    </item>
    
    <item>
      <title>A statistical view of some recent climate controversies</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-05-07/</link>
      <pubDate>Thu, 07 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2015winter/2015-05-07/</guid>
      <description>Date: 2015-05-07 Time: 15:30-16:30 Location: Université de Sherbrooke Abstract: This talk looks at some recent climate controversies from a statistical standpoint. The issues are motivated via changepoints and their detection. Changepoints are ubiquitous features in climatic time series, occurring whenever stations relocate or gauges are changed. Ignoring changepoints can produce spurious trend conclusions. Changepoint tests involving cumulative sums, likelihood ratio, and maximums of F-statistics are introduced; the asymptotic distributions of these statistics are quantified under the changepoint-free null hypothesis.</description>
    </item>
    
    <item>
      <title>Functional data analysis and related topics</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-01-15/</link>
      <pubDate>Thu, 15 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2015winter/2015-01-15/</guid>
      <description>Date: 2015-01-15 Time: 16:00-17:00 Location: CRM 1360 (U. de Montréal) Abstract: Functional data analysis (FDA) has received substantial attention, with applications arising from various disciplines, such as engineering, public health, finance etc. In general, the FDA approaches focus on nonparametric underlying models that assume the data are observed from realizations of stochastic processes satisfying some regularity conditions, e.g., smoothness constraints. The estimation and inference procedures usually do not depend on merely a finite number of parameters, which contrasts with parametric models, and exploit techniques, such as smoothing methods and dimension reduction, that allow data to speak for themselves.</description>
    </item>
    
    <item>
      <title>High-dimensional phenomena in mathematical statistics and convex analysis</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-11-20/</link>
      <pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2014fall/2014-11-20/</guid>
      <description>Date: 2014-11-20 Time: 16:00-17:00 Location: CRM 1360 (U. de Montréal) Abstract: Statistical models in which the ambient dimension is of the same order or larger than the sample size arise frequently in different areas of science and engineering. Although high-dimensional models of this type date back to the work of Kolmogorov, they have been the subject of intensive study over the past decade, and have interesting connections to many branches of mathematics (including concentration of measure, random matrix theory, convex geometry, and information theory).</description>
    </item>
    
    <item>
      <title>Adaptive piecewise polynomial estimation via trend filtering</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-04-11/</link>
      <pubDate>Fri, 11 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2014winter/2014-04-11/</guid>
      <description>Date: 2014-04-11 Time: 15:30-16:30 Location: Salle KPMG, 1er étage HEC Montréal Abstract: We will discuss trend filtering, a recently proposed tool of Kim et al. (2009) for nonparametric regression. The trend filtering estimate is defined as the minimizer of a penalized least squares criterion, in which the penalty term sums the absolute kth order discrete derivatives over the input points. Perhaps not surprisingly, trend filtering estimates appear to have the structure of kth degree spline functions, with adaptively chosen knot points (we say “appear” here as trend filtering estimates are not really functions over continuous domains, and are only defined over the discrete set of inputs).</description>
    </item>
    
    <item>
      <title>Insurance company operations and dependence modeling</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-03-21/</link>
      <pubDate>Fri, 21 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2014winter/2014-03-21/</guid>
      <description>Date: 2014-03-21 Time: 15:30-16:30 Location: BURN 107 Abstract: Actuaries and other analysts have long had the responsibility in insurance company operations for various financial functions including (i) ratemaking, the process of setting premiums, (ii) loss reserving, the process of predicting obligations that arise from policies, and (iii) claims management, including fraud detection. With the advent of modern computing capabilities and detailed and novel data sources, new opportunities to make an impact on insurance company operations are extensive.</description>
    </item>
    
    <item>
      <title>ABC as the new empirical Bayes approach?</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-02-28/</link>
      <pubDate>Fri, 28 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2014winter/2014-02-28/</guid>
      <description>Date: 2014-02-28 Time: 13:30-14:30 Location: UdM, Pav. Roger-Gaudry, Salle S-116 Abstract: Approximate Bayesian computation (ABC) has now become an essential tool for the analysis of complex stochastic models when the likelihood function is unavailable. The approximation is seen as a nuisance from a computational statistic point of view but we argue here it is also a blessing from an inferential perspective. We illustrate this paradoxical stand in the case of dynamic models and population genetics models.</description>
    </item>
    
    <item>
      <title>Calibration of computer experiments with large data structures</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-01-24/</link>
      <pubDate>Fri, 24 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2014winter/2014-01-24/</guid>
      <description>Date: 2014-01-24 Time: 15:30-16:30 Location: Salle 1355, pavillon André-Aisenstadt (CRM) Abstract: Statistical model calibration of computer models is commonly done in a wide variety of scientific endeavours. In the end, this exercise amounts to solving an inverse problem and a form of regression. Gaussian process model are very convenient in this setting as non-parametric regression estimators and provide sensible inference properties. However, when the data structures are large, fitting the model becomes difficult.</description>
    </item>
    
    <item>
      <title>Great probabilists publish posthumously</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-12-06/</link>
      <pubDate>Fri, 06 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2013fall/2013-12-06/</guid>
      <description>Date: 2013-12-06 Time: 15:30-16:30 Location: UQAM Salle SH-3420 Abstract: Jacob Bernoulli died in 1705. His great book Ars Conjectandi was published in 1713, 300 years ago. Thomas Bayes died in 1761. His great paper was read to the Royal Society of London in December 1763, 250 years ago, and published in 1764. These anniversaries are noted by discussing new evidence regarding the circumstances of publication, which in turn can lead to a better understanding of the works themselves.</description>
    </item>
    
    <item>
      <title>Signal detection in high dimension: Testing sphericity against spiked alternatives</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-11-29/</link>
      <pubDate>Fri, 29 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2013fall/2013-11-29/</guid>
      <description>Date: 2013-11-29 Time: 15:30-16:30 Location: Concordia MB-2.270 Abstract: We consider the problem of testing the null hypothesis of sphericity for a high-dimensional covariance matrix against the alternative of a finite (unspecified) number of symmetry-breaking directions (multispiked alternatives) from the point of view of the asymptotic theory of statistical experiments. The region lying below the so-called phase transition or impossibility threshold is shown to be a contiguity region. Simple analytical expressions are derived for the asymptotic power envelope and the asymptotic powers of existing tests.</description>
    </item>
    
    <item>
      <title>XY - Basketball meets Big Data</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-10-25/</link>
      <pubDate>Fri, 25 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2013fall/2013-10-25/</guid>
      <description>Date: 2013-10-25 Time: 15:30-16:30 Location: HEC Montréal Salle CIBC 1er étage Abstract: In this talk, I will explore the state of the art in the analysis and modeling of player tracking data in the NBA. In the past, player tracking data has been used primarily for visualization, such as understanding the spatial distribution of a player’s shooting characteristics, or to extract summary statistics, such as the distance traveled by a player in a given game.</description>
    </item>
    
    <item>
      <title>Measurement error and variable selection in parametric and nonparametric models</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-09-27/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2013fall/2013-09-27/</guid>
      <description>Date: 2013-09-27 Time: 15:30-16:30 Location: RPHYS 114 Abstract: This talk will start with a discussion of the relationships between LASSO estimation, ridge regression, and attenuation due to measurement error as motivation for, and introduction to, a new generalizable approach to variable selection in parametric and nonparametric regression and discriminant analysis. The approach transcends the boundaries of parametric/nonparametric models. It will first be described in the familiar context of linear regression where its relationship to the LASSO will be described in detail.</description>
    </item>
    
    <item>
      <title>Arup Bose: Consistency of large dimensional sample covariance matrix under weak dependence</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-04-12/</link>
      <pubDate>Fri, 12 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2013winter/2013-04-12/</guid>
      <description>Date: 2013-04-12 Time: 14:30-15:30 Location: Concordia Abstract: Estimation of large dimensional covariance matrix has been of interest recently. One model assumes that there are $p$ dimensional independent identically distributed Gaussian observations $X_1, \ldots , X_n$ with dispersion matrix $\Sigma_p$ and $p$ grows much faster than $n$. Appropriate convergence rate results have been established in the literature for tapered and banded estimators of $\Sigma_p$ which are based on the sample variance covariance matrix of $n$ observations.</description>
    </item>
    
    <item>
      <title>Hélène Massam: The hyper Dirichlet revisited: a characterization</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-03-22/</link>
      <pubDate>Fri, 22 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2013winter/2013-03-22/</guid>
      <description>Date: 2013-03-22 Time: 14:30-15:30 Location: BURN 107 Abstract: We give a characterization of the hyper Dirichlet distribution hyper Markov with respect to a decomposable graph $G$ (or equivalently a moral directed acyclic graph). For $X=(X_1,\ldots,X_d)$ following the hyper Dirichlet distribution, our characterization is through the so-called &amp;ldquo;local and global independence properties&amp;rdquo; for a carefully designed family of orders of the variables $X_1,\ldots,X_d$.
The hyper Dirichlet for general directed acyclic graphs was derived from a characterization of the Dirichlet distribution given by Geiger and Heckerman (1997).</description>
    </item>
    
    <item>
      <title>Victor Chernozhukov: Inference on treatment effects after selection amongst high-dimensional controls</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-01-18/</link>
      <pubDate>Fri, 18 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2013winter/2013-01-18/</guid>
      <description>Date: 2013-01-18 Time: 14:30-15:30 Location: BURN 306 Abstract: We propose robust methods for inference on the effect of a treatment variable on a scalar outcome in the presence of very many controls. Our setting is a partially linear model with possibly non-Gaussian and heteroscedastic disturbances. Our analysis allows the number of controls to be much larger than the sample size. To make informative inference feasible, we require the model to be approximately sparse; that is, we require that the effect of confounding factors can be controlled for up to a small approximation error by conditioning on a relatively small number of controls whose identities are unknown.</description>
    </item>
    
    <item>
      <title>What percentage of children in the U.S. are eating a healthy diet? A statistical approach</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-12-14/</link>
      <pubDate>Fri, 14 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2012fall/2012-12-14/</guid>
      <description>Date: 2012-12-14 Time: 14:30-15:30 Location: Concordia, Room LB 921-04 Abstract: In the United States the preferred method of obtaining dietary intake data is the 24-hour dietary recall, yet the measure of most interest is usual or long-term average daily intake, which is impossible to measure. Thus, usual dietary intake is assessed with considerable measurement error. Also, diet represents numerous foods, nutrients and other components, each of which have distinctive attributes. Sometimes, it is useful to examine intake of these components separately, but increasingly nutritionists are interested in exploring them collectively to capture overall dietary patterns and their effect on various diseases.</description>
    </item>
    
    <item>
      <title>A nonparametric Bayesian model for local clustering</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-11-23/</link>
      <pubDate>Fri, 23 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2012fall/2012-11-23/</guid>
      <description>Date: 2012-11-23 Time: 14:30-15:30 Location: BURN 107 Abstract: We propose a nonparametric Bayesian local clustering (NoB-LoC) approach for heterogeneous data. Using genomics data as an example, the NoB-LoC clusters genes into gene sets and simultaneously creates multiple partitions of samples, one for each gene set. In other words, the sample partitions are nested within the gene sets. Inference is guided by a joint probability model on all random elements. Biologically, the model formalizes the notion that biological samples cluster differently with respect to different genetic processes, and that each process is related to only a small subset of genes.</description>
    </item>
    
    <item>
      <title>Observational studies in healthcare: are they any good?</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-10-19/</link>
      <pubDate>Fri, 19 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2012fall/2012-10-19/</guid>
      <description>Date: 2012-10-19 Time: 14:30-15:30 Location: UdeM Abstract: Observational healthcare data, such as administrative claims and electronic health records, play an increasingly prominent role in healthcare. Pharmacoepidemiologic studies in particular routinely estimate temporal associations between medical product exposure and subsequent health outcomes of interest, and such studies influence prescribing patterns and healthcare policy more generally. Some authors have questioned the reliability and accuracy of such studies, but few previous efforts have attempted to measure their performance.</description>
    </item>
    
    <item>
      <title>Regularized semiparametric functional linear regression</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-09-21/</link>
      <pubDate>Fri, 21 Sep 2012 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2012fall/2012-09-21/</guid>
      <description>Date: 2012-09-21 Time: 14:30-15:30 Location: McGill, Burnside Hall 1214 Abstract: In many scientific experiments we need to face analysis with functional data, where the observations are sampled from random process, together with a potentially large number of non-functional covariates. The complex nature of functional data makes it difficult to directly apply existing methods to model selection and estimation. We propose and study a new class of penalized semiparametric functional linear regression to characterize the regression relation between a scalar response and multiple covariates, including both functional covariates and scalar covariates.</description>
    </item>
    
    <item>
      <title>Li: High-dimensional feature selection using hierarchical Bayesian logistic regression with heavy-tailed priors | Rao: Best predictive estimation for linear mixed models with applications to small area estimation</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-04-13/</link>
      <pubDate>Fri, 13 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2012winter/2012-04-13/</guid>
      <description>Date: 2012-04-13 Time: 14:00-16:30 Location: MAASS 217 Abstract: Li: The problem of selecting the most useful features from a great many (eg, thousands) of candidates arises in many areas of modern sciences. An interesting problem from genomic research is that, from thousands of genes that are active (expressed) in certain tissue cells, we want to ﬁnd the genes that can be used to separate tissues of diﬀerent classes (eg. cancer and normal).</description>
    </item>
    
    <item>
      <title>Using tests of homoscedasticity to test missing completely at random | Hugh Chipman: Sequential optimization of a computer model and other Active Learning problems</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-03-09/</link>
      <pubDate>Fri, 09 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2012winter/2012-03-09/</guid>
      <description>Date: 2012-03-09 Time: 14:00-16:30 Location: UQAM, 201 ave. du Président-Kennedy, salle 5115 Abstract: Li: The problem of selecting the most useful features from a great many (eg, thousands) of candidates arises in many areas of modern sciences. An interesting problem from genomic research is that, from thousands of genes that are active (expressed) in certain tissue cells, we want to ﬁnd the genes that can be used to separate tissues of diﬀerent classes (eg.</description>
    </item>
    
    <item>
      <title>Stute: Principal component analysis of the Poisson Process | Blath: Longterm properties of the symbiotic branching model</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-02-10/</link>
      <pubDate>Fri, 10 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2012winter/2012-02-10/</guid>
      <description>Date: 2012-02-10 Time: 14:00-16:30 Location: Concordia Abstract: Stute: The Poisson Process constitutes a well-known model for describing random events over time. It has many applications in marketing research, insurance mathematics and finance. Though it has been studied for decades not much is known how to check (in a non-asymptotic way) the validity of the Poisson Process. In this talk we present the principal component decomposition of the Poisson Process which enables us to derive finite sample properties of associated goodness-of-fit tests.</description>
    </item>
    
    <item>
      <title>Bayesian approaches to evidence synthesis in clinical practice guideline development</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-01-13/</link>
      <pubDate>Fri, 13 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2012winter/2012-01-13/</guid>
      <description>Date: 2012-01-13 Time: 15:30-16:30 Location: Concordia, Library Building LB-921.04 Abstract: The American College of Cardiology Foundation (ACCF) and the American Heart Association (AHA) have jointly engaged in the production of guideline in the area of cardiovascular disease since 1980. The developed guidelines are intended to assist health care providers in clinical decision making by describing a range of generally acceptable approaches for the diagnosis, management, or prevention of specific diseases or conditions.</description>
    </item>
    
    <item>
      <title>Detecting evolution in experimental ecology: Diagnostics for missing state variables</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-12-09/</link>
      <pubDate>Fri, 09 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2011fall/2011-12-09/</guid>
      <description>Date: 2011-12-09 Time: 15:30-16:30 Location: UQAM Salle 5115 Abstract: This talk considers goodness of fit diagnostics for time-series data from processes approximately modeled by systems of nonlinear ordinary differential equations. In particular, we seek to determine three nested causes of lack of fit: (i) unmodeled stochastic forcing, (ii) mis-specified functional forms and (iii) mis-specified state variables. Testing lack of fit in differential equations is challenging since the model is expressed in terms of rates of change of the measured variables.</description>
    </item>
    
    <item>
      <title>Guérin: An ergodic variant of the telegraph process for a toy model of bacterial chemotaxis | Staicu: Skewed functional processes and their applications</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-11-11/</link>
      <pubDate>Fri, 11 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2011fall/2011-11-11/</guid>
      <description>Date: 2011-11-11 Time: 14:00-16:30 Location: UdeM Abstract: Guérin: I will study the long time behavior of a variant of the classic telegraph process, with non-constant jump rates that induce a drift towards the origin. This process can be seen as a toy model for velocity-jump processes recently proposed as mathematical models of bacterial chemotaxis. I will give its invariant law and construct an explicit coupling for velocity and position, providing exponential ergodicity with moreover a quantitative control of the total variation distance to equilibrium at each time instant.</description>
    </item>
    
    <item>
      <title>Dupuis: Modeling non-stationary extremes: The case of heat waves | Davis: Estimating extremal dependence in time series via the extremogram</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-10-14/</link>
      <pubDate>Fri, 14 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2011fall/2011-10-14/</guid>
      <description>Date: 2011-10-14 Time: 14:00-16:30 Location: TROTTIER 1080 Abstract: Dupuis: Environmental processes are often non-stationary since climate patterns cause systematic seasonal effects and long-term climate changes cause trends. The usual limit models are not applicable for non-stationary processes, but models from standard extreme value theory can be used along with statistical modeling to provide useful inference. Traditional approaches include letting model parameters be a function of covariates or using time-varying thresholds. These approaches are inadequate for the study of heat waves however and we show how a recent pre-processing approach by Eastoe and Tawn (2009) can be used in conjunction with an innovative change-point analysis to model daily maximum temperature.</description>
    </item>
    
    <item>
      <title>Susko: Properties of Bayesian posteriors and bootstrap support in phylogenetic inference | Labbe: An integrated hierarchical Bayesian model for multivariate eQTL genetic mapping</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-09-09/</link>
      <pubDate>Fri, 09 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>https://mcgillstat.github.io/post/2011fall/2011-09-09/</guid>
      <description>Date: 2011-09-09 Time: 14:00-16:30 Location: UdeM, Pav. André-Aisenstadt, SALLE 1360 Abstract: Susko: The data generated by large scale sequencing projects is complex, high-dimensional, multivariate discrete data. In studies of evolutionary biology, the parameter space of evolutionary trees is an unusual additional complication from a statistical perspective. In this talk I will briefly introduce the general approaches to utilizing sequence data in phylogenetic inference. A particular issue of interest in phylogenetic inference is assessments of uncertainty about the true tree or structures that might be present in it.</description>
    </item>
    
  </channel>
</rss>
