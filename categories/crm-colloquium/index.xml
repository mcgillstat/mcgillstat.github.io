<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CRM-Colloquium on McGill Statistics Seminars</title>
    <link>https://mcgillstat.github.io/categories/crm-colloquium/</link>
    <description>Recent content in CRM-Colloquium on McGill Statistics Seminars</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Feb 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://mcgillstat.github.io/categories/crm-colloquium/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Structure learning for extremal graphical models</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-02-18/</link>
      <pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022winter/2022-02-18/</guid>
      <description>&lt;h4 id=&#34;date-2022-02-18&#34;&gt;Date: 2022-02-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsumontrealzoomusj85105423917pwdenm3mgpfnkzku2damjritmo0n0juut09httpsumontrealzoomusj85105423917pwdenm3mgpfnkzku2damjritmo0n0juut09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/85105423917?pwd=enM3MGpFNkZKU2daMjRITmo0N0JUUT09&#34;&gt;https://umontreal.zoom.us/j/85105423917?pwd=enM3MGpFNkZKU2daMjRITmo0N0JUUT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-851-0542-3917&#34;&gt;Meeting ID: 851 0542 3917&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-403790&#34;&gt;Passcode: 403790&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Extremal graphical models are sparse statistical models for multivariate extreme events.  The underlying graph encodes conditional independencies and enables a visual interpretation of the complex extremal dependence structure.  For the important case of tree models, we provide a data-driven methodology for learning the graphical structure.  We show that sample versions of the extremal correlation and a new summary statistic, which we call the extremal variogram, can be used as weights for a minimum spanning tree to consistently recover the true underlying tree.  Remarkably, this implies that extremal tree models can be learned in a completely non-parametric fashion by using simple summary statistics and without the need to assume discrete distributions, existence of densities, or parametric models for marginal or bivariate distributions.  Extensions to more general graphs are also discussed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Risk assessment, heavy tails, and asymmetric least squares techniques</title>
      <link>https://mcgillstat.github.io/post/2022winter/2022-01-28/</link>
      <pubDate>Fri, 28 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2022winter/2022-01-28/</guid>
      <description>&lt;h4 id=&#34;date-2022-01-28&#34;&gt;Date: 2022-01-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsumontrealzoomusj93983313215pwdclb6cunssjavrmfmme1pblhktutsqt09httpsumontrealzoomusj93983313215pwdclb6cunssjavrmfmme1pblhktutsqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09&#34;&gt;https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-8331-3215&#34;&gt;Meeting ID: 939 8331 3215&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-096952&#34;&gt;Passcode: 096952&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statistical risk assessment, in particular in finance and insurance, requires estimating simple indicators to summarize the risk incurred in a given situation.  Of most interest is to infer extreme levels of risk so as to be able to manage high-impact rare events such as extreme climate episodes or stock market crashes.  A standard procedure in this context, whether in the academic, industrial or regulatory circles, is to estimate a well-chosen single quantile (or Value-at-Risk).  One drawback of quantiles is that they only take into account the frequency of an extreme event, and in particular do not give an idea of what the typical magnitude of such an event would be.  Another issue is that they do not induce a coherent risk measure, which is a serious concern in actuarial and financial applications. In this talk, after giving a leisurely tour of extreme quantile estimation, I will explain how, starting from the formulation of a quantile as the solution of an optimization problem, one may come up with two alternative families of risk measures, called expectiles and extremiles, in order to address these two drawbacks.  I will give a broad overview of their properties, as well as of their estimation at extreme levels in heavy-tailed models, and explain why they constitute sensible alternatives for risk assessment using real data applications.  This is based on joint work with Abdelaati Daouia, Irène Gijbels, Stéphane Girard, Simone Padoan and Antoine Usseglio-Carleve.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adventures with Partial Identifications in Studies of Marked Individuals</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-11-26/</link>
      <pubDate>Fri, 26 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-11-26/</guid>
      <description>&lt;h4 id=&#34;date-2021-11-26&#34;&gt;Date: 2021-11-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsumontrealzoomusj93983313215pwdclb6cunssjavrmfmme1pblhktutsqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-8331-3215&#34;&gt;Meeting ID: 939 8331 3215&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-096952&#34;&gt;Passcode: 096952&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Monitoring marked individuals is a common strategy in studies of wild animals (referred to as mark-recapture or capture-recapture experiments) and hard to track human populations (referred to as multi-list methods or multiple-systems estimation).  A standard assumption of these techniques is that individuals can be identified uniquely and without error, but this can be violated in many ways.  In some cases, it may not be possible to identify individuals uniquely because of the study design or the choice of marks.  Other times, errors may occur so that individuals are incorrectly identified.  I will discuss work with my collaborators over the past 10 ye ars developing methods to account for problems that arise when are only individuals are only partially identified.  I will present theoretical aspects of this research, including an introduction to the latent multinomial model and algebraic statistics, and also describe applications to studies of species ranging from the golden mantella (an endangered frog endemic to Madagascar measuring only 20 mm) to the whale shark (the largest known species of sh measuring up to 19 m).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Opinionated practices for teaching reproducibility: motivation, guided instruction and practice</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-10-29/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-10-29/</guid>
      <description>&lt;h4 id=&#34;date-2021-10-29&#34;&gt;Date: 2021-10-29&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsumontrealzoomusj93983313215pwdclb6cunssjavrmfmme1pblhktutsqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-8331-3215&#34;&gt;Meeting ID: 939 8331 3215&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-096952&#34;&gt;Passcode: 096952&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the data science courses at the University of British Columbia, we define data science as the study, development and practice of reproducible and auditable processes to obtain insight from data. While reproducibility is core to our definition, most data science learners enter the field with other aspects of data science in mind, for example predictive modelling, which is often one of the most interesting topic to novices. This fact, along with the highly technical nature of the industry standard reproducibility tools currently employed in data science, present out-ofthe gate challenges in teaching reproducibility in the data science classroom. Put simply, students are not as intrinsically motivated to learn this topic, and it is not an easy one for them to learn. What can a data science educator do? Over several iterations of teaching courses focused on reproducible data science tools and workflows, we have found that providing extra motivation, guided instruction and lots of practice are key to effectively teaching this challenging, yet important subject. Here we present examples of how we deeply motivate, effectively guide and provide ample practice opportunities to data science students to effectively engage them in learning about this topic.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep down, everyone wants to be causal</title>
      <link>https://mcgillstat.github.io/post/2021fall/2021-09-24/</link>
      <pubDate>Fri, 24 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021fall/2021-09-24/</guid>
      <description>&lt;h4 id=&#34;date-2021-09-24&#34;&gt;Date: 2021-09-24&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1500-1600-montreal-time&#34;&gt;Time: 15:00-16:00 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;httpsmcgillzoomusj9791073141httpsmcgillzoomusj9791073141&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/9791073141&#34;&gt;https://mcgill.zoom.us/j/9791073141&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-979-107-3141&#34;&gt;Meeting ID: 979 107 3141&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the data science courses at the University of British Columbia, we define data science as the study, development and practice of reproducible and auditable processes to obtain insight from data. While reproducibility is core to our definition, most data science learners enter the field with other aspects of data science in mind, for example predictive modelling, which is often one of the most interesting topic to novices. This fact, along with the highly technical nature of the industry standard reproducibility tools currently employed in data science, present out-ofthe gate challenges in teaching reproducibility in the data science classroom. Put simply, students are not as intrinsically motivated to learn this topic, and it is not an easy one for them to learn. What can a data science educator do? Over several iterations of teaching courses focused on reproducible data science tools and workflows, we have found that providing extra motivation, guided instruction and lots of practice are key to effectively teaching this challenging, yet important subject. Here we present examples of how we deeply motivate, effectively guide and provide ample practice opportunities to data science students to effectively engage them in learning about this topic.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Nonparametric Tests for Informative Selection in Complex Surveys</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-03-12/</link>
      <pubDate>Fri, 12 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-03-12/</guid>
      <description>&lt;h4 id=&#34;date-2021-03-12&#34;&gt;Date: 2021-03-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsumontrealzoomusj93983313215pwdclb6cunssjavrmfmme1pblhktutsqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-8331-3215&#34;&gt;Meeting ID: 939 8331 3215&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-096952&#34;&gt;Passcode: 096952&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Informative selection, in which the distribution of response variables given that they are sampled is different from their distribution in the population, is pervasive in complex surveys.  Failing to take such informativeness into account can produce severe inferential errors, including biased and inconsistent estimation of population parameters.  While several parametric procedures exist to test for informative selection, these methods are limited in scope and their parametric assumptions are difficult to assess.  We consider two classes of nonparametric tests of informative selection.  The first class is motivated by classic nonparametric two-sample tests.  We compare weighted and unweighted empirical distribution functions and obtain tests for informative selection that are analogous to Kolmogorov-Smirnov and Cramer-von Mises.  For the second class of tests, we adapt a kernel-based learning method that compares distributions based on their maximum mean discrepancy.  The asymptotic distributions of the test statistics are established under the null hypothesis of noninformative selection.  Simulation results show that our tests have power competitive with existing parametric tests in a correctly specified parametric setting, and better than those tests under model misspecification.  A recreational angling application illustrates the methodology.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spatio-temporal methods for estimating subsurface ocean thermal response to tropical cyclones</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-02-12/</link>
      <pubDate>Fri, 12 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-02-12/</guid>
      <description>&lt;h4 id=&#34;date-2021-02-12&#34;&gt;Date: 2021-02-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsumontrealzoomusj93983313215pwdclb6cunssjavrmfmme1pblhktutsqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-8331-3215&#34;&gt;Meeting ID: 939 8331 3215&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-096952&#34;&gt;Passcode: 096952&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Tropical cyclones (TCs), driven by heat exchange between the air and sea, pose a substantial risk to many communities around the world.  Accurate characterization of the subsurface ocean thermal response to TC passage is crucial for accurate TC intensity forecasts and for understanding the role TCs play in the global climate system, yet that characterization is complicated by the high-noise ocean environment, correlations inherent in spatio-temporal data, relative scarcity of in situ observations and the entanglement of the TC-induced signal with seasonal signals.  We present a general methodological framework that addresses these difficulties, integrating existing techniques in seasonal mean field estimation, Gaussian process modeling, and nonparametric regression into a functional ANOVA model.  Importantly, we improve upon past work by properly handling seasonality, providing rigorous uncertainty quantification, and treating time as a continuous variable, rather than producing estimates that are binned in time.  This functional ANOVA model is estimated using in situ subsurface temperature profiles from the Argo fleet of autonomous floats through a multi-step procedure, which (1) characterizes the upper ocean seasonal shift during the TC season; (2) models the variability in the temperature observations; (3) fits a thin plate spline using the variability estimates to account for heteroskedasticity and correlation between the observations.  This spline fit reveals the ocean thermal response to TC passage.  Through this framework, we obtain new scientific insights into the interaction between TCs and the ocean on a global scale, including a three-dimensional characterization of the near-surface and subsurface cooling along the TC storm track and the mixing-induced subsurface warming on the track&amp;rsquo;s right side.  Joint work with Addison Hu, Ann Lee, Donata Giglio and Kimberly Wood.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Small Area Estimation in Low- and Middle-Income Countries</title>
      <link>https://mcgillstat.github.io/post/2021winter/2021-01-29/</link>
      <pubDate>Fri, 29 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2021winter/2021-01-29/</guid>
      <description>&lt;h4 id=&#34;date-2021-01-29&#34;&gt;Date: 2021-01-29&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsumontrealzoomusj93983313215pwdclb6cunssjavrmfmme1pblhktutsqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-8331-3215&#34;&gt;Meeting ID: 939 8331 3215&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-096952&#34;&gt;Passcode: 096952&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The under-five mortality rate (U5MR) is a key barometer of the health of a nation. Unfortunately, many people living in low- and middle-income countries are not covered by civil registration systems. This makes estimation of the U5MR, particularly at the subnational level, difficult. In this talk, I will describe models that have been developed to produce the official United Nations (UN) subnational U5MR estimates in 22 countries. Estimation is based on household surveys, which use stratified, two-stage cluster sampling. I will describe a range of area- and unit-level models and describe the rationale for the modeling we carry out. Data sparsity in time and space is a key challenge, and smoothing models are vital. I will discuss the advantages and disadvantages of discrete and continuous spatial models, in the context of estimation at the scale at which health interventions are made. Other issues that will be touched upon include: design-based versus model-based inference; adjustments for HIV epidemics; the inclusion of so-called indirect (summary birth history) data; reproducibility through software availability; benchmarking; how to deal with incomplete geographical data; and working with the UN to produce estimates.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Approximate Cross-Validation for Large Data and High Dimensions</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-11-13/</link>
      <pubDate>Fri, 13 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-11-13/</guid>
      <description>&lt;h4 id=&#34;date-2020-11-13&#34;&gt;Date: 2020-11-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpcrmumontrealcacolloque-sciences-mathematiques-quebeccsmq&#34;&gt;&lt;a href=&#34;http://crm.umontreal.ca/colloque-sciences-mathematiques-quebec/#csmq&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The error or variability of statistical and machine learning algorithms&#xA;is often assessed by repeatedly re-fitting a model with different&#xA;weighted versions of the observed data.  The ubiquitous tools of&#xA;cross-validation (CV) and the bootstrap are examples of this technique.&#xA;These methods are powerful in large part due to their model agnosticism&#xA;but can be slow to run on modern, large data sets due to the need to&#xA;repeatedly re-fit the model.  We use a linear approximation to the&#xA;dependence of the fitting procedure on the weights, producing results&#xA;that can be faster than repeated re-fitting by orders of magnitude.&#xA;This linear approximation is sometimes known as the &amp;ldquo;infinitesimal&#xA;jackknife&amp;rdquo; (IJ) in the statistics literature, where it has mostly been&#xA;used as a theoretical tool to prove asymptotic results.  We provide&#xA;explicit finite-sample error bounds for the infinitesimal jackknife in&#xA;terms of a small number of simple, verifiable assumptions.  Without&#xA;further modification, though, we note that the IJ deteriorates in&#xA;accuracy in high dimensions and incurs a running time roughly cubic in&#xA;dimension.  We additionally show, then, how dimensionality reduction can&#xA;be used to successfully run the IJ in high dimensions when data is&#xA;sparse or low rank.  Simulated and real-data experiments support our&#xA;theory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Data Science, Classification, Clustering and Three-Way Data</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-10-02/</link>
      <pubDate>Fri, 02 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-10-02/</guid>
      <description>&lt;h4 id=&#34;date-2020-10-02&#34;&gt;Date: 2020-10-02&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsumontrealzoomusj93983313215pwdclb6cunssjavrmfmme1pblhktutsqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/93983313215?pwd=clB6cUNsSjAvRmFMME1PblhkTUtsQT09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-939-8331-3215&#34;&gt;Meeting ID: 939 8331 3215&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-096952&#34;&gt;Passcode: 096952&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Data science is discussed along with some historical perspective.  Selected problems in classification are considered, either via specific datasets or general problem types.  In each case, the problem is introduced before one or more potential solutions are discussed and applied.  The problems discussed include data with outliers, longitudinal data, and three-way data.  The proposed approaches are generally mixture model-based.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Machine Learning for Causal Inference</title>
      <link>https://mcgillstat.github.io/post/2020fall/2020-09-11/</link>
      <pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020fall/2020-09-11/</guid>
      <description>&lt;h4 id=&#34;date-2020-09-11&#34;&gt;Date: 2020-09-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1600-1700&#34;&gt;Time: 16:00-17:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;zoom-linkhttpsumontrealzoomusj96525367383pwddzburjbvc2fwtgpyruh4aurbz0rvqt09&#34;&gt;&lt;a href=&#34;https://umontreal.zoom.us/j/96525367383?pwd=dzBURjBvc2FWTGpyRUh4aURBZ0RvQT09&#34;&gt;Zoom Link&lt;/a&gt;&lt;/h4&gt;&#xA;&lt;h4 id=&#34;meeting-id-965-2536-7383&#34;&gt;Meeting ID: 965 2536 7383&lt;/h4&gt;&#xA;&lt;h4 id=&#34;passcode-421254&#34;&gt;Passcode: 421254&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Given advances in machine learning over the past decades, it is now possible to accurately solve difficult non-parametric prediction problems in a way that is routine and reproducible. In this talk, I’ll discuss how machine learning tools can be rigorously integrated into observational study analyses, and how they interact with classical statistical ideas around randomization, semiparametric modeling, double robustness, etc. I’ll also survey some recent advances in methods for treatment heterogeneity. When deployed carefully, machine learning enables us to develop causal estimators that reflect an observational study design more closely than basic linear regression based methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Neyman-Pearson classification: parametrics and sample size requirement</title>
      <link>https://mcgillstat.github.io/post/2020winter/2020-02-28/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2020winter/2020-02-28/</guid>
      <description>&lt;h4 id=&#34;date-2020-02-28&#34;&gt;Date: 2020-02-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burnside-1104&#34;&gt;Location: BURNSIDE 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The Neyman-Pearson (NP) paradigm in binary classification seeks classifiers that achieve a minimal type II error while enforcing the prioritized type I error controlled under some user-specified level alpha. This paradigm serves naturally in applications such as severe disease diagnosis and spam detection, where people have clear priorities among the two error types. Recently, Tong, Feng and Li (2018) proposed a nonparametric umbrella algorithm that adapts all scoring-type classification methods (e.g., logistic regression, support vector machines, random forest) to respect the given type I error (i.e., conditional probability of classifying a class 0 observation as class 1 under the 0-1 coding) upper bound alpha with high probability, without specific distributional assumptions on the features and the responses. Universal the umbrella algorithm is, it demands an explicit minimum sample size requirement on class 0, which is often the more scarce class, such as in rare disease diagnosis applications. In this work, we employ the parametric linear discriminant analysis (LDA) model and propose a new parametric thresholding algorithm, which does not need the minimum sample size requirements on class 0 observations and thus is suitable for small sample applications such as rare disease diagnosis. Leveraging both the existing nonparametric and the newly proposed parametric thresholding rules, we propose four LDA-based NP classifiers, for both low- and high-dimensional settings. On the theoretical front, we prove NP oracle inequalities for one proposed classifier, where the rate for excess type II error benefits from the explicit parametric model assumption. Furthermore, as NP classifiers involve a sample splitting step of class 0 observations,  we construct a new adaptive sample splitting scheme that can be applied universally to NP classifiers, and this adaptive strategy reduces the type II error of these classifiers. The proposed NP classifiers are implemented in the R package nproc.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Formulation and solution of stochastic inverse problems for science and engineering models</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-11-22/</link>
      <pubDate>Fri, 22 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-11-22/</guid>
      <description>&lt;h4 id=&#34;date-2019-11-22&#34;&gt;Date: 2019-11-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1600-1700&#34;&gt;Time: 16:00-17:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-pavillon-kennedy-pk-5115-uqam&#34;&gt;Location: Pavillon Kennedy, PK-5115, UQAM&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The stochastic inverse problem of determining probability&#xA;structures on input parameters for a physics model corresponding to a&#xA;given probability structure on the output of the model forms the core of&#xA;scientific inference and engineering design. We describe a formulation&#xA;and solution method for stochastic inverse problems that is based on&#xA;functional analysis, differential geometry, and probability/measure&#xA;theory. This approach yields a computationally tractable problem while&#xA;avoiding alterations of the model like regularization and ad hoc&#xA;assumptions about the probability structures. We present several&#xA;examples, including a high-dimensional application to determination of&#xA;parameter fields in storm surge models. We also describe work aimed at&#xA;defining a notion of condition for stochastic inverse problems and&#xA;tackling the related problem of designing sets of optimal observable&#xA;quantities.&lt;/p&gt;</description>
    </item>
    <item>
      <title>General Bayesian Modeling</title>
      <link>https://mcgillstat.github.io/post/2019fall/2019-11-01/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019fall/2019-11-01/</guid>
      <description>&lt;h4 id=&#34;date-2019-11-01&#34;&gt;Date: 2019-11-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1600-1700&#34;&gt;Time: 16:00-17:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1104&#34;&gt;Location: BURN 1104&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The work is motivated by the inflexibility of Bayesian modeling; in that only parameters of&#xA;probability models are required to be connected with data. The idea is to generalize this by&#xA;allowing arbitrary unknowns to be connected with data via loss functions. An updating process&#xA;is then detailed which can be viewed as arising in at least a couple of ways - one being purely&#xA;axiomatically driven.&#xA;The further exploration of replacing probability model based approaches to inference with loss&#xA;functions is ongoing. Joint work with Chris Holmes, Pier Giovanni Bissiri and Simon Lyddon.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Network models, sampling, and symmetry properties</title>
      <link>https://mcgillstat.github.io/post/2019winter/2019-02-01/</link>
      <pubDate>Fri, 01 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2019winter/2019-02-01/</guid>
      <description>&lt;h4 id=&#34;date-2019-02-01&#34;&gt;Date: 2019-02-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;A recent body of work, by myself and many others, aims to develop a statistical theory of network data for problems a single network is observed. Of the models studied in this area, graphon models are probably most widely known in statistics. I will explain the relationship between three aspects of this work: (1) Specific models, such as graphon models, graphex models, and edge-exchangeable graphs. (2) Sampling theory for networks, specifically in the case statisticians might refer to as an infinite-population limit. (3) Invariance properties, especially various forms of exchangeability. I will also present recent results that show how statistically relevant results (such as central limit theorems) can be derived from such invariance properties.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Law of Large Populations: The return of the long-ignored N and how it can affect our 2020 vision</title>
      <link>https://mcgillstat.github.io/post/2018winter/2018-02-16/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2018winter/2018-02-16/</guid>
      <description>&lt;h4 id=&#34;date-2018-02-16&#34;&gt;Date: 2018-02-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-mcgill-university-otto-maass-217&#34;&gt;Location: McGill University, OTTO MAASS 217&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;For over a century now, we statisticians have successfully convinced ourselves and almost everyone else, that in statistical inference the size of the population N can be ignored, especially when it is large.  Instead, we focused on the size of the sample, n, the key driving force for both the Law of Large Numbers and the Central Limit Theorem. We were thus taught that the statistical error (standard error) goes down with n typically at the rate of 1/√n.   However, all these rely on the presumption that our data have perfect quality, in the sense of being equivalent to a probabilistic sample.  A largely overlooked statistical identity, a potential counterpart to the Euler identity in mathematics, reveals a Law of Large Populations (LLP), a law that we should be all afraid of. That is, once we lose control over data quality, the systematic error (bias) in the usual estimators, relative to the benchmarking standard error from simple random sampling, goes up with N at the rate of √N.   The coefficient in front of √N can be viewed as a data defect index, which is the simple Pearson correlation between the reporting/recording indicator and the value reported/recorded.  Because of the multiplier√N, a seemingly tiny correlation, say, 0.005, can have detrimental effect on the quality of inference.  Without understanding of this LLP,  “big data” can do more harm than good because of the drastically inflated precision assessment hence a gross overconfidence, setting us up to be caught by surprise when the reality unfolds, as we all experienced during the 2016 US presidential election. Data from Cooperative Congressional Election Study (CCES, conducted by Stephen Ansolabehere, Douglas River and others, and analyzed by Shiro Kuriwaki),   are used to estimate the data defect index for the 2016 US election, with the aim to gain a clearer vision for the 2020 election and beyond.&lt;/p&gt;</description>
    </item>
    <item>
      <title>150 years (and more) of data analysis in Canada</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-11-24/</link>
      <pubDate>Fri, 24 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-11-24/</guid>
      <description>&lt;h4 id=&#34;date-2017-11-24&#34;&gt;Date: 2017-11-24&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-lea-232&#34;&gt;Location: LEA 232&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;As Canada celebrates its 150th anniversary, it may be good to reflect on the past and future of data analysis and statistics in this country. In this talk, I will review the Victorian Statistics Movement and its effect in Canada, data analysis by a Montréal physician in the 1850s, a controversy over data analysis in the 1850s and 60s centred in Montréal, John A. MacDonald’s use of statistics, the Canadian insurance industry and the use of statistics, the beginning of mathematical statistics in Canada, the Fisherian revolution, the influence of Fisher, Neyman and Pearson, the computer revolution, and the emergence of data science.&lt;/p&gt;</description>
    </item>
    <item>
      <title>McNeil: Spectral backtests of forecast distributions with application to risk management | Jasiulis-Goldyn: Asymptotic properties and renewal theory for Kendall random walks</title>
      <link>https://mcgillstat.github.io/post/2017fall/2017-09-29/</link>
      <pubDate>Fri, 29 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017fall/2017-09-29/</guid>
      <description>&lt;h4 id=&#34;date-2017-09-29&#34;&gt;Date: 2017-09-29&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1630&#34;&gt;Time: 14:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;McNeil&lt;/em&gt;: In this talk we study a class of backtests for forecast distributions in which the test statistic is a spectral transformation that weights exceedance events by a function of the modelled probability level. The choice of the kernel function makes explicit the user’s priorities for model performance. The class of spectral backtests includes tests of unconditional coverage and tests of conditional coverage. We show how the class embeds a wide variety of backtests in the existing literature, and propose novel variants as well. We assess the size and power of the backtests in realistic sample sizes, and in particular demonstrate the tradeoff between power and specificity in validating quantile forecasts.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Instrumental Variable Regression with Survival Outcomes</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-04-06/</link>
      <pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-04-06/</guid>
      <description>&lt;h4 id=&#34;date-2017-04-06&#34;&gt;Date: 2017-04-06&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-universite-laval-pavillon-vachon-salle-3840&#34;&gt;Location: Universite Laval, Pavillon Vachon, Salle 3840&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Instrumental variable (IV) methods are popular in non-experimental studies to estimate the causal effects of medical interventions or exposures. These approaches allow for the consistent estimation of such effects even if important confounding factors are unobserved. Despite the increasing use of these methods, there have been few extensions of IV methods to censored data regression problems. We discuss challenges in applying IV structural equational modelling techniques to the proportional hazards model and suggest alternative modelling frameworks. We demonstrate the utility of the accelerated lifetime and additive hazards models for IV analyses with censored data. Assuming linear structural equation models for either the event time or the hazard function, we proposed closed-form, two-stage estimators for the causal effect in the structural models for the failure time outcomes. The asymptotic properties of the estimators are derived and the resulting inferences are shown to perform well in simulation studies and in an application to a data set on the effectiveness of a novel chemotherapeutic agent for colon cancer.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Inference in dynamical systems</title>
      <link>https://mcgillstat.github.io/post/2017winter/2017-03-17/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2017winter/2017-03-17/</guid>
      <description>&lt;h4 id=&#34;date-2017-03-17&#34;&gt;Date: 2017-03-17&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider the asymptotic consistency of maximum likelihood parameter estimation for dynamical systems observed with noise. Under suitable conditions on the dynamical systems and the observations, we show that maximum likelihood parameter estimation is consistent. Furthermore, we show how some well-studied properties of dynamical systems imply the general statistical properties related to maximum likelihood estimation. Finally, we exhibit classical families of dynamical systems for which maximum likelihood estimation is consistent. Examples include shifts of finite type with Gibbs measures and Axiom A attractors with SRB measures. We also relate Bayesian inference to the thermodynamic formalism in tracking dynamical systems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>High-dimensional changepoint estimation via sparse projection</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-12-01/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-12-01/</guid>
      <description>&lt;h4 id=&#34;date-2016-12-01&#34;&gt;Date: 2016-12-01&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-708&#34;&gt;Location: BURN 708&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Changepoints are a very common feature of Big Data that arrive in the form of a data stream. We study high-dimensional time series in which, at certain time points, the mean structure changes in a sparse subset of the coordinates. The challenge is to borrow strength across the coordinates in order to detect smaller changes than could be observed in any individual component series. We propose a two-stage procedure called &amp;lsquo;inspect&amp;rsquo; for estimation of the changepoints: first, we argue that a good projection direction can be obtained as the leading left singular vector of the matrix that solves a convex optimisation problem derived from the CUSUM transformation of the time series. We then apply an existing univariate changepoint detection algorithm to the projected series. Our theory provides strong guarantees on both the number of estimated changepoints and the rates of convergence of their locations, and our numerical studies validate its highly competitive empirical performance for a wide range of data generating mechanisms.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Efficient tests of covariate effects in two-phase failure time studies</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-10-28/</link>
      <pubDate>Fri, 28 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-10-28/</guid>
      <description>&lt;h4 id=&#34;date-2016-10-28&#34;&gt;Date: 2016-10-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-1205&#34;&gt;Location: BURN 1205&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Two-phase studies are frequently used when observations on certain variables are expensive or difficult to obtain. One such situation is when a cohort exists for which certain variables have been measured (phase 1 data); then, a sub-sample of individuals is selected, and additional data are collected on them (phase 2). Efficiency for tests and estimators can be increased by basing the selection of phase 2 individuals on data collected at phase 1. For example, in large cohorts, expensive genomic measurements are often collected at phase 2, with oversampling of persons with “extreme” phenotypic responses. A second example is case-cohort or nested case-control studies involving times to rare events, where phase 2 oversamples persons who have experienced the event by a certain time. In this talk I will describe two-phase studies on failure times, present efficient methods for testing covariate effects. Some extensions to more complex outcomes and areas needing further development will be discussed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Statistical inference for fractional diffusion processes</title>
      <link>https://mcgillstat.github.io/post/2016fall/2016-09-16/</link>
      <pubDate>Fri, 16 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016fall/2016-09-16/</guid>
      <description>&lt;h4 id=&#34;date-2016-09-16&#34;&gt;Date: 2016-09-16&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1600-1700&#34;&gt;Time: 16:00-17:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-lb-92104-library-building-concordia-univ&#34;&gt;Location: LB-921.04, Library Building, Concordia Univ.&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;There are some time series which exhibit long-range dependence as noticed by Hurst in his investigations of river water levels along Nile river. Long-range dependence is connected with the concept of self-similarity in that increments of a self-similar process with stationary increments exhibit long-range dependence under some conditions. Fractional Brownian motion is an example of such a process. We discuss statistical inference for stochastic processes modeled by stochastic differential equations driven by a fractional Brownian motion. These processes are termed as fractional diffusion processes. Since fractional Brownian motion is not a semimartingale, it is not possible to extend the notion of a stochastic integral with respect to a fractional Brownian motion following the ideas of Ito integration. There are other methods of extending integration with respect to a fractional Brownian motion. Suppose a complete path of a fractional diffusion process is observed over a finite time interval. We will present some results on inference problems for such processes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ridges and valleys in the high excursion sets of Gaussian random fields</title>
      <link>https://mcgillstat.github.io/post/2016winter/2016-03-10/</link>
      <pubDate>Thu, 10 Mar 2016 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2016winter/2016-03-10/</guid>
      <description>&lt;h4 id=&#34;date-2016-03-10&#34;&gt;Date: 2016-03-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-maass-217-mcgill&#34;&gt;Location: MAASS 217, McGill&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;It is well known that normal random variables do not like taking large values.  Therefore, a continuous Gaussian random field on a compact set does not like exceeding a large level.  If it does exceed a large level at some point, it tends to go back below the level a short distance away from that point.  One, therefore, does not expect the excursion set above a high for such a field to possess any interesting structure.  Nonetheless, if we want to know how likely are two points in such an excursion set to be connected by a path (&amp;ldquo;a ridge&amp;rdquo;) in the excursion set, how do we figure that out? If we know that a ridge in the excursion set exists (e.g.  the field is above a high level on the surface of a sphere), how likely is there to be also a valley (e.g.  the field going to below a fraction of the level somewhere inside that sphere)?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Causal discovery with confidence using invariance principles</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-12-10/</link>
      <pubDate>Thu, 10 Dec 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-12-10/</guid>
      <description>&lt;h4 id=&#34;date-2015-12-10&#34;&gt;Date: 2015-12-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-udem-pav-roger-gaudry-salle-s-116&#34;&gt;Location: UdeM, Pav. Roger-Gaudry, salle S-116&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;What is interesting about causal inference? One of the most compelling aspects is that any prediction under a causal model is valid in environments that are possibly very different to the environment used for inference. For example, variables can be actively changed and predictions will still be valid and useful. This invariance is very useful but still leaves open the difficult question of inference. We propose to turn this invariance principle around and exploit the invariance for inference. If we observe a system in different environments (or under different but possibly not well specified interventions) we can identify all models that are invariant. We know that any causal model has to be in this subset of invariant models. This allows causal inference with valid confidence intervals. We propose different estimators, depending on the nature of the interventions and depending on whether hidden variables and feedbacks are present. Some empirical examples demonstrate the power and possible pitfalls of this approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Inference regarding within-family association in disease onset times under biased sampling schemes</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-11-26/</link>
      <pubDate>Thu, 26 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-11-26/</guid>
      <description>&lt;h4 id=&#34;date-2015-11-26&#34;&gt;Date: 2015-11-26&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-306&#34;&gt;Location: BURN 306&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In preliminary studies of the genetic basis for chronic conditions, interest routinely lies in the within-family dependence in disease status. When probands are selected from disease registries and their respective families are recruited, a variety of ascertainment bias-corrected methods of inference are available which are typically based on models for correlated binary data. This approach ignores the age that family members are at the time of assessment. We consider copula-based models for assessing the within-family dependence in the disease onset time and disease progression, based on right-censored and current status observation of the non-probands. Inferences based on likelihood, composite likelihood and estimating functions are each discussed and compared in terms of asymptotic and empirical  relative efficiency. This is joint work with Yujie Zhong.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A knockoff filter for controlling the false discovery rate</title>
      <link>https://mcgillstat.github.io/post/2015fall/2015-10-30/</link>
      <pubDate>Fri, 30 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015fall/2015-10-30/</guid>
      <description>&lt;h4 id=&#34;date-2015-10-30&#34;&gt;Date: 2015-10-30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1600-1700&#34;&gt;Time: 16:00-17:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-salle-1360-pavillon-andré-aisenstadt-université-de-montréa&#34;&gt;Location: Salle 1360, Pavillon André-Aisenstadt, Université de Montréa&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The big data era has created a new scientific paradigm: collect data first, ask questions later. Imagine that we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR) - the expected fraction of false discoveries among all discoveries - is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable. We introduce the knockoff filter, a new variable selection procedure controlling the FDR in the statistical linear model whenever there are at least as many observations as variables. This method works by constructing fake variables, knockoffs, which can then be used as controls for the true variables; the method achieves exact FDR control in finite-sample settings no matter the design or covariates, the number of variables in the model, and the amplitudes of the unknown regression coefficients, and does not require any knowledge of the noise level. This is joint work with Rina Foygel Barber.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A statistical view of some recent climate controversies</title>
      <link>https://mcgillstat.github.io/post/2015winter/2015-05-07/</link>
      <pubDate>Thu, 07 May 2015 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2015winter/2015-05-07/</guid>
      <description>&lt;h4 id=&#34;date-2015-05-07&#34;&gt;Date: 2015-05-07&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-université-de-sherbrooke&#34;&gt;Location: Université de Sherbrooke&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This talk looks at some recent climate controversies from a statistical standpoint. The issues are motivated via changepoints and their detection. Changepoints are ubiquitous features in climatic time series, occurring whenever stations relocate or gauges are changed. Ignoring changepoints can produce spurious trend conclusions. Changepoint tests involving cumulative sums, likelihood ratio, and maximums of F-statistics are introduced; the asymptotic distributions of these statistics are quantified under the changepoint-free null hypothesis. The case of multiple changepoints is considered. The methods are used to study several controversies, including extreme temperature trends in the United States and Atlantic Basin tropical cyclone counts and strengths.&lt;/p&gt;</description>
    </item>
    <item>
      <title>High-dimensional phenomena in mathematical statistics and convex analysis</title>
      <link>https://mcgillstat.github.io/post/2014fall/2014-11-20/</link>
      <pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014fall/2014-11-20/</guid>
      <description>&lt;h4 id=&#34;date-2014-11-20&#34;&gt;Date: 2014-11-20&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1600-1700&#34;&gt;Time: 16:00-17:00&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-crm-1360-u-de-montréal&#34;&gt;Location: CRM 1360 (U. de Montréal)&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statistical models in which the ambient dimension is of the same order or larger than the sample size arise frequently in different areas of science and engineering. Although high-dimensional models of this type date back to the work of Kolmogorov, they have been the subject of intensive study over the past decade, and have interesting connections to many branches of mathematics (including concentration of measure, random matrix theory, convex geometry, and information theory). In this talk, we provide a broad overview of the general area, including vignettes on phase transitions in high-dimensional graph recovery, and randomized approximations of convex programs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adaptive piecewise polynomial estimation via trend filtering</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-04-11/</link>
      <pubDate>Fri, 11 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-04-11/</guid>
      <description>&lt;h4 id=&#34;date-2014-04-11&#34;&gt;Date: 2014-04-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-salle-kpmg-1er-étage-hec-montréal&#34;&gt;Location: Salle KPMG, 1er étage HEC Montréal&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We will discuss trend filtering, a recently proposed tool of Kim et al. (2009) for nonparametric regression. The trend filtering estimate is defined as the minimizer of a penalized least squares criterion, in which the penalty term sums the absolute kth order discrete derivatives over the input points. Perhaps not surprisingly, trend filtering estimates appear to have the structure of kth degree spline functions, with adaptively chosen knot points (we say “appear” here as trend filtering estimates are not really functions over continuous domains, and are only defined over the discrete set of inputs). This brings to mind comparisons to other nonparametric regression tools that also produce adaptive splines; in particular, we will compare trend filtering to smoothing splines, which penalize the sum of squared derivatives across input points, and to locally adaptive regression splines (Mammen &amp;amp; van de Geer 1997), which penalize the total variation of the kth derivative.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Insurance company operations and dependence modeling</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-03-21/</link>
      <pubDate>Fri, 21 Mar 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-03-21/</guid>
      <description>&lt;h4 id=&#34;date-2014-03-21&#34;&gt;Date: 2014-03-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-107&#34;&gt;Location: BURN 107&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Actuaries and other analysts have long had the responsibility in insurance company operations for various financial functions including  (i) ratemaking, the process of setting premiums, (ii) loss reserving, the process of predicting obligations that arise from policies, and (iii) claims management, including fraud detection. With the advent of modern computing capabilities and detailed and novel data sources, new  opportunities to make an impact on insurance company operations are extensive.&lt;/p&gt;</description>
    </item>
    <item>
      <title>ABC as the new empirical Bayes approach?</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-02-28/</link>
      <pubDate>Fri, 28 Feb 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-02-28/</guid>
      <description>&lt;h4 id=&#34;date-2014-02-28&#34;&gt;Date: 2014-02-28&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1330-1430&#34;&gt;Time: 13:30-14:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-udm-pav-roger-gaudry-salle-s-116&#34;&gt;Location: UdM, Pav. Roger-Gaudry, Salle S-116&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Approximate Bayesian computation (ABC) has now become an essential tool for the analysis of complex stochastic models when the likelihood function is unavailable. The approximation is seen as a nuisance from a computational statistic point of view but we argue here it is also a blessing from an inferential perspective. We illustrate this paradoxical stand in the case of dynamic models and population genetics models. There are also major inference difficulties, as detailed in the case of Bayesian model choice.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Calibration of computer experiments with large data structures</title>
      <link>https://mcgillstat.github.io/post/2014winter/2014-01-24/</link>
      <pubDate>Fri, 24 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2014winter/2014-01-24/</guid>
      <description>&lt;h4 id=&#34;date-2014-01-24&#34;&gt;Date: 2014-01-24&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-salle-1355-pavillon-andré-aisenstadt-crm&#34;&gt;Location: Salle 1355, pavillon André-Aisenstadt (CRM)&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Statistical model calibration of computer models is commonly done in a wide variety of scientific endeavours. In the end, this exercise amounts to solving an inverse problem and a form of regression.  Gaussian process model are very convenient in this setting as non-parametric regression estimators and provide sensible inference properties.  However, when the data structures are large, fitting the model becomes difficult.  In this work, new methodology for calibrating large computer experiments is presented. We proposed to perform the calibration exercise by modularizing a hierarchical statistical model with approximate emulation via local Gaussian processes.  The approach is motivated by an application to radiative shock hydrodynamics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Great probabilists publish posthumously</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-12-06/</link>
      <pubDate>Fri, 06 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-12-06/</guid>
      <description>&lt;h4 id=&#34;date-2013-12-06&#34;&gt;Date: 2013-12-06&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-uqam-salle-sh-3420&#34;&gt;Location: UQAM Salle SH-3420&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Jacob Bernoulli died in 1705. His great book Ars Conjectandi was published in 1713, 300 years ago. Thomas Bayes died in 1761. His great paper was read to the Royal Society of London in December 1763, 250 years ago, and published in 1764. These anniversaries are noted by discussing new evidence regarding the circumstances of publication, which in turn can lead to a better understanding of the works themselves. As to whether or not these examples of posthumous publication suggest a career move for any modern probabilist; that question is left to the audience.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Signal detection in high dimension: Testing sphericity against spiked alternatives</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-11-29/</link>
      <pubDate>Fri, 29 Nov 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-11-29/</guid>
      <description>&lt;h4 id=&#34;date-2013-11-29&#34;&gt;Date: 2013-11-29&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-concordia-mb-2270&#34;&gt;Location: Concordia MB-2.270&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We consider the problem of testing the null hypothesis of sphericity for a high-dimensional covariance matrix against the alternative  of a finite (unspecified) number of symmetry-breaking directions (multispiked alternatives) from the point of view of the asymptotic theory of statistical experiments.  The region lying below the so-called phase transition or  impossibility threshold is shown to be a contiguity region.  Simple analytical expressions are derived for the asymptotic power envelope and the asymptotic powers of existing tests. These asymptotic powers are shown to lie very substantially below the power envelope; some of them even  trivially coincide with the size of the test. In contrast, the asymptotic power of the likelihood ratio test is shown to be uniformly close to the same.&lt;/p&gt;</description>
    </item>
    <item>
      <title>XY - Basketball meets Big Data</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-10-25/</link>
      <pubDate>Fri, 25 Oct 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-10-25/</guid>
      <description>&lt;h4 id=&#34;date-2013-10-25&#34;&gt;Date: 2013-10-25&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-hec-montréal-salle-cibc-1er-étage&#34;&gt;Location: HEC Montréal Salle CIBC 1er étage&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In this talk, I will explore the state of the art in the analysis and modeling of player tracking data in the NBA.  In the past, player tracking data has been used primarily for visualization, such as understanding the spatial distribution of a player’s shooting characteristics, or to extract summary statistics, such as the distance traveled by a player in a given game.  In this talk, I will present how we&amp;rsquo;re using advanced statistics and machine learning tools to answer previously unanswerable questions about the NBA.  Examples include “How should teams configure their defensive matchups to minimize a player’s effectiveness?”, “Who are the best decision makers in the NBA?”, and “Who was responsible for the most points against in the NBA last season?”&lt;/p&gt;</description>
    </item>
    <item>
      <title>Measurement error and variable selection in parametric and nonparametric models</title>
      <link>https://mcgillstat.github.io/post/2013fall/2013-09-27/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013fall/2013-09-27/</guid>
      <description>&lt;h4 id=&#34;date-2013-09-27&#34;&gt;Date: 2013-09-27&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-rphys-114&#34;&gt;Location: RPHYS 114&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This talk will start with a discussion of the relationships between LASSO estimation, ridge regression, and attenuation due to measurement error as motivation for, and introduction to, a new generalizable approach to variable selection in parametric and nonparametric regression and discriminant analysis. The approach transcends the boundaries of parametric/nonparametric models. It will first be described in the familiar context of linear regression where its relationship to the LASSO will be described in detail. The latter part of the talk will focus on implementation of the approach to nonparametric modeling where sparse dependence on covariates is desired. Applications to two- and multi-category classification problems will be discussed in detail.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Arup Bose: Consistency of large dimensional sample covariance matrix under weak dependence</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-04-12/</link>
      <pubDate>Fri, 12 Apr 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-04-12/</guid>
      <description>&lt;h4 id=&#34;date-2013-04-12&#34;&gt;Date: 2013-04-12&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-concordia&#34;&gt;Location: Concordia&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Estimation of large dimensional covariance matrix has been of interest recently. One model assumes that there are  $p$ dimensional independent identically distributed Gaussian observations $X_1, \ldots , X_n$ with dispersion matrix $\Sigma_p$ and $p$ grows much faster than $n$. Appropriate convergence rate results have been established in the literature for tapered and banded estimators of $\Sigma_p$ which are based on the sample variance covariance matrix of $n$ observations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hélène Massam: The hyper Dirichlet revisited: a characterization</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-03-22/</link>
      <pubDate>Fri, 22 Mar 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-03-22/</guid>
      <description>&lt;h4 id=&#34;date-2013-03-22&#34;&gt;Date: 2013-03-22&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-107&#34;&gt;Location: BURN 107&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We give a characterization of the hyper Dirichlet distribution hyper Markov with respect to a decomposable graph $G$ (or equivalently a moral directed acyclic graph). For $X=(X_1,\ldots,X_d)$ following the hyper Dirichlet distribution, our characterization is through the so-called &amp;ldquo;local and global independence properties&amp;rdquo; for a carefully designed family of orders of the variables $X_1,\ldots,X_d$.&lt;/p&gt;&#xA;&lt;p&gt;The hyper Dirichlet for general directed acyclic graphs was derived from a characterization of the Dirichlet distribution given by Geiger and Heckerman (1997). This characterization of the Dirichlet for $X=(X_1,\ldots,X_d)$ is obtained through a functional equation derived from the local and global independence properties for two different orders of the variables. These two orders are seemingly chosen haphazardly but, as our results show, this is not so. Our results generalize those of Geiger and Heckerman (1997) and are given without the assumption of existence of a positive density for $X$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Victor Chernozhukov: Inference on treatment effects after selection amongst high-dimensional controls</title>
      <link>https://mcgillstat.github.io/post/2013winter/2013-01-18/</link>
      <pubDate>Fri, 18 Jan 2013 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2013winter/2013-01-18/</guid>
      <description>&lt;h4 id=&#34;date-2013-01-18&#34;&gt;Date: 2013-01-18&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-306&#34;&gt;Location: BURN 306&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We propose robust methods for inference on the effect of a treatment variable on a scalar outcome in the presence of very many controls. Our setting is a partially linear model with possibly non-Gaussian and heteroscedastic disturbances. Our analysis allows the number of controls to be much larger than the sample size. To make informative inference feasible, we require the model to be approximately sparse; that is, we require that the effect of confounding factors can be controlled for up to a small approximation error by conditioning on a relatively small number of controls whose identities are unknown. The latter condition makes it possible to estimate the treatment effect by selecting approximately the right set of controls. We develop a novel estimation and uniformly valid inference method for the treatment effect in this setting, called the &amp;ldquo;post-double-selection&amp;rdquo; method. Our results apply to Lasso-type methods used for covariate selection as well as to any other model selection method that is able to find a sparse model with good approximation properties.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What percentage of children in the U.S. are eating a healthy diet? A statistical approach</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-12-14/</link>
      <pubDate>Fri, 14 Dec 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-12-14/</guid>
      <description>&lt;h4 id=&#34;date-2012-12-14&#34;&gt;Date: 2012-12-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-concordia--room-lb-921-04&#34;&gt;Location: Concordia,  Room LB 921-04&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In the United States the preferred method of obtaining dietary intake data is the 24-hour dietary recall, yet the measure of most interest is usual or long-term average daily intake, which is impossible to measure. Thus, usual dietary intake is assessed with considerable measurement error. Also, diet represents numerous foods, nutrients and other components, each of which have distinctive attributes. Sometimes, it is useful to examine intake of these components separately, but increasingly nutritionists are interested in exploring them collectively to capture overall dietary patterns and their effect on various diseases. Consumption of these components varies widely: some are consumed daily by almost everyone on every day, while others are episodically consumed so that 24-hour recall data are zero-inflated. In addition, they are often correlated with each other. Finally, it is often preferable to analyze the amount of a dietary component relative to the amount of energy (calories) in a diet because dietary recommendations often vary with energy level.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A nonparametric Bayesian model for local clustering</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-11-23/</link>
      <pubDate>Fri, 23 Nov 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-11-23/</guid>
      <description>&lt;h4 id=&#34;date-2012-11-23&#34;&gt;Date: 2012-11-23&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-burn-107&#34;&gt;Location: BURN 107&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;We propose a nonparametric Bayesian local clustering (NoB-LoC) approach for heterogeneous data.  Using genomics data as an example, the NoB-LoC clusters genes into gene sets and simultaneously creates multiple partitions of samples, one for each gene set. In other words, the sample partitions are nested within the gene sets.  Inference is guided by a joint probability model on all random elements. Biologically, the model formalizes the notion that biological samples cluster differently with respect to different genetic processes, and that each process is related to only a small subset of genes. These local features are importantly different from global clustering approaches such as hierarchical clustering, which create one partition of samples that applies for all genes in the data set. Furthermore, the NoB-LoC includes a special cluster of genes that do not give rise to any meaningful partition of samples. These genes could be irrelevant to the disease conditions under investigation. Similarly, for a given gene set, the NoB-LoC includes a subset of samples that do not co-cluster with other samples. The samples in this special cluster could, for example, be those whose disease subtype is not characterized by the particular gene set.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Observational studies in healthcare: are they any good?</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-10-19/</link>
      <pubDate>Fri, 19 Oct 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-10-19/</guid>
      <description>&lt;h4 id=&#34;date-2012-10-19&#34;&gt;Date: 2012-10-19&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-udem&#34;&gt;Location: UdeM&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;Observational healthcare data, such as administrative claims and electronic health records, play an increasingly prominent role in healthcare.  Pharmacoepidemiologic studies in particular routinely estimate temporal associations between medical product exposure and subsequent health outcomes of interest, and such studies influence prescribing patterns and healthcare policy more generally.  Some authors have questioned the reliability and accuracy of such studies, but few previous efforts have attempted to measure their performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Regularized semiparametric functional linear regression</title>
      <link>https://mcgillstat.github.io/post/2012fall/2012-09-21/</link>
      <pubDate>Fri, 21 Sep 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012fall/2012-09-21/</guid>
      <description>&lt;h4 id=&#34;date-2012-09-21&#34;&gt;Date: 2012-09-21&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1430-1530&#34;&gt;Time: 14:30-15:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-mcgill-burnside-hall-1214&#34;&gt;Location: McGill, Burnside Hall 1214&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;In many scientific experiments we need to face analysis with functional data, where the observations are sampled from random process, together with a potentially large number of non-functional covariates. The complex nature of functional data makes it difficult to directly apply existing methods to model selection and estimation. We propose and study a new class of penalized semiparametric functional linear regression to characterize the regression relation between a scalar response and multiple covariates, including both functional covariates and scalar covariates. The resulting method provides a unified and flexible framework to jointly model functional and non-functional predictors, identify important covariates, and improve efficiency and interpretability of the estimates. Featured with two types of regularization: the shrinkage on the effects of scalar covariates and the truncation on principal components of the functional predictor, the new approach is flexible and effective in dimension reduction. One key contribution of this paper is to study theoretical properties of the regularized semiparametric functional linear model. We establish oracle and consistency properties under mild conditions by allowing possibly diverging number of scalar covariates and simultaneously taking the infinite-dimensional functional predictor into account. We illustrate the new estimator with extensive simulation studies, and then apply it to an image data analysis.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Li: High-dimensional feature selection using hierarchical Bayesian logistic regression with heavy-tailed priors | Rao: Best predictive estimation for linear mixed models with applications to small area estimation</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-04-13/</link>
      <pubDate>Fri, 13 Apr 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-04-13/</guid>
      <description>&lt;h4 id=&#34;date-2012-04-13&#34;&gt;Date: 2012-04-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1400-1630&#34;&gt;Time: 14:00-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-maass-217&#34;&gt;Location: MAASS 217&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Li&lt;/em&gt;: The problem of selecting the most useful features from a great many (eg, thousands) of candidates arises in many areas of modern sciences. An interesting problem from genomic research is that, from thousands of genes that are active (expressed) in certain tissue cells, we want to ﬁnd the genes that can be used to separate tissues of diﬀerent classes (eg. cancer and normal). In this paper, we report a Bayesian logistic regression method based on heavytailed priors with moderately small degree freedom (such as 1) and small scale (such as 0.01), and using Gibbs sampling to do the computation. We show that it can distinctively separate a couple of useful features from a large number of useless ones, and discriminate many redundant correlated features. We also show that this method is very stable to the choice of scale. We apply our method to a microarray data set related to prostate cancer, and identify only 3 genes out of 6033 candidates that can separate cancer and normal tissues very well in leave-one-out cross-validation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using tests of homoscedasticity to test missing completely at random | Hugh Chipman: Sequential optimization of a computer model and other Active Learning problems</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-03-09/</link>
      <pubDate>Fri, 09 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-03-09/</guid>
      <description>&lt;h4 id=&#34;date-2012-03-09&#34;&gt;Date: 2012-03-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1400-1630&#34;&gt;Time: 14:00-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-uqam-201-ave-du-président-kennedy-salle-5115&#34;&gt;Location: UQAM, 201 ave. du Président-Kennedy, salle 5115&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Li&lt;/em&gt;: The problem of selecting the most useful features from a great many (eg, thousands) of candidates arises in many areas of modern sciences. An interesting problem from genomic research is that, from thousands of genes that are active (expressed) in certain tissue cells, we want to ﬁnd the genes that can be used to separate tissues of diﬀerent classes (eg. cancer and normal). In this paper, we report a Bayesian logistic regression method based on heavytailed priors with moderately small degree freedom (such as 1) and small scale (such as 0.01), and using Gibbs sampling to do the computation. We show that it can distinctively separate a couple of useful features from a large number of useless ones, and discriminate many redundant correlated features. We also show that this method is very stable to the choice of scale. We apply our method to a microarray data set related to prostate cancer, and identify only 3 genes out of 6033 candidates that can separate cancer and normal tissues very well in leave-one-out cross-validation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stute: Principal component analysis of the Poisson Process | Blath: Longterm properties of the symbiotic branching model</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-02-10/</link>
      <pubDate>Fri, 10 Feb 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-02-10/</guid>
      <description>&lt;h4 id=&#34;date-2012-02-10&#34;&gt;Date: 2012-02-10&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1400-1630&#34;&gt;Time: 14:00-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-concordia&#34;&gt;Location: Concordia&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Stute&lt;/em&gt;: The Poisson Process constitutes a well-known model for describing random events over time.  It has many applications in marketing research, insurance mathematics and finance.  Though it has been studied for decades not much is known how to check (in a non-asymptotic way) the validity of the Poisson Process.  In this talk we present the principal component decomposition of the Poisson Process which enables us to derive finite sample properties of associated goodness-of-fit tests.  In the first step we show that the Fourier-transforms of the components contain Bessel and Struve functions.  Inversion leads to densities which are modified arc sin distributions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bayesian approaches to evidence synthesis in clinical practice guideline development</title>
      <link>https://mcgillstat.github.io/post/2012winter/2012-01-13/</link>
      <pubDate>Fri, 13 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2012winter/2012-01-13/</guid>
      <description>&lt;h4 id=&#34;date-2012-01-13&#34;&gt;Date: 2012-01-13&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-concordia-library-building-lb-92104&#34;&gt;Location: Concordia, Library Building LB-921.04&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;The American College of Cardiology Foundation (ACCF) and the American Heart Association (AHA) have jointly engaged in the production of guideline in the area of cardiovascular disease since 1980. The developed guidelines are intended to&#xA;assist health care providers in clinical decision making by describing a range of generally acceptable approaches for the diagnosis, management, or prevention of specific diseases or conditions. This talk describes some of our work under a contract with ACCF/AHA for applying Bayesian methods to guideline recommendation development. In a demonstration example, we use Bayesian meta-analysis strategies to summarize evidence on the comparative effectiveness between Percutaneous coronary intervention and Coronary artery bypass grafting for patients with unprotected left main coronary artery disease. We show the usefulness and flexibility of Bayesian methods in handling data arisen from studies with different designs (e.g. RCTs and observational studies), performing indirect comparison among treatments when studies with direct comparisons are unavailable, and accounting for historical data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Detecting evolution in experimental ecology: Diagnostics for missing state variables</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-12-09/</link>
      <pubDate>Fri, 09 Dec 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-12-09/</guid>
      <description>&lt;h4 id=&#34;date-2011-12-09&#34;&gt;Date: 2011-12-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1530-1630&#34;&gt;Time: 15:30-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-uqam-salle-5115&#34;&gt;Location: UQAM Salle 5115&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;This talk considers goodness of fit diagnostics for time-series data from processes approximately modeled by systems of nonlinear ordinary differential equations. In particular, we seek to determine three nested causes of lack of fit: (i) unmodeled stochastic forcing, (ii) mis-specified functional forms and (iii) mis-specified state variables. Testing lack of fit in differential equations is challenging since the model is expressed in terms of rates of change of the measured variables. Here, lack of fit is represented on the model scale via time-varying parameters. We develop tests for each of the three cases above through bootstrap and permutation methods.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Guérin: An ergodic variant of the telegraph process for a toy model of bacterial chemotaxis | Staicu: Skewed functional processes and their applications</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-11-11/</link>
      <pubDate>Fri, 11 Nov 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-11-11/</guid>
      <description>&lt;h4 id=&#34;date-2011-11-11&#34;&gt;Date: 2011-11-11&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1400-1630&#34;&gt;Time: 14:00-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-udem&#34;&gt;Location: UdeM&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Guérin&lt;/em&gt;: I will study the long time behavior of a variant of the classic telegraph process, with non-constant jump rates that induce a drift towards the origin. This process can be seen as a toy model for velocity-jump processes recently proposed as mathematical models of bacterial chemotaxis. I will give its invariant law and construct an explicit coupling for velocity and position, providing exponential ergodicity with moreover a quantitative control of the total variation distance to equilibrium at each time instant. It is a joint work with Joaquin Fontbona (Universidad de Santiago, Chile) and Florent Malrieu (Université Rennes 1, France).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dupuis: Modeling non-stationary extremes: The case of heat waves | Davis: Estimating extremal dependence in time series via the extremogram</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-10-14/</link>
      <pubDate>Fri, 14 Oct 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-10-14/</guid>
      <description>&lt;h4 id=&#34;date-2011-10-14&#34;&gt;Date: 2011-10-14&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1400-1630&#34;&gt;Time: 14:00-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-trottier-1080&#34;&gt;Location: TROTTIER 1080&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Dupuis&lt;/em&gt;: Environmental processes are often non-stationary since climate patterns cause systematic seasonal effects and long-term climate changes cause trends.  The usual limit models are not applicable for non-stationary processes, but models from standard extreme value theory can be used along with statistical modeling to provide useful inference. Traditional approaches include letting model parameters be a function of covariates or using time-varying thresholds. These approaches are inadequate for the study of heat waves however and we show how a recent pre-processing approach by Eastoe and Tawn (2009) can be used in conjunction with an innovative change-point analysis to model daily maximum temperature.  The model is then fitted to data from four U.S. cities and used to estimate the recurrence probabilities of runs over seasonally high temperatures.  We show that the probability of long and intense heat waves has increased considerably over 50 years.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Susko: Properties of Bayesian posteriors and bootstrap support in phylogenetic inference | Labbe: An integrated hierarchical Bayesian model for multivariate eQTL genetic mapping</title>
      <link>https://mcgillstat.github.io/post/2011fall/2011-09-09/</link>
      <pubDate>Fri, 09 Sep 2011 00:00:00 +0000</pubDate>
      <guid>https://mcgillstat.github.io/post/2011fall/2011-09-09/</guid>
      <description>&lt;h4 id=&#34;date-2011-09-09&#34;&gt;Date: 2011-09-09&lt;/h4&gt;&#xA;&lt;h4 id=&#34;time-1400-1630&#34;&gt;Time: 14:00-16:30&lt;/h4&gt;&#xA;&lt;h4 id=&#34;location-udem-pav-andré-aisenstadt-salle-1360&#34;&gt;Location: UdeM, Pav. André-Aisenstadt, SALLE 1360&lt;/h4&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&#xA;&lt;p&gt;&lt;em&gt;Susko&lt;/em&gt;: The data generated by large scale sequencing projects is complex, high-dimensional, multivariate discrete data. In studies of evolutionary biology, the parameter space of evolutionary trees is an unusual additional complication from a statistical perspective. In this talk I will briefly introduce the general approaches to utilizing sequence data in phylogenetic inference. A particular issue of interest in phylogenetic inference is assessments of uncertainty about the true tree or structures that might be present in it. The primary way in which uncertainty is assessed in practice is through bootstrap support (BP) for splits, large values indicating strong support for the split. A difficulty with this measure, however, has been deciding how large is large enough. We discuss the interpretation of BP and ways of adjusting it so that it has an interpretation similar to a p-value. A related issue, having to do with the behaviour of methods when data are generated from a star tree, gives rise to an interesting example in which, due to the unusual statistical nature,Bayesian and maximum likelihood methods give strikingly different results, even asymptotically.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
