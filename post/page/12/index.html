<!DOCTYPE html>
<html>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4321&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="pinterest" content="nopin">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta name="generator" content="Hugo 0.136.5">

/post/index.xml

<link rel="canonical" href="http://localhost:4321/post/">


    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/solarized_dark.min.css">
    <title>Past Seminar Series - McGill Statistics Seminars</title>
    
    <link href="http://localhost:4321/css/styles.css" rel="stylesheet">
    

  </head>

  <body>
    
    
    

    <header class="l-header">
      <nav class="navbar navbar-default">
        <div class="container">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://localhost:4321/">McGill Statistics Seminars</a>
          </div>

          
          <div id="navbar" class="collapse navbar-collapse">
            
            <ul class="nav navbar-nav navbar-right">
              
              
              <li><a href="/">Current Seminar Series</a></li>
              
              
              
              <li><a href="/post/">Past Seminar Series</a></li>
              
              
            </ul>
            
          </div>
          

        </div>
      </nav>
    </header>

    <main>
      <div class="container">
        
<div class="row">
  <div class="col-md-8">

    
    <header class="page-header">
      <h1>Past Seminar Series</h1>
    </header>
    

    <ul class="p-articles">
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2019-03-15T00:00:00JST">Mar 15, 2019</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2019winter/2019-03-15/">Hierarchical Bayesian Modelling for Wireless Cellular Networks</a></h2>
    <h3 class="post-meta">Deniz Ustebay · Mar 15, 2019 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2019-03-15">Date: 2019-03-15</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>With the recent advances in wireless technologies, base stations are becoming more sophisticated. The network operators are also able to collect more data to improve network performance and user experience. In this paper we concentrate on modeling performance of wireless cells using hierarchical Bayesian modeling framework. This framework provides a principled way to navigate the space between the option of creating one model to represent all cells in a network and the option of creating separate models at each cell. The former option ignores the variations between cells (complete pooling) whereas the latter is overly noisy and ignores the common patterns in cells (no pooling). The hierarchical Bayesian model strikes a trade-off between these two extreme cases and enables us to do partial pooling of the data from all cells. This is done by estimating a parametric population distribution and assuming that each cell is a sample from this distribution. Because this model is fully Bayesian, it provides uncertainty intervals around each estimated parameter which can be used by network operators making network management decisions. We examine the performance of this method on a synthetic dataset and a real dataset collected from a cellular network.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2019winter/2019-03-15/" title="Hierarchical Bayesian Modelling for Wireless Cellular Networks">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2019-03-01T00:00:00JST">Mar 1, 2019</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2019winter/2019-03-01/">Statistical Inference for partially observed branching processes, with application to hematopoietic lineage tracking</a></h2>
    <h3 class="post-meta">Jason Xu · Mar 1, 2019 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2019-03-01">Date: 2019-03-01</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1104">Location: BURN 1104</h4>
<h2 id="abstract">Abstract:</h2>
<p>The likelihood function is central to many statistical procedures, but poses challenges in classical and modern data settings. Motivated by cell lineage tracking experiments to study hematopoiesis (the process of blood cell production), we present recent methodology enabling likelihood-based inference for partially observed data arising from continuous-time branching processes. These computational advances allow principled procedures such as maximum likelihood estimation, posterior inference, and expectation-maximization (EM) algorithms in previously intractable data settings. We then discuss limitations and alternatives when data are very large or generated from a hidden process, and potential ways forward using ideas from sparse optimization.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2019winter/2019-03-01/" title="Statistical Inference for partially observed branching processes, with application to hematopoietic lineage tracking">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2019-02-22T00:00:00JST">Feb 22, 2019</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2019winter/2019-02-22/">Uniform, nonparametric, non-asymptotic confidence sequences</a></h2>
    <h3 class="post-meta">Aaditya Ramdas · Feb 22, 2019 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2019-02-22">Date: 2019-02-22</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>A confidence sequence is a sequence of confidence intervals that is uniformly valid over an unbounded time horizon. In this paper, we develop non-asymptotic confidence sequences under nonparametric conditions that achieve arbitrary precision. Our technique draws a connection between the classical Cramer-Chernoff method, the law of the iterated logarithm (LIL), and the sequential probability ratio test (SPRT)—our confidence sequences extend the first to produce time-uniform concentration bounds, provide tight non-asymptotic characterizations of the second, and generalize the third to nonparametric settings, including sub-Gaussian and Bernstein conditions, self-normalized processes, and matrix martingales. We strengthen and generalize existing constructions of finite-time iterated logarithm (“finite LIL”) bounds. We illustrate the generality of our proof techniques by deriving an empirical-Bernstein finite LIL bound as well as a novel upper LIL bound for the maximum eigenvalue of a sum of random matrices. Finally, we demonstrate the utility of our approach with applications to covariance matrix estimation and to estimation of sample average treatment effect under the Neyman-Rubin potential outcomes model, for which we give a non-asymptotic, sequential estimation strategy which handles adaptive treatment mechanisms such as Efron’s biased coin design.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2019winter/2019-02-22/" title="Uniform, nonparametric, non-asymptotic confidence sequences">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2019-02-15T00:00:00JST">Feb 15, 2019</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2019winter/2019-02-15/">Causal Inference with Unmeasured Confounding: an Instrumental Variable Approach</a></h2>
    <h3 class="post-meta">Linbo Wang · Feb 15, 2019 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2019-02-15">Date: 2019-02-15</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>Causal inference is a challenging problem because causation cannot be established from observational data alone. Researchers typically rely on additional sources of information to infer causation from association. Such information may come from powerful designs such as randomization, or background knowledge such as information on all confounders. However, perfect designs or background knowledge required for establishing causality may not always be available in practice. In this talk, I use novel causal identification results to show that the instrumental variable approach can be used to combine the power of design and background knowledge to draw causal conclusions. I also introduce novel estimation tools to construct estimators that are robust, efficient and enjoy good finite sample properties. These methods will be discussed in the context of a randomized encouragement design for a flu vaccine.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2019winter/2019-02-15/" title="Causal Inference with Unmeasured Confounding: an Instrumental Variable Approach">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2019-02-08T00:00:00JST">Feb 8, 2019</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2019winter/2019-02-08/">Patient-Specific Finite Element Analysis of Human Heart: Mathematical and Statistical Opportunities and Challenges</a></h2>
    <h3 class="post-meta">Alireza Heidari · Feb 8, 2019 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2019-02-08">Date: 2019-02-08</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1104">Location: BURN 1104</h4>
<h2 id="abstract">Abstract:</h2>
<p>Cardiovascular diseases (CVD) are the leading cause of death globally and ranks second in
Canada, costing the Canadian economy over $20 billion every year. Despite the recent progress in CVD through prevention, lifestyle changes, and the use of biomedical treatments to improve survival rates and quality of life, there has been a lack in the integration of computer-aided engineering (CAE) in this field. Clinically, proposing cut-off values while taking into consideration patient-specific risk is of paramount importance for increased rate ofsurvival and improved quality of life. Computational modeling has proved to be used in determining parameters that cannot be assessed experimentally. The latest developments in computational modelling of human heart are presented and the constitutive equations, the key ingredient of these in-silico modellings of human heart, are discussed. Finite Element analysis of cardiac diseases provide a framework to generate synthetic data for developing statistical models when collecting the real data require invasive procedure. The idea of virtual personalized cardiology will be discussed.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2019winter/2019-02-08/" title="Patient-Specific Finite Element Analysis of Human Heart: Mathematical and Statistical Opportunities and Challenges">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2019-02-01T00:00:00JST">Feb 1, 2019</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2019winter/2019-02-01/">Network models, sampling, and symmetry properties</a></h2>
    <h3 class="post-meta">Peter Orbanz · Feb 1, 2019 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2019-02-01">Date: 2019-02-01</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>A recent body of work, by myself and many others, aims to develop a statistical theory of network data for problems a single network is observed. Of the models studied in this area, graphon models are probably most widely known in statistics. I will explain the relationship between three aspects of this work: (1) Specific models, such as graphon models, graphex models, and edge-exchangeable graphs. (2) Sampling theory for networks, specifically in the case statisticians might refer to as an infinite-population limit. (3) Invariance properties, especially various forms of exchangeability. I will also present recent results that show how statistically relevant results (such as central limit theorems) can be derived from such invariance properties.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2019winter/2019-02-01/" title="Network models, sampling, and symmetry properties">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2019-01-25T00:00:00JST">Jan 25, 2019</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2019winter/2019-01-25/">Modern Non-Problems in Optimization: Applications to Statistics and Machine Learning</a></h2>
    <h3 class="post-meta">Ying Cui · Jan 25, 2019 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2019-01-25">Date: 2019-01-25</h4>
<h4 id="time-1600-1700">Time: 16:00-17:00</h4>
<h4 id="location-burn-920">Location: BURN 920</h4>
<h2 id="abstract">Abstract:</h2>
<p>We have witnessed a lot of exciting development of data science in recent years. From the perspective of optimization, many modern data-science problems involve some basic ``non’’-properties that lack systematic treatment by the current approaches for the sake of the computation convenience. These non-properties include the coupling of the non-convexity, non-differentiability and non-determinism. In this talk, we present rigorous computational methods for solving two typical non-problems: the piecewise linear regression and the feed-forward deep neural network. The algorithmic framework is an integration of the first order non-convex majorization-minimization method and the second order non-smooth Newton methods. Numerical experiments demonstrate the effectiveness of our proposed approach. Contrary to existing methods for solving non-problems which provide at best very weak guarantees on the computed solutions obtained in practical implementation, our rigorous mathematical treatment aims to understand properties of these computed solutions with reference to both the empirical and the population risk minimizations.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2019winter/2019-01-25/" title="Modern Non-Problems in Optimization: Applications to Statistics and Machine Learning">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2019-01-18T00:00:00JST">Jan 18, 2019</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2019winter/2019-01-18/">Singularities of the information matrix and longitudinal data with change points</a></h2>
    <h3 class="post-meta">Masoud Asgharian · Jan 18, 2019 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2019-01-18">Date: 2019-01-18</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>Non-singularity of the information matrix plays a key role in model identification and the asymptotic theory of statistics. For many statistical models, however, this condition seems virtually impossible to verify. An example of such models is a class of mixture models associated with multi-path change-point problems (MCP) which can model longitudinal data with change points. The MCP models are similar in nature to mixture-of-experts models in machine learning. The question then arises as to how often the non-singularity assumption of the information matrix fails to hold. We show that</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2019winter/2019-01-18/" title="Singularities of the information matrix and longitudinal data with change points">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2019-01-11T00:00:00JST">Jan 11, 2019</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2019winter/2019-01-11/">Magic Cross-Validation Theory for Large-Margin Classification</a></h2>
    <h3 class="post-meta">Boxiang Wang · Jan 11, 2019 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2019-01-11">Date: 2019-01-11</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>Cross-validation (CV) is perhaps the most widely used tool for tuning supervised machine learning algorithms in order to achieve better generalization error rate. In this paper, we focus on leave-one-out cross-validation (LOOCV) for the support vector machine (SVM) and related algorithms. We first address two wide-spreading misconceptions on LOOCV. We show that LOOCV, ten-fold, and five-fold CV are actually well-matched in estimating the generalization error, and the computation speed of LOOCV is not necessarily slower than that of ten-fold and five-fold CV. We further present a magic CV theory with a surprisingly simple recipe which allows users to very efficiently tune the SVM. We then apply the magic CV theory to demonstrate a straightforward way to prove the Bayes risk consistency of the SVM. We have implemented our algorithms in a publicly available R package magicsvm, which is much faster than the state-of-the-art SVM solvers. We demonstrate our methods on extensive simulations and benchmark examples.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2019winter/2019-01-11/" title="Magic Cross-Validation Theory for Large-Margin Classification">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2018-11-23T00:00:00JST">Nov 23, 2018</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2018fall/2018-11-23/">p-values vs Bayes factors: Is there a compromise?</a></h2>
    <h3 class="post-meta">David Wolfson · Nov 23, 2018 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2018-11-23">Date: 2018-11-23</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1104">Location: BURN 1104</h4>
<h2 id="abstract">Abstract:</h2>
<p>This is not a research talk. Rather, the goal is to address the topic of the talk title through a
2017 multi-authored paper published in Nature Human Behaviour. The Nature article proposes that the standard cut-off significance level of .05 should be replaced by a cut-off level of .005 when new discoveries are being claimed. The authors attribute the high proportion of irreducible results in the literature that accompany claimed new discoveries, in part, to the low-bar cut-off of .05. Their fix is built around the Bayes factor. I will begin with a brief presentation of the difference between the frequentist and Bayesian approaches to statistical inference, and lead into p-values vs Bayes factors for hypothesis testing before discussing the Nature article itself. It is hoped that the talk will provoke thought about the way we do statistics.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2018fall/2018-11-23/" title="p-values vs Bayes factors: Is there a compromise?">Read More…</a>
  </footer>
  
</article>
</li>
      
    </ul>

    
<nav>
  <ul class="pager">

    
    <li><a href="/post/page/11/">Previous</a></li>
    

    
    <li><a href="/post/page/13/">Next</a></li>
    

  </ul>
</nav>



  </div>
  <div class="col-md-4">
    
<aside class="l-sidebar">

  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">Recent Talks</div>
    </div>
    <div class="list-group">
      
      <a href="http://localhost:4321/tags/2024-fall/" class="list-group-item"> · Nov 8, 2024</a>
      
      <a href="http://localhost:4321/post/2024fall/2024-11-08/" class="list-group-item">Christian Genest · Nov 8, 2024</a>
      
      <a href="http://localhost:4321/categories/" class="list-group-item"> · Nov 8, 2024</a>
      
      <a href="http://localhost:4321/categories/mcgill-statistics-seminar/" class="list-group-item"> · Nov 8, 2024</a>
      
      <a href="http://localhost:4321/tags/" class="list-group-item"> · Nov 8, 2024</a>
      
    </div>
  </section>

  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">CATEGORY</div>
    </div>
    <div class="list-group">
      
      <a href="http://localhost:4321/categories/mcgill-statistics-seminar" class="list-group-item">mcgill statistics seminar</a>
      
      <a href="http://localhost:4321/categories/crm-ssc-prize-address" class="list-group-item">crm-ssc prize address</a>
      
      <a href="http://localhost:4321/categories/crm-colloquium" class="list-group-item">crm-colloquium</a>
      
    </div>
  </section>
  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">TAG</div>
    </div>
    <div class="list-group">
      
      <a href="http://localhost:4321/tags/2024-winter" class="list-group-item">2024 winter</a>
      
      <a href="http://localhost:4321/tags/2024-fall" class="list-group-item">2024 fall</a>
      
      <a href="http://localhost:4321/tags/2023-winter" class="list-group-item">2023 winter</a>
      
      <a href="http://localhost:4321/tags/2023-summer" class="list-group-item">2023 summer</a>
      
      <a href="http://localhost:4321/tags/2023-fall" class="list-group-item">2023 fall</a>
      
      <a href="http://localhost:4321/tags/2022-winter" class="list-group-item">2022 winter</a>
      
      <a href="http://localhost:4321/tags/2022-fall" class="list-group-item">2022 fall</a>
      
      <a href="http://localhost:4321/tags/2021-winter" class="list-group-item">2021 winter</a>
      
      <a href="http://localhost:4321/tags/2021-fall" class="list-group-item">2021 fall</a>
      
      <a href="http://localhost:4321/tags/2020-winter" class="list-group-item">2020 winter</a>
      
      <a href="http://localhost:4321/tags/2020-fall" class="list-group-item">2020 fall</a>
      
      <a href="http://localhost:4321/tags/2019-winter" class="list-group-item">2019 winter</a>
      
      <a href="http://localhost:4321/tags/2019-fall" class="list-group-item">2019 fall</a>
      
      <a href="http://localhost:4321/tags/2018-winter" class="list-group-item">2018 winter</a>
      
      <a href="http://localhost:4321/tags/2018-fall" class="list-group-item">2018 fall</a>
      
      <a href="http://localhost:4321/tags/2017-winter" class="list-group-item">2017 winter</a>
      
      <a href="http://localhost:4321/tags/2017-fall" class="list-group-item">2017 fall</a>
      
      <a href="http://localhost:4321/tags/2016-winter" class="list-group-item">2016 winter</a>
      
      <a href="http://localhost:4321/tags/2016-fall" class="list-group-item">2016 fall</a>
      
      <a href="http://localhost:4321/tags/2015-winter" class="list-group-item">2015 winter</a>
      
      <a href="http://localhost:4321/tags/2015-fall" class="list-group-item">2015 fall</a>
      
      <a href="http://localhost:4321/tags/2014-winter" class="list-group-item">2014 winter</a>
      
      <a href="http://localhost:4321/tags/2014-fall" class="list-group-item">2014 fall</a>
      
      <a href="http://localhost:4321/tags/2013-winter" class="list-group-item">2013 winter</a>
      
      <a href="http://localhost:4321/tags/2013-fall" class="list-group-item">2013 fall</a>
      
      <a href="http://localhost:4321/tags/2012-winter" class="list-group-item">2012 winter</a>
      
      <a href="http://localhost:4321/tags/2012-fall" class="list-group-item">2012 fall</a>
      
      <a href="http://localhost:4321/tags/2011-fall" class="list-group-item">2011 fall</a>
      
    </div>
  </section>
  

</aside>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    
  </div>
</div>

      </div>
    </main>

    <footer class="l-footer">
      <div class="container">
        <p><span class="h-logo">&copy; McGill Statistics Seminars</span></p>
        <aside>
          <p><a href="http://www.mcgill.ca/mathstat/">Department of Mathematics and Statistics</a>.</p>
          <p><a href="https://www.mcgill.ca/">McGill University</a></p>
        </aside>
      </div>
    </footer>

    <script src="//code.jquery.com/jquery-3.1.1.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

