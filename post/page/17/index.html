<!DOCTYPE html>
<html>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4321&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="pinterest" content="nopin">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta name="generator" content="Hugo 0.136.5">

/post/index.xml

<link rel="canonical" href="http://localhost:4321/post/">


    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/solarized_dark.min.css">
    <title>Past Seminar Series - McGill Statistics Seminars</title>
    
    <link href="http://localhost:4321/css/styles.css" rel="stylesheet">
    

  </head>

  <body>
    
    
    

    <header class="l-header">
      <nav class="navbar navbar-default">
        <div class="container">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="http://localhost:4321/">McGill Statistics Seminars</a>
          </div>

          
          <div id="navbar" class="collapse navbar-collapse">
            
            <ul class="nav navbar-nav navbar-right">
              
              
              <li><a href="/">Current Seminar Series</a></li>
              
              
              
              <li><a href="/post/">Past Seminar Series</a></li>
              
              
            </ul>
            
          </div>
          

        </div>
      </nav>
    </header>

    <main>
      <div class="container">
        
<div class="row">
  <div class="col-md-8">

    
    <header class="page-header">
      <h1>Past Seminar Series</h1>
    </header>
    

    <ul class="p-articles">
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2017-03-10T00:00:00JST">Mar 10, 2017</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2017winter/2017-03-10/">High-throughput single-cell biology: The challenges and opportunities for machine learning scientists</a></h2>
    <h3 class="post-meta">Nima Aghaeepour · Mar 10, 2017 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2017-03-10">Date: 2017-03-10</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>The immune system does a lot more than killing “foreign” invaders. It’s a powerful sensory system that can detect stress levels, infections, wounds, and even cancer tumors. However, due to the complex interplay between different cell types and signaling pathways, the amount of data produced to characterize all different aspects of the immune system (tens of thousands of genes measured and hundreds of millions of cells, just from a single patient) completely overwhelms existing bioinformatics tools. My laboratory specializes in the development of machine learning techniques that address the unique challenges of high-throughput single-cell immunology. Sharing our lab space with a clinical and an immunological research laboratory, my students and fellows are directly exposed to the real-world challenges and opportunities of bringing machine learning and immunology to the (literal) bedside.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2017winter/2017-03-10/" title="High-throughput single-cell biology: The challenges and opportunities for machine learning scientists">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2017-02-24T00:00:00JST">Feb 24, 2017</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2017winter/2017-02-24/">The first pillar of statistical wisdom</a></h2>
    <h3 class="post-meta">James A. Hanley · Feb 24, 2017 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2017-02-24">Date: 2017-02-24</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>This talk will provide an introduction to the first of the pillars in Stephen Stigler&rsquo;s 2016 book The Seven Pillars of Statistical Wisdom, namely “Aggregation.” It will focus on early instances of the sample mean in scientific work, on the early error distributions, and on how their “centres” were fitted.</p>
<h2 id="speaker">Speaker</h2>
<p>James A. Hanley is a Professor in the Department of Epidemiology, Biostatistics and Occupational Health, at McGill University.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2017winter/2017-02-24/" title="The first pillar of statistical wisdom">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2017-02-17T00:00:00JST">Feb 17, 2017</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2017winter/2017-02-17/">Building end-to-end dialogue systems using deep neural architectures</a></h2>
    <h3 class="post-meta">Joelle Pineau · Feb 17, 2017 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2017-02-17">Date: 2017-02-17</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>The ability for a computer to converse in a natural and coherent manner with a human has long been held as one of the important steps towards solving artificial intelligence. In this talk I will present recent results on building dialogue systems from large corpuses using deep neural architectures. I will highlight several challenges related to data acquisition, algorithmic development, and performance evaluation.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2017winter/2017-02-17/" title="Building end-to-end dialogue systems using deep neural architectures">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2017-02-10T00:00:00JST">Feb 10, 2017</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2017winter/2017-02-10/">Sparse envelope model: Efficient estimation and response variable selection in multivariate linear regression</a></h2>
    <h3 class="post-meta">Zhihua Su · Feb 10, 2017 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2017-02-10">Date: 2017-02-10</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>The envelope model is a method for efficient estimation in multivariate linear regression. In this article, we propose the sparse envelope model, which is motivated by applications where some response variables are invariant to changes of the predictors and have zero regression coefficients. The envelope estimator is consistent but not sparse, and in many situations it is important to identify the response variables for which the regression coefficients are zero. The sparse envelope model performs variable selection on the responses and preserves the efficiency gains offered by the envelope model. Response variable selection arises naturally in many applications, but has not been studied as thoroughly as predictor variable selection. In this article, we discuss response variable selection in both the standard multivariate linear regression and the envelope contexts. In response variable selection, even if a response has zero coefficients, it still should be retained to improve the estimation efficiency of the nonzero coefficients. This is different from the practice in predictor variable selection. We establish consistency, the oracle property and obtain the asymptotic distribution of the sparse envelope estimator.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2017winter/2017-02-10/" title="Sparse envelope model: Efficient estimation and response variable selection in multivariate linear regression">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2017-02-03T00:00:00JST">Feb 3, 2017</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2017winter/2017-02-03/">MM algorithms for variance component models</a></h2>
    <h3 class="post-meta">Hua Zhou · Feb 3, 2017 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2017-02-03">Date: 2017-02-03</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>Variance components estimation and mixed model analysis are central themes in statistics with applications in numerous scientific disciplines. Despite the best efforts of generations of statisticians and numerical analysts, maximum likelihood estimation and restricted maximum likelihood estimation of variance component models remain numerically challenging. In this talk, we present a novel iterative algorithm for variance components estimation based on the minorization-maximization (MM) principle. MM algorithm is trivial to implement and competitive on large data problems. The algorithm readily extends to more complicated problems such as linear mixed models, multivariate response models possibly with missing data, maximum a posteriori estimation, and penalized estimation. We demonstrate, both numerically and theoretically, that it converges faster than the classical EM algorithm when the number of variance components is greater than two.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2017winter/2017-02-03/" title="MM algorithms for variance component models">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2017-01-27T00:00:00JST">Jan 27, 2017</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2017winter/2017-01-27/">Bayesian inference for conditional copula models</a></h2>
    <h3 class="post-meta">Radu Craiu · Jan 27, 2017 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2017-01-27">Date: 2017-01-27</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-room-6254-pavillon-andre-aisenstadt-2920-udem">Location: ROOM 6254 Pavillon Andre-Aisenstadt 2920, UdeM</h4>
<h2 id="abstract">Abstract:</h2>
<p>Conditional copula models describe dynamic changes in dependence and are useful in establishing high dimensional dependence structures or in joint modelling of response vectors in regression settings. We describe some of the methods developed for estimating the calibration function when multiple predictors are needed and for resolving some of the model choice questions concerning the selection of copula families and the shape of the calibration function. This is joint work with Evgeny Levi, Avideh Sabeti and Mian Wei.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2017winter/2017-01-27/" title="Bayesian inference for conditional copula models">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2017-01-20T00:00:00JST">Jan 20, 2017</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2017winter/2017-01-20/">Order selection in multidimensional finite mixture models</a></h2>
    <h3 class="post-meta">Tudor Manole · Jan 20, 2017 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2017-01-20">Date: 2017-01-20</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>Finite mixture models provide a natural framework for analyzing data from heterogeneous populations. In practice, however, the number of hidden subpopulations in the data may be unknown. The problem of estimating the order of a mixture model, namely the number of subpopulations, is thus crucial for many applications. In this talk, we present a new penalized likelihood solution to this problem, which is applicable to models with a multidimensional parameter space. The order of the model is estimated by starting with a large number of mixture components, which are clustered and then merged via two penalty functions. Doing so estimates the unknown parameters of the mixture, at the same time as the order. We will present extensive simulation studies, showing our approach outperforms many of the most common methods for this problem, such as the Bayesian Information Criterion. Real data examples involving normal and multinomial mixtures further illustrate its performance.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2017winter/2017-01-20/" title="Order selection in multidimensional finite mixture models">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2017-01-13T00:00:00JST">Jan 13, 2017</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2017winter/2017-01-13/">(Sparse) exchangeable graphs</a></h2>
    <h3 class="post-meta">Victor Veitch · Jan 13, 2017 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2017-01-13">Date: 2017-01-13</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>Many popular statistical models for network valued datasets fall under the remit of the graphon framework, which (implicitly) assumes the networks are densely connected. However, this assumption rarely holds for the real-world networks of practical interest. We introduce a new class of models for random graphs that generalises the dense graphon models to the sparse graph regime, and we argue that this meets many of the desiderata one would demand of a model to serve as the foundation for a statistical analysis of real-world networks. The key insight is to define the models by way of a novel notion of exchangeability; this is analogous to the specification of conditionally i.i.d. models by way of de Finetti&rsquo;s representation theorem. We further develop this model class by explaining the foundations of sampling and estimation of network models in this setting. The later result can be can be understood as the (sparse) graph analogue of estimation via the empirical distribution in the i.i.d. sequence setting.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2017winter/2017-01-13/" title="(Sparse) exchangeable graphs">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2016-12-02T00:00:00JST">Dec 2, 2016</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2016fall/2016-12-02/">Modeling dependence in bivariate multi-state processes: A frailty approach</a></h2>
    <h3 class="post-meta">Andrea Giussani · Dec 2, 2016 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2016-12-02">Date: 2016-12-02</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-1205">Location: BURN 1205</h4>
<h2 id="abstract">Abstract:</h2>
<p>The aim of this talk is to present a statistical framework for the analysis of dependent bivariate multistate processes, allowing one to study the dependence both across subjects in a pair and among individual-specific events. As for the latter, copula- based models are employed, whereas dependence between multi-state models can be accomplished by means of frailties. The well known Marshall-Olkin Bivariate Exponential Distribution (MOBVE) is considered for the joint distribution of frailties. The reason is twofold: on the one hand, it allows one to model shocks that affect the two individual-specific frailties; on the other hand, the MOBVE is the only bivariate exponential distribution with exponential marginals, which allows for the modeling of each multi-state process as a shared frailty model. We first discuss a frailty bivariate survival model with some new results, and then move to the construction of the frailty bivariate multi-state model, with the corresponding observed data likelihood maximization estimating procedure in presence of right censoring. The last part of the talk will be dedicated to some open problems related to the modeling of multiple multi-state processes in presence of Marshall-Olkin type copulas.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2016fall/2016-12-02/" title="Modeling dependence in bivariate multi-state processes: A frailty approach">Read More…</a>
  </footer>
  
</article>
</li>
      
      <li><article class="li">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2016-12-01T00:00:00JST">Dec 1, 2016</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="http://localhost:4321/post/">post</a></li>
      
    </ul>
    <h2 class="title"><a href="http://localhost:4321/post/2016fall/2016-12-01/">High-dimensional changepoint estimation via sparse projection</a></h2>
    <h3 class="post-meta">Richard Samworth · Dec 1, 2016 </h3>
  </header>

  
  <div class="summary"><h4 id="date-2016-12-01">Date: 2016-12-01</h4>
<h4 id="time-1530-1630">Time: 15:30-16:30</h4>
<h4 id="location-burn-708">Location: BURN 708</h4>
<h2 id="abstract">Abstract:</h2>
<p>Changepoints are a very common feature of Big Data that arrive in the form of a data stream. We study high-dimensional time series in which, at certain time points, the mean structure changes in a sparse subset of the coordinates. The challenge is to borrow strength across the coordinates in order to detect smaller changes than could be observed in any individual component series. We propose a two-stage procedure called &lsquo;inspect&rsquo; for estimation of the changepoints: first, we argue that a good projection direction can be obtained as the leading left singular vector of the matrix that solves a convex optimisation problem derived from the CUSUM transformation of the time series. We then apply an existing univariate changepoint detection algorithm to the projected series. Our theory provides strong guarantees on both the number of estimated changepoints and the rates of convergence of their locations, and our numerical studies validate its highly competitive empirical performance for a wide range of data generating mechanisms.</p></div>

  
  <footer>
    <a href="http://localhost:4321/post/2016fall/2016-12-01/" title="High-dimensional changepoint estimation via sparse projection">Read More…</a>
  </footer>
  
</article>
</li>
      
    </ul>

    
<nav>
  <ul class="pager">

    
    <li><a href="/post/page/16/">Previous</a></li>
    

    
    <li><a href="/post/page/18/">Next</a></li>
    

  </ul>
</nav>



  </div>
  <div class="col-md-4">
    
<aside class="l-sidebar">

  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">Recent Talks</div>
    </div>
    <div class="list-group">
      
      <a href="http://localhost:4321/tags/2024-fall/" class="list-group-item"> · Nov 15, 2024</a>
      
      <a href="http://localhost:4321/categories/" class="list-group-item"> · Nov 15, 2024</a>
      
      <a href="http://localhost:4321/categories/mcgill-statistics-seminar/" class="list-group-item"> · Nov 15, 2024</a>
      
      <a href="http://localhost:4321/post/2024fall/2024-11-15/" class="list-group-item">Simone Brugiapaglia · Nov 15, 2024</a>
      
      <a href="http://localhost:4321/tags/" class="list-group-item"> · Nov 15, 2024</a>
      
    </div>
  </section>

  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">CATEGORY</div>
    </div>
    <div class="list-group">
      
      <a href="http://localhost:4321/categories/mcgill-statistics-seminar" class="list-group-item">mcgill statistics seminar</a>
      
      <a href="http://localhost:4321/categories/crm-ssc-prize-address" class="list-group-item">crm-ssc prize address</a>
      
      <a href="http://localhost:4321/categories/crm-colloquium" class="list-group-item">crm-colloquium</a>
      
    </div>
  </section>
  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">TAG</div>
    </div>
    <div class="list-group">
      
      <a href="http://localhost:4321/tags/2024-winter" class="list-group-item">2024 winter</a>
      
      <a href="http://localhost:4321/tags/2024-fall" class="list-group-item">2024 fall</a>
      
      <a href="http://localhost:4321/tags/2023-winter" class="list-group-item">2023 winter</a>
      
      <a href="http://localhost:4321/tags/2023-summer" class="list-group-item">2023 summer</a>
      
      <a href="http://localhost:4321/tags/2023-fall" class="list-group-item">2023 fall</a>
      
      <a href="http://localhost:4321/tags/2022-winter" class="list-group-item">2022 winter</a>
      
      <a href="http://localhost:4321/tags/2022-fall" class="list-group-item">2022 fall</a>
      
      <a href="http://localhost:4321/tags/2021-winter" class="list-group-item">2021 winter</a>
      
      <a href="http://localhost:4321/tags/2021-fall" class="list-group-item">2021 fall</a>
      
      <a href="http://localhost:4321/tags/2020-winter" class="list-group-item">2020 winter</a>
      
      <a href="http://localhost:4321/tags/2020-fall" class="list-group-item">2020 fall</a>
      
      <a href="http://localhost:4321/tags/2019-winter" class="list-group-item">2019 winter</a>
      
      <a href="http://localhost:4321/tags/2019-fall" class="list-group-item">2019 fall</a>
      
      <a href="http://localhost:4321/tags/2018-winter" class="list-group-item">2018 winter</a>
      
      <a href="http://localhost:4321/tags/2018-fall" class="list-group-item">2018 fall</a>
      
      <a href="http://localhost:4321/tags/2017-winter" class="list-group-item">2017 winter</a>
      
      <a href="http://localhost:4321/tags/2017-fall" class="list-group-item">2017 fall</a>
      
      <a href="http://localhost:4321/tags/2016-winter" class="list-group-item">2016 winter</a>
      
      <a href="http://localhost:4321/tags/2016-fall" class="list-group-item">2016 fall</a>
      
      <a href="http://localhost:4321/tags/2015-winter" class="list-group-item">2015 winter</a>
      
      <a href="http://localhost:4321/tags/2015-fall" class="list-group-item">2015 fall</a>
      
      <a href="http://localhost:4321/tags/2014-winter" class="list-group-item">2014 winter</a>
      
      <a href="http://localhost:4321/tags/2014-fall" class="list-group-item">2014 fall</a>
      
      <a href="http://localhost:4321/tags/2013-winter" class="list-group-item">2013 winter</a>
      
      <a href="http://localhost:4321/tags/2013-fall" class="list-group-item">2013 fall</a>
      
      <a href="http://localhost:4321/tags/2012-winter" class="list-group-item">2012 winter</a>
      
      <a href="http://localhost:4321/tags/2012-fall" class="list-group-item">2012 fall</a>
      
      <a href="http://localhost:4321/tags/2011-fall" class="list-group-item">2011 fall</a>
      
    </div>
  </section>
  

</aside>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    
  </div>
</div>

      </div>
    </main>

    <footer class="l-footer">
      <div class="container">
        <p><span class="h-logo">&copy; McGill Statistics Seminars</span></p>
        <aside>
          <p><a href="http://www.mcgill.ca/mathstat/">Department of Mathematics and Statistics</a>.</p>
          <p><a href="https://www.mcgill.ca/">McGill University</a></p>
        </aside>
      </div>
    </footer>

    <script src="//code.jquery.com/jquery-3.1.1.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

