<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="pinterest" content="nopin">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta name="generator" content="Hugo 0.139.4">



<link rel="canonical" href="https://mcgillstat.github.io/post/2025winter/2025-04-04/">


    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/solarized_dark.min.css">
    <title>Normalization effects on deep neural networks and deep learning for scientific problems - McGill Statistics Seminars</title>
    
<meta name="description" content="&lt;h4 id=&#34;date-2025-04-04&#34;&gt;Date: 2025-04-04&lt;/h4&gt;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&lt;h4 id=&#34;httpsmcgillzoomusj81100654212httpsmcgillzoomusj81100654212&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81100654212&#34;&gt;https://mcgill.zoom.us/j/81100654212&lt;/a&gt;&lt;/h4&gt;&lt;h4 id=&#34;meeting-id-811-0065-4212&#34;&gt;Meeting ID: 811 0065 4212&lt;/h4&gt;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&lt;p&gt;We study the effect of normalization on the layers of deep neural networks. A given layer $i$ with $N_{i}$ hidden units is normalized by $1/N_{i}^{\gamma_{i}}$ with $\gamma_{i}\in[1/2,1]$. We study the effect of the choice of the $\gamma_{i}$ on the statistical behavior of the neural network’s output (such as variance) as well as on the test accuracy and generalization properties of the architecture.  We find that in terms of variance of the neural network’s output and test accuracy the best choice is to choose the $\gamma_{i}$’s to be equal to one, which is the mean-field scaling. We also find that this is particularly true for the outer layer, in that the neural network’s behavior is more sensitive in the scaling of the outer layer as opposed to the scaling of the inner layers. The mechanism for the mathematical analysis is an asymptotic expansion for the neural network’s output. An important practical consequence of the analysis is that it provides a systematic and mathematically informed way to choose the learning rate hyperparameters. Such a choice guarantees that the neural network behaves in a statistically robust way as the number of hidden units $N_i$ grow.  Time permitting, I will discuss applications of these ideas to design of deep learning algorithms for scientific problems including solving high dimensional partial differential equations (PDEs), closure of PDE models and reinforcement learning with applications to financial engineering, turbulence and more.&lt;/p&gt;">

<meta property="og:title" content="Normalization effects on deep neural networks and deep learning for scientific problems - McGill Statistics Seminars">
<meta property="og:type" content="article">
<meta property="og:url" content="https://mcgillstat.github.io/post/2025winter/2025-04-04/">
<meta property="og:image" content="https://mcgillstat.github.io/images/default.png">
<meta property="og:site_name" content="McGill Statistics Seminars">
<meta property="og:description" content="&lt;h4 id=&#34;date-2025-04-04&#34;&gt;Date: 2025-04-04&lt;/h4&gt;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&lt;h4 id=&#34;httpsmcgillzoomusj81100654212httpsmcgillzoomusj81100654212&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81100654212&#34;&gt;https://mcgill.zoom.us/j/81100654212&lt;/a&gt;&lt;/h4&gt;&lt;h4 id=&#34;meeting-id-811-0065-4212&#34;&gt;Meeting ID: 811 0065 4212&lt;/h4&gt;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&lt;p&gt;We study the effect of normalization on the layers of deep neural networks. A given layer $i$ with $N_{i}$ hidden units is normalized by $1/N_{i}^{\gamma_{i}}$ with $\gamma_{i}\in[1/2,1]$. We study the effect of the choice of the $\gamma_{i}$ on the statistical behavior of the neural network’s output (such as variance) as well as on the test accuracy and generalization properties of the architecture.  We find that in terms of variance of the neural network’s output and test accuracy the best choice is to choose the $\gamma_{i}$’s to be equal to one, which is the mean-field scaling. We also find that this is particularly true for the outer layer, in that the neural network’s behavior is more sensitive in the scaling of the outer layer as opposed to the scaling of the inner layers. The mechanism for the mathematical analysis is an asymptotic expansion for the neural network’s output. An important practical consequence of the analysis is that it provides a systematic and mathematically informed way to choose the learning rate hyperparameters. Such a choice guarantees that the neural network behaves in a statistically robust way as the number of hidden units $N_i$ grow.  Time permitting, I will discuss applications of these ideas to design of deep learning algorithms for scientific problems including solving high dimensional partial differential equations (PDEs), closure of PDE models and reinforcement learning with applications to financial engineering, turbulence and more.&lt;/p&gt;">
<meta property="og:locale" content="ja_JP">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="McGill Statistics Seminars">
<meta name="twitter:url" content="https://mcgillstat.github.io/post/2025winter/2025-04-04/">
<meta name="twitter:title" content="Normalization effects on deep neural networks and deep learning for scientific problems - McGill Statistics Seminars">
<meta name="twitter:description" content="&lt;h4 id=&#34;date-2025-04-04&#34;&gt;Date: 2025-04-04&lt;/h4&gt;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&lt;h4 id=&#34;httpsmcgillzoomusj81100654212httpsmcgillzoomusj81100654212&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81100654212&#34;&gt;https://mcgill.zoom.us/j/81100654212&lt;/a&gt;&lt;/h4&gt;&lt;h4 id=&#34;meeting-id-811-0065-4212&#34;&gt;Meeting ID: 811 0065 4212&lt;/h4&gt;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&lt;p&gt;We study the effect of normalization on the layers of deep neural networks. A given layer $i$ with $N_{i}$ hidden units is normalized by $1/N_{i}^{\gamma_{i}}$ with $\gamma_{i}\in[1/2,1]$. We study the effect of the choice of the $\gamma_{i}$ on the statistical behavior of the neural network’s output (such as variance) as well as on the test accuracy and generalization properties of the architecture.  We find that in terms of variance of the neural network’s output and test accuracy the best choice is to choose the $\gamma_{i}$’s to be equal to one, which is the mean-field scaling. We also find that this is particularly true for the outer layer, in that the neural network’s behavior is more sensitive in the scaling of the outer layer as opposed to the scaling of the inner layers. The mechanism for the mathematical analysis is an asymptotic expansion for the neural network’s output. An important practical consequence of the analysis is that it provides a systematic and mathematically informed way to choose the learning rate hyperparameters. Such a choice guarantees that the neural network behaves in a statistically robust way as the number of hidden units $N_i$ grow.  Time permitting, I will discuss applications of these ideas to design of deep learning algorithms for scientific problems including solving high dimensional partial differential equations (PDEs), closure of PDE models and reinforcement learning with applications to financial engineering, turbulence and more.&lt;/p&gt;">
<meta name="twitter:image" content="https://mcgillstat.github.io/images/default.png">


<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "NewsArticle",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id":"https:\/\/mcgillstat.github.io\/"
    },
    "headline": "Normalization effects on deep neural networks and deep learning for scientific problems - McGill Statistics Seminars",
    "image": {
      "@type": "ImageObject",
      "url": "https:\/\/mcgillstat.github.io\/images\/default.png",
      "height": 800,
      "width": 800
    },
    "datePublished": "2025-04-04T00:00:00JST",
    "dateModified": "2025-04-04T00:00:00JST",
    "author": {
      "@type": "Person",
      "name": "McGill Statistics Seminars"
    },
    "publisher": {
      "@type": "Organization",
      "name": "McGill Statistics Seminars",
      "logo": {
        "@type": "ImageObject",
        "url": "https:\/\/mcgillstat.github.io\/images/logo.png",
        "width": 600,
        "height": 60
      }
    },
    "description": "\u003ch4 id=\u0022date-2025-04-04\u0022\u003eDate: 2025-04-04\u003c\/h4\u003e\n\u003ch4 id=\u0022time-1530-1630-montreal-time\u0022\u003eTime: 15:30-16:30 (Montreal time)\u003c\/h4\u003e\n\u003ch4 id=\u0022location-in-person-burnside-1104\u0022\u003eLocation: In person, Burnside 1104\u003c\/h4\u003e\n\u003ch4 id=\u0022httpsmcgillzoomusj81100654212httpsmcgillzoomusj81100654212\u0022\u003e\u003ca href=\u0022https:\/\/mcgill.zoom.us\/j\/81100654212\u0022\u003ehttps:\/\/mcgill.zoom.us\/j\/81100654212\u003c\/a\u003e\u003c\/h4\u003e\n\u003ch4 id=\u0022meeting-id-811-0065-4212\u0022\u003eMeeting ID: 811 0065 4212\u003c\/h4\u003e\n\u003ch4 id=\u0022passcode-none\u0022\u003ePasscode: None\u003c\/h4\u003e\n\u003ch2 id=\u0022abstract\u0022\u003eAbstract:\u003c\/h2\u003e\n\u003cp\u003eWe study the effect of normalization on the layers of deep neural networks. A given layer $i$ with $N_{i}$ hidden units is normalized by $1\/N_{i}^{\\gamma_{i}}$ with $\\gamma_{i}\\in[1\/2,1]$. We study the effect of the choice of the $\\gamma_{i}$ on the statistical behavior of the neural network’s output (such as variance) as well as on the test accuracy and generalization properties of the architecture.  We find that in terms of variance of the neural network’s output and test accuracy the best choice is to choose the $\\gamma_{i}$’s to be equal to one, which is the mean-field scaling. We also find that this is particularly true for the outer layer, in that the neural network’s behavior is more sensitive in the scaling of the outer layer as opposed to the scaling of the inner layers. The mechanism for the mathematical analysis is an asymptotic expansion for the neural network’s output. An important practical consequence of the analysis is that it provides a systematic and mathematically informed way to choose the learning rate hyperparameters. Such a choice guarantees that the neural network behaves in a statistically robust way as the number of hidden units $N_i$ grow.  Time permitting, I will discuss applications of these ideas to design of deep learning algorithms for scientific problems including solving high dimensional partial differential equations (PDEs), closure of PDE models and reinforcement learning with applications to financial engineering, turbulence and more.\u003c\/p\u003e"
  }
</script>


    <link href="https://mcgillstat.github.io/css/styles.css" rel="stylesheet">
    

  </head>

  <body>
    
    
    

    <header class="l-header">
      <nav class="navbar navbar-default">
        <div class="container">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://mcgillstat.github.io/">McGill Statistics Seminars</a>
          </div>

          
          <div id="navbar" class="collapse navbar-collapse">
            
            <ul class="nav navbar-nav navbar-right">
              
              
              <li><a href="/">Current Seminar Series</a></li>
              
              
              
              <li><a href="/post/">Past Seminar Series</a></li>
              
              
            </ul>
            
          </div>
          

        </div>
      </nav>
    </header>

    <main>
      <div class="container">
        
<div class="row">
  <div class="col-md-8">

    <nav class="p-crumb">
      <ol class="breadcrumb">
        <li><a href="https://mcgillstat.github.io/"><i class="fa fa-home" aria-hidden="true"></i></a></li>
        
        <li itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb"><a href="https://mcgillstat.github.io/post/" itemprop="url"><span itemprop="title">post</span></a></li>
        
        <li class="active">Konstantinos Spiliopoulos</li>
      </ol>
    </nav>

    <article class="single">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2025-04-04T00:00:00JST">Apr 4, 2025</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="https://mcgillstat.github.io/post/">post</a></li>
      
    </ul>

    <h2 class="title">Normalization effects on deep neural networks and deep learning for scientific problems</h1>
    <h3 class="post-meta">Konstantinos Spiliopoulos </h3>
    
  </header>

  

  <div class="article-body"><h4 id="date-2025-04-04">Date: 2025-04-04</h4>
<h4 id="time-1530-1630-montreal-time">Time: 15:30-16:30 (Montreal time)</h4>
<h4 id="location-in-person-burnside-1104">Location: In person, Burnside 1104</h4>
<h4 id="httpsmcgillzoomusj81100654212httpsmcgillzoomusj81100654212"><a href="https://mcgill.zoom.us/j/81100654212">https://mcgill.zoom.us/j/81100654212</a></h4>
<h4 id="meeting-id-811-0065-4212">Meeting ID: 811 0065 4212</h4>
<h4 id="passcode-none">Passcode: None</h4>
<h2 id="abstract">Abstract:</h2>
<p>We study the effect of normalization on the layers of deep neural networks. A given layer $i$ with $N_{i}$ hidden units is normalized by $1/N_{i}^{\gamma_{i}}$ with $\gamma_{i}\in[1/2,1]$. We study the effect of the choice of the $\gamma_{i}$ on the statistical behavior of the neural network’s output (such as variance) as well as on the test accuracy and generalization properties of the architecture.  We find that in terms of variance of the neural network’s output and test accuracy the best choice is to choose the $\gamma_{i}$’s to be equal to one, which is the mean-field scaling. We also find that this is particularly true for the outer layer, in that the neural network’s behavior is more sensitive in the scaling of the outer layer as opposed to the scaling of the inner layers. The mechanism for the mathematical analysis is an asymptotic expansion for the neural network’s output. An important practical consequence of the analysis is that it provides a systematic and mathematically informed way to choose the learning rate hyperparameters. Such a choice guarantees that the neural network behaves in a statistically robust way as the number of hidden units $N_i$ grow.  Time permitting, I will discuss applications of these ideas to design of deep learning algorithms for scientific problems including solving high dimensional partial differential equations (PDEs), closure of PDE models and reinforcement learning with applications to financial engineering, turbulence and more.</p>
<h2 id="speaker">Speaker</h2>
<p>Konstantinos Spiliopoulos is the Director of Statistics and a Professor of Mathematics and Statistics at Boston University  (BU). He is a member of the Hariri Institute for Computing and of the Center for Information and Systems Engineering at BU. Between 2009-2012, he was a Prager Assistant Professor at the Division of Applied Mathematics at Brown University, and he earned his PhD in Mathematical Statistics at University of Maryland in 2009. He has been at BU since 2012. Currently, Professor Spiliopoulos works on mathematical and computational methods for machine learning algorithms, stochastic online algorithms and deep learning for scientific problems, accelerated Monte Carlo methods for sampling and optimization, agent-based modeling and applications to financial engineering, biological systems and opinion and social dynamics. He has received the Stochastics and Dynamics Best Paper Award in 2023, the  Simons Fellow in Mathematics Award in 2020, the NSF Career Award in 2016, the Hariri Institute Junior Fellowship Award in 2013, the Seymour Goldberg Paper Award in 2008 and the Eygenidio Foundation Award in 2004.  His work has been continuously funded by NSF, Simons Foundations and the DoD. He has co-authored the book &ldquo;Mathematical Foundations of Deep Learning&rdquo; to appear later in 2025.</p>
</div>

  <footer class="article-footer">
    
    
    
    <section class="bordered">
      <header>
        <div class="panel-title">CATEGORIES</div>
      </header>
      <div>
        <ul class="p-terms">
          
          <li><a href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar/">McGill Statistics Seminar</a></li>
          
        </ul>
      </div>
    </section>
    
    
    
    <section class="bordered">
      <header>
        <div class="panel-title">TAGS</div>
      </header>
      <div>
        <ul class="p-terms">
          
          <li><a href="https://mcgillstat.github.io/tags/2025-winter/">2025 Winter</a></li>
          
        </ul>
      </div>
    </section>
    
    
  </footer>

</article>


    
  </div>

  <div class="col-md-4">
    
<aside class="l-sidebar">

  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">Recent Talks</div>
    </div>
    <div class="list-group">
      
      <a href="https://mcgillstat.github.io/tags/2025-fall/" class="list-group-item"> · Oct 3, 2025</a>
      
      <a href="https://mcgillstat.github.io/categories/" class="list-group-item"> · Oct 3, 2025</a>
      
      <a href="https://mcgillstat.github.io/post/2025fall/2025-10-03/" class="list-group-item">Rachel Morris · Oct 3, 2025</a>
      
      <a href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar/" class="list-group-item"> · Oct 3, 2025</a>
      
      <a href="https://mcgillstat.github.io/tags/" class="list-group-item"> · Oct 3, 2025</a>
      
    </div>
  </section>

  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">CATEGORY</div>
    </div>
    <div class="list-group">
      
      <a href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar" class="list-group-item">mcgill statistics seminar</a>
      
      <a href="https://mcgillstat.github.io/categories/crm-ssc-prize-address" class="list-group-item">crm-ssc prize address</a>
      
      <a href="https://mcgillstat.github.io/categories/crm-colloquium" class="list-group-item">crm-colloquium</a>
      
    </div>
  </section>
  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">TAG</div>
    </div>
    <div class="list-group">
      
      <a href="https://mcgillstat.github.io/tags/2025-winter" class="list-group-item">2025 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2025-fall" class="list-group-item">2025 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2024-winter" class="list-group-item">2024 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2024-fall" class="list-group-item">2024 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2023-winter" class="list-group-item">2023 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2023-summer" class="list-group-item">2023 summer</a>
      
      <a href="https://mcgillstat.github.io/tags/2023-fall" class="list-group-item">2023 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2022-winter" class="list-group-item">2022 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2022-fall" class="list-group-item">2022 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2021-winter" class="list-group-item">2021 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2021-fall" class="list-group-item">2021 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2020-winter" class="list-group-item">2020 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2020-fall" class="list-group-item">2020 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2019-winter" class="list-group-item">2019 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2019-fall" class="list-group-item">2019 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2018-winter" class="list-group-item">2018 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2018-fall" class="list-group-item">2018 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2017-winter" class="list-group-item">2017 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2017-fall" class="list-group-item">2017 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2016-winter" class="list-group-item">2016 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2016-fall" class="list-group-item">2016 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2015-winter" class="list-group-item">2015 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2015-fall" class="list-group-item">2015 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2014-winter" class="list-group-item">2014 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2014-fall" class="list-group-item">2014 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2013-winter" class="list-group-item">2013 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2013-fall" class="list-group-item">2013 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2012-winter" class="list-group-item">2012 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2012-fall" class="list-group-item">2012 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2011-fall" class="list-group-item">2011 fall</a>
      
    </div>
  </section>
  

</aside>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    
  </div>
</div>

      </div>
    </main>

    <footer class="l-footer">
      <div class="container">
        <p><span class="h-logo">&copy; McGill Statistics Seminars</span></p>
        <aside>
          <p><a href="http://www.mcgill.ca/mathstat/">Department of Mathematics and Statistics</a>.</p>
          <p><a href="https://www.mcgill.ca/">McGill University</a></p>
        </aside>
      </div>
    </footer>

    <script src="//code.jquery.com/jquery-3.1.1.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

