<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Past Seminar Series on McGill Statistics Seminars</title>
    <link>/post/</link>
    <description>Recent content in Past Seminar Series on McGill Statistics Seminars</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Sep 2012 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Quantile LASSO in Nonparametric Models with Changepoints Under Optional Shape Constraints</title>
      <link>/post/2018fall/2018-09-14/</link>
      <pubDate>Fri, 14 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-09-14/</guid>
      <description>Date: 2018-09-14 Time: 15:30-16:30 Location: BURN 1104 Abstract: Nonparametric models are popular modeling tools because of their natural overall flexibility. In our approach, we apply nonparametric techniques for panel data structures with changepoints and optional shape constraints and the estimation is performed in a fully data driven manner by utilizing atomic pursuit methods – LASSO regularization techniques in particular. However, in order to obtain robust estimates and, also, to have a more complex insight into the underlying data structure, we target conditional quantiles rather then the conditional mean only.</description>
    </item>
    
    <item>
      <title>Association Measures for Clustered Competing Risks Data</title>
      <link>/post/2018fall/2018-09-07/</link>
      <pubDate>Fri, 07 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018fall/2018-09-07/</guid>
      <description>Date: 2018-09-07 Time: 15:30-16:30 Location: BURN 1104 Abstract: In this work, we propose a semiparametric model for multivariate clustered competing risks data when the cause-specific failure times and the occurrence of competing risk events among subjects within the same cluster are of interest. The cause-specific hazard functions are assumed to follow Cox proportional hazard models, and the associations between failure times given the same or different cause events and the associations between occurrences of competing risk events within the same cluster are investigated through copula models.</description>
    </item>
    
    <item>
      <title>Methodological challenges in using point-prevalence versus cohort data in risk factor analyses of hospital-acquired infections</title>
      <link>/post/2018winter/2018-04-27/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-04-27/</guid>
      <description>Date: 2018-04-27 Time: 15:30-16:30 Location: BURN 1205 Abstract: To explore the impact of length-biased sampling on the evaluation of risk factors of nosocomial infections in point-prevalence studies. We used cohort data with full information including the exact date of the nosocomial infection and mimicked an artificial one-day prevalence study by picking a sample from this cohort study. Based on the cohort data, we studied the underlying multi-state model which accounts for nosocomial infection as an intermediate and discharge/death as competing events.</description>
    </item>
    
    <item>
      <title>Kernel Nonparametric Overlap-based Syncytial Clustering</title>
      <link>/post/2018winter/2018-04-20/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-04-20/</guid>
      <description>Date: 2018-04-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Standard clustering algorithms can find regular-structured clusters such as ellipsoidally- or spherically-dispersed groups, but are more challenged with groups lacking formal structure or definition. Syncytial clustering is the name that we introduce for methods that merge groups obtained from standard clustering algorithms in order to reveal complex group structure in the data. Here, we develop a distribution-free fully-automated syncytial algorithm that can be used with the computationally efficient k-means or other algorithms.</description>
    </item>
    
    <item>
      <title>Empirical likelihood and robust regression in diffusion tensor imaging data analysis</title>
      <link>/post/2018winter/2018-04-06/</link>
      <pubDate>Fri, 06 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-04-06/</guid>
      <description>Date: 2018-04-06 Time: 15:30-16:30 Location: BURN 1205 Abstract: With modern technology development, functional responses are observed frequently in various scientific fields including neuroimaging data analysis. Empirical likelihood as a nonparametric data-driven technique has become an important statistical inference methodology. In this paper, motivated by diffusion tensor imaging (DTI) data we propose three generalized empirical likelihood-based methods that accommodate within-curve dependence on the varying coefficient model with functional responses and embed a robust regression idea.</description>
    </item>
    
    <item>
      <title>Some development on dynamic computer experiments</title>
      <link>/post/2018winter/2018-03-23/</link>
      <pubDate>Fri, 23 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-03-23/</guid>
      <description>Date: 2018-03-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: Computer experiments refer to the study of real systems using complex simulation models. They have been widely used as efficient, economical alternatives to physical experiments. Computer experiments with time series outputs are called dynamic computer experiments. In this talk, we consider two problems of such experiments: emulation of large-scale dynamic computer experiments and inverse problem. For the first problem, we proposed a computationally efficient modelling approach which sequentially finds a set of local design points based on a new criterion specifically designed for emulating dynamic computer simulators.</description>
    </item>
    
    <item>
      <title>Statistical Genomics for Understanding Complex Traits</title>
      <link>/post/2018winter/2018-03-16/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-03-16/</guid>
      <description>Date: 2018-03-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: Over the last decade, advances in measurement technologies has enabled researchers to generate multiple types of high-dimensional &amp;ldquo;omics&amp;rdquo; datasets for large cohorts. These data provide an opportunity to derive a mechanistic understanding of human complex traits. However, inferring meaningful biological relationships from these data is challenging due to high-dimensionality , noise, and abundance of confounding factors. In this talk, I&amp;rsquo;ll describe statistical approaches for robust analysis of genomic data from large population studies, with a focus on 1) understanding the nature of confounding and approaches for addressing them and 2) understanding the genomic correlates of aging and dementia.</description>
    </item>
    
    <item>
      <title>Sparse Penalized Quantile Regression: Method, Theory, and Algorithm</title>
      <link>/post/2018winter/2018-02-23/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-02-23/</guid>
      <description>Date: 2018-02-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: Sparse penalized quantile regression is a useful tool for variable selection, robust estimation, and heteroscedasticity detection in high-dimensional data analysis. We discuss the variable selection and estimation properties of the lasso and folded concave penalized quantile regression via non-asymptotic arguments. We also consider consistent parameter tuning therein. The computational issue of the sparse penalized quantile regression has not yet been fully resolved in the literature, due to non-smoothness of the quantile regression loss function.</description>
    </item>
    
    <item>
      <title>The Law of Large Populations: The return of the long-ignored N and how it can affect our 2020 vision</title>
      <link>/post/2018winter/2018-02-16/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-02-16/</guid>
      <description>Date: 2018-02-16 Time: 15:30-16:30 Location: McGill University, OTTO MAASS 217 Abstract: For over a century now, we statisticians have successfully convinced ourselves and almost everyone else, that in statistical inference the size of the population N can be ignored, especially when it is large. Instead, we focused on the size of the sample, n, the key driving force for both the Law of Large Numbers and the Central Limit Theorem.</description>
    </item>
    
    <item>
      <title>Methodological considerations for the analysis of relative treatment effects in multi-drug-resistant tuberculosis from fused observational studies</title>
      <link>/post/2018winter/2018-02-09/</link>
      <pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-02-09/</guid>
      <description>Date: 2018-02-09 Time: 15:30-16:30 Location: BURN 1205 Abstract: Multi-drug-resistant tuberculosis (MDR-TB) is defined as strains of tuberculosis that do not respond to at least the two most used anti-TB drugs. After diagnosis, the intensive treatment phase for MDR-TB involves taking several alternative antibiotics concurrently. The Collaborative Group for Meta-analysis of Individual Patient Data in MDR-TB has assembled a large, fused dataset of over 30 observational studies comparing the effectiveness of 15 antibiotics.</description>
    </item>
    
    <item>
      <title>A new approach to model financial data: The Factorial Hidden Markov Volatility Model</title>
      <link>/post/2018winter/2018-02-02/</link>
      <pubDate>Fri, 02 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-02-02/</guid>
      <description>Date: 2018-02-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: A new process, the factorial hidden Markov volatility (FHMV) model, is proposed to model financial returns or realized variances. This process is constructed based on a factorial hidden Markov model structure and corresponds to a parsimoniously parametrized hidden Markov model that includes thousands of volatility states. The transition probability matrix of the underlying Markov chain is structured so that the multiplicity of its second largest eigenvalue can be greater than one.</description>
    </item>
    
    <item>
      <title>Back to the future: why I think REGRESSION is the new black in genetic association studies</title>
      <link>/post/2018winter/2018-01-26/</link>
      <pubDate>Fri, 26 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-01-26/</guid>
      <description>Date: 2018-01-26 Time: 15:30-16:30 Location: ROOM 6254 Pavillon Andre-Aisenstadt 2920, UdeM Abstract: Linear regression remains an important framework in the era of big and complex data. In this talk I present some recent examples where we resort to the classical simple linear regression model and its celebrated extensions in novel settings. The Eureka moment came while reading Wu and Guan&amp;rsquo;s (2015) comments on our generalized Kruskal-Wallis (GKW) test (Elif Acar and Sun 2013, Biometrics).</description>
    </item>
    
    <item>
      <title>Generalized Sparse Additive Models</title>
      <link>/post/2018winter/2018-01-19/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-01-19/</guid>
      <description>Date: 2018-01-19 Time: 15:30-16:30 Location: BURN 1205 Abstract: I will present a unified approach to the estimation of generalized sparse additive models in high dimensional regression problems. Our approach is based on combining structure-inducing and sparsity penalties in a single regression problem. It allows for the use of a large family of structure-inducing penalties: Those characterized by semi-norm constraints. This includes finite dimensional linear subspaces, sobolev and holder classes, classes with bounded total variation, among others.</description>
    </item>
    
    <item>
      <title>Modelling RNA stability for decoding the regulatory programs that drive human diseases</title>
      <link>/post/2018winter/2018-01-12/</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018winter/2018-01-12/</guid>
      <description>Date: 2018-01-12 Time: 15:30-16:30 Location: BURN 1205 Abstract: The key determinant of the identity and behaviour of the cell is gene regulation, i.e. which genes are active and which genes are inactive in a particular cell. One of the least understood aspects of gene regulation is RNA stability: genes produce RNA molecules to carry their genetic information – the more stable these RNA molecules are, the longer they can function within the cell, and the less stable they are, the more rapidly they are removed from the pool of active molecules.</description>
    </item>
    
    <item>
      <title>Fisher’s method revisited: set-based genetic association and interaction studies</title>
      <link>/post/2017fall/2017-12-01/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-12-01/</guid>
      <description>Date: 2017-12-01 Time: 15:30-16:30 Location: BURN 1205 Abstract: Fisher’s method, also known as Fisher’s combined probability test, is commonly used in meta-analyses to combine p-values from the same test applied to K independent samples to evaluate a common null hypothesis. Here we propose to use it to combine p-values from different tests applied to the same sample in two settings: when jointly analyzing multiple genetic variants in set-based genetic association studies, or when jointly capturing main and interaction effects in the presence of missing one of the interacting variables.</description>
    </item>
    
    <item>
      <title>150 years (and more) of data analysis in Canada</title>
      <link>/post/2017fall/2017-11-24/</link>
      <pubDate>Fri, 24 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-11-24/</guid>
      <description>Date: 2017-11-24 Time: 15:30-16:30 Location: LEA 232 Abstract: As Canada celebrates its 150th anniversary, it may be good to reflect on the past and future of data analysis and statistics in this country. In this talk, I will review the Victorian Statistics Movement and its effect in Canada, data analysis by a Montréal physician in the 1850s, a controversy over data analysis in the 1850s and 60s centred in Montréal, John A.</description>
    </item>
    
    <item>
      <title>A log-linear time algorithm for constrained changepoint detection</title>
      <link>/post/2017fall/2017-11-17/</link>
      <pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-11-17/</guid>
      <description>Date: 2017-11-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: Changepoint detection is a central problem in time series and genomic data. For some applications, it is natural to impose constraints on the directions of changes. One example is ChIP-seq data, for which adding an up-down constraint improves peak detection accuracy, but makes the optimization problem more complicated. In this talk I will explain how a recently proposed functional pruning algorithm can be generalized to solve such constrained changepoint detection problems.</description>
    </item>
    
    <item>
      <title>PAC-Bayesian Generalizations Bounds for Deep Neural Networks</title>
      <link>/post/2017fall/2017-11-10/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-11-10/</guid>
      <description>Date: 2017-11-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this &amp;ldquo;deep learning&amp;rdquo; regime.</description>
    </item>
    
    <item>
      <title>How to do statistics</title>
      <link>/post/2017fall/2017-11-03/</link>
      <pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-11-03/</guid>
      <description>Date: 2017-11-03 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, I will outline how to do (Bayesian) statistics. I will focus particularly on the things that need to be done before you see data, including prior specification and checking that your inference algorithm actually works.
Speaker Daniel Simpson is an Assistant Professor in the Department of Statistical Sciences, University of Toronto</description>
    </item>
    
    <item>
      <title>Penalized robust regression estimation with applications to proteomics</title>
      <link>/post/2017fall/2017-10-27/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-10-27/</guid>
      <description>Date: 2017-10-27 Time: 15:30-16:30 Location: BURN 1205 Abstract: In many current applications, scientists can easily measure a very large number of variables (for example, hundreds of protein levels), some of which are expected be useful to explain or predict a specific response variable of interest. These potential explanatory variables are most likely to contain redundant or irrelevant information, and in many cases, their quality and reliability may be suspect. We developed two penalized robust regression estimators that can be used to identify a useful subset of explanatory variables to predict the response, while protecting the resulting estimator against possible aberrant observations in the data set.</description>
    </item>
    
    <item>
      <title>Statistical optimization and nonasymptotic robustness</title>
      <link>/post/2017fall/2017-10-20/</link>
      <pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-10-20/</guid>
      <description>Date: 2017-10-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Statistical optimization has generated quite some interest recently. It refers to the case where hidden and local convexity can be discovered in most cases for nonconvex problems, making polynomial algorithms possible. It relies on a careful analysis of the geometry near global optima. In this talk, I will explore this issue by focusing on sparse regression problems in high dimensions. A computational framework named iterative local adaptive majorize-minimization (I-LAMM) will be proposed to simultaneously control algorithmic complexity and statistical error.</description>
    </item>
    
    <item>
      <title>Quantifying spatial flood risks: A comparative study of max-stable models</title>
      <link>/post/2017fall/2017-10-13/</link>
      <pubDate>Fri, 13 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-10-13/</guid>
      <description>Date: 2017-10-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: In various applications, evaluating spatial risks (such as floods, heatwaves or storms) is a key problem. The aim of this talk is to make use of extreme value theory and max-stable processes to provide quantitative answers to this issue. A review of the literature will be provided, as well as a wide comparative study based on a simulation design mimicking daily rainfall in France.</description>
    </item>
    
    <item>
      <title>McNeil: Spectral backtests of forecast distributions with application to risk management | Jasiulis-Goldyn: Asymptotic properties and renewal theory for Kendall random walks</title>
      <link>/post/2017fall/2017-09-29/</link>
      <pubDate>Fri, 29 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-09-29/</guid>
      <description>Date: 2017-09-29 Time: 14:30-16:30 Location: BURN 1205 Abstract: McNeil: In this talk we study a class of backtests for forecast distributions in which the test statistic is a spectral transformation that weights exceedance events by a function of the modelled probability level. The choice of the kernel function makes explicit the user’s priorities for model performance. The class of spectral backtests includes tests of unconditional coverage and tests of conditional coverage.</description>
    </item>
    
    <item>
      <title>BET on independence</title>
      <link>/post/2017fall/2017-09-22/</link>
      <pubDate>Fri, 22 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-09-22/</guid>
      <description>Date: 2017-09-22 Time: 14:00-15:00 Location: BRONF179 Abstract: We study the problem of nonparametric dependence detection. Many existing methods suffer severe power loss due to non-uniform consistency, which we illustrate with a paradox. To avoid such power loss, we approach the nonparametric test of independence through the new framework of binary expansion statistics (BEStat) and binary expansion testing (BET), which examine dependence through a filtration induced by marginal binary expansions. Through a novel decomposition of the likelihood of contingency tables whose sizes are powers of 2, we show that the interactions of binary variables in the filtration are complete sufficient statistics for dependence.</description>
    </item>
    
    <item>
      <title>Our quest for robust time series forecasting at scale</title>
      <link>/post/2017fall/2017-09-15/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-09-15/</guid>
      <description>Date: 2017-09-15 Time: 15:30-16:30 Location: BURN 1205 Abstract: The demand for time series forecasting at Google has grown rapidly along with the company since its founding. Initially, the various business and engineering needs led to a multitude of forecasting approaches, most reliant on direct analyst support. The volume and variety of the approaches, and in some cases their inconsistency, called out for an attempt to unify, automate, and extend forecasting methods, and to distribute the results via tools that could be deployed reliably across the company.</description>
    </item>
    
    <item>
      <title>Genomics like it&#39;s 1960: Inferring human history</title>
      <link>/post/2017fall/2017-09-08/</link>
      <pubDate>Fri, 08 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017fall/2017-09-08/</guid>
      <description>Date: 2017-09-08 Time: 15:30-16:30 Location: BURN 1205 Abstract: A central goal of population genetics is the inference of the biological, evolutionary and demographic forces that shaped human diversity. Large-scale sequencing experiments provide fantastic opportunities to learn about human history and biology if we can overcome computational and statistical challenges. I will discuss how simple mid-century statistical approaches, such as the jackknife and Kolmogorov equations, can be combined in unexpected ways to solve partial differential equations, optimize genomic study design, and learn about the spread of modern humans since our common African origins.</description>
    </item>
    
    <item>
      <title>Instrumental Variable Regression with Survival Outcomes</title>
      <link>/post/2017winter/2017-04-06/</link>
      <pubDate>Thu, 06 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-04-06/</guid>
      <description>Date: 2017-04-06 Time: 15:30-16:30 Location: Universite Laval, Pavillon Vachon, Salle 3840 Abstract: Instrumental variable (IV) methods are popular in non-experimental studies to estimate the causal effects of medical interventions or exposures. These approaches allow for the consistent estimation of such effects even if important confounding factors are unobserved. Despite the increasing use of these methods, there have been few extensions of IV methods to censored data regression problems. We discuss challenges in applying IV structural equational modelling techniques to the proportional hazards model and suggest alternative modelling frameworks.</description>
    </item>
    
    <item>
      <title>Distributed kernel regression for large-scale data</title>
      <link>/post/2017winter/2017-03-31/</link>
      <pubDate>Fri, 31 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-03-31/</guid>
      <description>Date: 2017-03-31 Time: 15:30-16:30 Location: BURN 1205 Abstract: In modern scientific research, massive datasets with huge numbers of observations are frequently encountered. To facilitate the computational process, a divide-and-conquer scheme is often used for the analysis of big data. In such a strategy, a full dataset is first split into several manageable segments; the final output is then aggregated from the individual outputs of the segments. Despite its popularity in practice, it remains largely unknown that whether such a distributive strategy provides valid theoretical inferences to the original data; if so, how efficient does it work?</description>
    </item>
    
    <item>
      <title>Bayesian sample size determination for clinical trials</title>
      <link>/post/2017winter/2017-03-24/</link>
      <pubDate>Fri, 24 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-03-24/</guid>
      <description>Date: 2017-03-24 Time: 15:30-16:30 Location: BURN 1205 Abstract: Sample size determination problem is an important task in the planning of clinical trials. The problem may be formulated formally in statistical terms. The most frequently used methods are based on the required size, and power of the trial for a specified treatment effect. In contrast to the Bayesian decision-theoretic approach, there is no explicit balancing of the cost of a possible increase in the size of the trial against the benefit of the more accurate information which it would give.</description>
    </item>
    
    <item>
      <title>Inference in dynamical systems</title>
      <link>/post/2017winter/2017-03-17/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-03-17/</guid>
      <description>Date: 2017-03-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: We consider the asymptotic consistency of maximum likelihood parameter estimation for dynamical systems observed with noise. Under suitable conditions on the dynamical systems and the observations, we show that maximum likelihood parameter estimation is consistent. Furthermore, we show how some well-studied properties of dynamical systems imply the general statistical properties related to maximum likelihood estimation. Finally, we exhibit classical families of dynamical systems for which maximum likelihood estimation is consistent.</description>
    </item>
    
    <item>
      <title>High-throughput single-cell biology: The challenges and opportunities for machine learning scientists</title>
      <link>/post/2017winter/2017-03-10/</link>
      <pubDate>Fri, 10 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-03-10/</guid>
      <description>Date: 2017-03-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: The immune system does a lot more than killing “foreign” invaders. It’s a powerful sensory system that can detect stress levels, infections, wounds, and even cancer tumors. However, due to the complex interplay between different cell types and signaling pathways, the amount of data produced to characterize all different aspects of the immune system (tens of thousands of genes measured and hundreds of millions of cells, just from a single patient) completely overwhelms existing bioinformatics tools.</description>
    </item>
    
    <item>
      <title>The first pillar of statistical wisdom</title>
      <link>/post/2017winter/2017-02-24/</link>
      <pubDate>Fri, 24 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-02-24/</guid>
      <description>Date: 2017-02-24 Time: 15:30-16:30 Location: BURN 1205 Abstract: This talk will provide an introduction to the first of the pillars in Stephen Stigler&amp;rsquo;s 2016 book The Seven Pillars of Statistical Wisdom, namely “Aggregation.” It will focus on early instances of the sample mean in scientific work, on the early error distributions, and on how their “centres” were fitted.
Speaker James A. Hanley is a Professor in the Department of Epidemiology, Biostatistics and Occupational Health, at McGill University.</description>
    </item>
    
    <item>
      <title>Building end-to-end dialogue systems using deep neural architectures</title>
      <link>/post/2017winter/2017-02-17/</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-02-17/</guid>
      <description>Date: 2017-02-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: The ability for a computer to converse in a natural and coherent manner with a human has long been held as one of the important steps towards solving artificial intelligence. In this talk I will present recent results on building dialogue systems from large corpuses using deep neural architectures. I will highlight several challenges related to data acquisition, algorithmic development, and performance evaluation.</description>
    </item>
    
    <item>
      <title>Sparse envelope model: Efficient estimation and response variable selection in multivariate linear regression</title>
      <link>/post/2017winter/2017-02-10/</link>
      <pubDate>Fri, 10 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-02-10/</guid>
      <description>Date: 2017-02-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: The envelope model is a method for efficient estimation in multivariate linear regression. In this article, we propose the sparse envelope model, which is motivated by applications where some response variables are invariant to changes of the predictors and have zero regression coefficients. The envelope estimator is consistent but not sparse, and in many situations it is important to identify the response variables for which the regression coefficients are zero.</description>
    </item>
    
    <item>
      <title>MM algorithms for variance component models</title>
      <link>/post/2017winter/2017-02-03/</link>
      <pubDate>Fri, 03 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-02-03/</guid>
      <description>Date: 2017-02-03 Time: 15:30-16:30 Location: BURN 1205 Abstract: Variance components estimation and mixed model analysis are central themes in statistics with applications in numerous scientific disciplines. Despite the best efforts of generations of statisticians and numerical analysts, maximum likelihood estimation and restricted maximum likelihood estimation of variance component models remain numerically challenging. In this talk, we present a novel iterative algorithm for variance components estimation based on the minorization-maximization (MM) principle.</description>
    </item>
    
    <item>
      <title>Bayesian inference for conditional copula models</title>
      <link>/post/2017winter/2017-01-27/</link>
      <pubDate>Fri, 27 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-01-27/</guid>
      <description>Date: 2017-01-27 Time: 15:30-16:30 Location: ROOM 6254 Pavillon Andre-Aisenstadt 2920, UdeM Abstract: Conditional copula models describe dynamic changes in dependence and are useful in establishing high dimensional dependence structures or in joint modelling of response vectors in regression settings. We describe some of the methods developed for estimating the calibration function when multiple predictors are needed and for resolving some of the model choice questions concerning the selection of copula families and the shape of the calibration function.</description>
    </item>
    
    <item>
      <title>Order selection in multidimensional finite mixture models</title>
      <link>/post/2017winter/2017-01-20/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-01-20/</guid>
      <description>Date: 2017-01-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Finite mixture models provide a natural framework for analyzing data from heterogeneous populations. In practice, however, the number of hidden subpopulations in the data may be unknown. The problem of estimating the order of a mixture model, namely the number of subpopulations, is thus crucial for many applications. In this talk, we present a new penalized likelihood solution to this problem, which is applicable to models with a multidimensional parameter space.</description>
    </item>
    
    <item>
      <title>(Sparse) exchangeable graphs</title>
      <link>/post/2017winter/2017-01-13/</link>
      <pubDate>Fri, 13 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017winter/2017-01-13/</guid>
      <description>Date: 2017-01-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: Many popular statistical models for network valued datasets fall under the remit of the graphon framework, which (implicitly) assumes the networks are densely connected. However, this assumption rarely holds for the real-world networks of practical interest. We introduce a new class of models for random graphs that generalises the dense graphon models to the sparse graph regime, and we argue that this meets many of the desiderata one would demand of a model to serve as the foundation for a statistical analysis of real-world networks.</description>
    </item>
    
    <item>
      <title>Modeling dependence in bivariate multi-state processes: A frailty approach</title>
      <link>/post/2016fall/2016-12-02/</link>
      <pubDate>Fri, 02 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-12-02/</guid>
      <description>Date: 2016-12-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: The aim of this talk is to present a statistical framework for the analysis of dependent bivariate multistate processes, allowing one to study the dependence both across subjects in a pair and among individual-specific events. As for the latter, copula- based models are employed, whereas dependence between multi-state models can be accomplished by means of frailties. The well known Marshall-Olkin Bivariate Exponential Distribution (MOBVE) is considered for the joint distribution of frailties.</description>
    </item>
    
    <item>
      <title>High-dimensional changepoint estimation via sparse projection</title>
      <link>/post/2016fall/2016-12-01/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-12-01/</guid>
      <description>Date: 2016-12-01 Time: 15:30-16:30 Location: BURN 708 Abstract: Changepoints are a very common feature of Big Data that arrive in the form of a data stream. We study high-dimensional time series in which, at certain time points, the mean structure changes in a sparse subset of the coordinates. The challenge is to borrow strength across the coordinates in order to detect smaller changes than could be observed in any individual component series.</description>
    </item>
    
    <item>
      <title>Spatio-temporal models for skewed processes</title>
      <link>/post/2016fall/2016-11-25/</link>
      <pubDate>Fri, 25 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-25/</guid>
      <description>Date: 2016-11-25 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the analysis of most spatio-temporal processes in environmental studies, observations present skewed distributions. Usually, a single transformation of the data is used to approximate normality, and stationary Gaussian processes are assumed to model the transformed data. The choice of transformation is key for spatial interpolation and temporal prediction. We propose a spatio-temporal model for skewed data that does not require the use of data transformation.</description>
    </item>
    
    <item>
      <title>Progress in theoretical understanding of deep learning</title>
      <link>/post/2016fall/2016-11-18/</link>
      <pubDate>Fri, 18 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-18/</guid>
      <description>Date: 2016-11-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: Deep learning has arisen around 2006 as a renewal of neural networks research allowing such models to have more layers. Theoretical investigations have shown that functions obtained as deep compositions of simpler functions (which includes both deep and recurrent nets) can express highly varying functions (with many ups and downs and different input regions that can be distinguished) much more efficiently (with fewer parameters) than otherwise, under a prior which seems to work well for artificial intelligence tasks.</description>
    </item>
    
    <item>
      <title>Tyler&#39;s M-estimator: Subspace recovery and high-dimensional regime</title>
      <link>/post/2016fall/2016-11-11/</link>
      <pubDate>Fri, 11 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-11/</guid>
      <description>Date: 2016-11-11 Time: 15:30-16:30 Location: BURN 1205 Abstract: Given a data set, Tyler&amp;rsquo;s M-estimator is a widely used covariance matrix estimator with robustness to outliers or heavy-tailed distribution. We will discuss two recent results of this estimator. First, we show that when a certain percentage of the data points are sampled from a low-dimensional subspace, Tyler&amp;rsquo;s M-estimator can be used to recover the subspace exactly. Second, in the high-dimensional regime that the number of samples n and the dimension p both go to infinity, p/n converges to a constant y between 0 and 1, and when the data samples are identically and independently generated from the Gaussian distribution N(0,I), we showed that the difference between the sample covariance matrix and a scaled version of Tyler&amp;rsquo;s M-estimator tends to zero in spectral norm, and the empirical spectral densities of both estimators converge to the Marcenko-Pastur distribution.</description>
    </item>
    
    <item>
      <title>Lawlor: Time-varying mixtures of Markov chains: An application to traffic modeling Piché: Bayesian nonparametric modeling of heterogeneous groups of censored data</title>
      <link>/post/2016fall/2016-11-04/</link>
      <pubDate>Fri, 04 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-04/</guid>
      <description>Date: 2016-11-04 Time: 15:30-16:30 Location: BURN 1205 Abstract: Piché: Analysis of survival data arising from different groups, whereby the data in each group is scarce, but abundant overall, is a common issue in applied statistics. Bayesian nonparametrics are tools of choice to handle such datasets given their ability to share information across groups. In this presentation, we will compare three popular Bayesian nonparametric methods on the modeling of survival functions coming from related heterogeneous groups.</description>
    </item>
    
    <item>
      <title>First talk: Bootstrap in practice | Second talk: Statistics and Big Data at Google</title>
      <link>/post/2016fall/2016-11-02/</link>
      <pubDate>Wed, 02 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-11-02/</guid>
      <description>Date: 2016-11-02 Time: 15:00-16:00 17:35-18:25 Location: 1st: BURN 306 2nd: ADAMS AUD Abstract: First talk: This talk focuses on three practical aspects of resampling: communication, accuracy, and software. I&amp;rsquo;ll introduce the bootstrap and permutation tests, and discussed how they may be used to help clients understand statistical results. I&amp;rsquo;ll talk about accuracy &amp;ndash; there are dramatic differences in how accurate different bootstrap methods are. Surprisingly, the most common bootstrap methods are less accurate than classical methods for small samples, and more accurate for larger samples.</description>
    </item>
    
    <item>
      <title>Efficient tests of covariate effects in two-phase failure time studies</title>
      <link>/post/2016fall/2016-10-28/</link>
      <pubDate>Fri, 28 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-10-28/</guid>
      <description>Date: 2016-10-28 Time: 15:30-16:30 Location: BURN 1205 Abstract: Two-phase studies are frequently used when observations on certain variables are expensive or difficult to obtain. One such situation is when a cohort exists for which certain variables have been measured (phase 1 data); then, a sub-sample of individuals is selected, and additional data are collected on them (phase 2). Efficiency for tests and estimators can be increased by basing the selection of phase 2 individuals on data collected at phase 1.</description>
    </item>
    
    <item>
      <title>Statistical analysis of two-level hierarchical clustered data</title>
      <link>/post/2016fall/2016-10-21/</link>
      <pubDate>Fri, 21 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-10-21/</guid>
      <description>Date: 2016-10-21 Time: 15:30-16:30 Location: BURN 1205 Abstract: Multi-level hierarchical clustered data are commonly seen in financial and biostatistics applications. In this talk, we introduce several modeling strategies for describing the dependent relationships for members within a cluster or between different clusters (in the same or different levels). In particular we will apply the hierarchical Kendall copula, first proposed by Brechmann (2014), to model two-level hierarchical clustered survival data. This approach provides a clever way of dimension reduction in modeling complicated multivariate data.</description>
    </item>
    
    <item>
      <title>A Bayesian finite mixture of bivariate regressions model for causal mediation analyses</title>
      <link>/post/2016fall/2016-10-14/</link>
      <pubDate>Fri, 14 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-10-14/</guid>
      <description>Date: 2016-10-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: Building on the work of Schwartz, Gelfand and Miranda (Statistics in Medicine (2010); 29(16), 1710-23), we propose a Bayesian finite mixture of bivariate regressions model for causal mediation analyses. Using an identifiability condition within each component of the mixture, we express the natural direct and indirect effects of the exposure on the outcome as functions of the component-specific regression coefficients. On the basis of simulated data, we examine the behaviour of the model for estimating these effects in situations where the associations between exposure, mediator and outcome are confounded, or not.</description>
    </item>
    
    <item>
      <title>Cellular tree classifiers</title>
      <link>/post/2016fall/2016-10-07/</link>
      <pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-10-07/</guid>
      <description>Date: 2016-10-07 Time: 15:30-16:30 Location: BURN 1205 Abstract: Suppose that binary classification is done by a tree method in which the leaves of a tree correspond to a partition of d-space. Within a partition, a majority vote is used. Suppose furthermore that this tree must be constructed recursively by implementing just two functions, so that the construction can be carried out in parallel by using &amp;ldquo;cells&amp;rdquo;: first of all, given input data, a cell must decide whether it will become a leaf or internal node in the tree.</description>
    </item>
    
    <item>
      <title>CoCoLasso for high-dimensional error-in-variables regression</title>
      <link>/post/2016fall/2016-09-30/</link>
      <pubDate>Fri, 30 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-09-30/</guid>
      <description>Date: 2016-09-30 Time: 15:30-16:30 Location: BURN 1205 Abstract: Much theoretical and applied work has been devoted to high-dimensional regression with clean data. However, we often face corrupted data in many applications where missing data and measurement errors cannot be ignored. Loh and Wainwright (2012) proposed a non-convex modification of the Lasso for doing high-dimensional regression with noisy and missing data. It is generally agreed that the virtues of convexity contribute fundamentally the success and popularity of the Lasso.</description>
    </item>
    
    <item>
      <title>Stein estimation of the intensity parameter of a stationary spatial Poisson point process</title>
      <link>/post/2016fall/2016-09-23/</link>
      <pubDate>Fri, 23 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-09-23/</guid>
      <description>Date: 2016-09-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: We revisit the problem of estimating the intensity parameter of a homogeneous Poisson point process observed in a bounded window of $R^d$ making use of a (now) old idea going back to James and Stein. For this, we prove an integration by parts formula for functionals defined on the Poisson space. This formula extends the one obtained by Privault and Réveillac (Statistical inference for Stochastic Processes, 2009) in the one-dimensional case and is well-suited to a notion of derivative of Poisson functionals which satisfy the chain rule.</description>
    </item>
    
    <item>
      <title>Statistical inference for fractional diffusion processes</title>
      <link>/post/2016fall/2016-09-16/</link>
      <pubDate>Fri, 16 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-09-16/</guid>
      <description>Date: 2016-09-16 Time: 16:00-17:00 Location: LB-921.04, Library Building, Concordia Univ. Abstract: There are some time series which exhibit long-range dependence as noticed by Hurst in his investigations of river water levels along Nile river. Long-range dependence is connected with the concept of self-similarity in that increments of a self-similar process with stationary increments exhibit long-range dependence under some conditions. Fractional Brownian motion is an example of such a process. We discuss statistical inference for stochastic processes modeled by stochastic differential equations driven by a fractional Brownian motion.</description>
    </item>
    
    <item>
      <title>Two-set canonical variate model in multiple populations with invariant loadings</title>
      <link>/post/2016fall/2016-09-09/</link>
      <pubDate>Fri, 09 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016fall/2016-09-09/</guid>
      <description>Date: 2016-09-09 Time: 15:30-16:30 Location: BURN 1205 Abstract: Goria and Flury (Definition 2.1, 1996) proposed the two-set canonical variate model (referred to as the CV-2 model hereafter) and its extension in multiple populations with invariant weight coefficients (Definition 2.2). The equality constraints imposed on the weight coefficients are in line with the approach to interpreting the canonical variates (i.e., the linear combinations of original variables) advocated by Harris (1975, 1989), Rencher (1988, 1992), and Rencher and Christensen (2003).</description>
    </item>
    
    <item>
      <title>Multivariate tests of associations based on univariate tests</title>
      <link>/post/2016winter/2016-04-08/</link>
      <pubDate>Fri, 08 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-04-08/</guid>
      <description>Date: 2016-04-08 Time: 15:30-16:30 Location: BURN 1205 Abstract: For testing two random vectors for independence, we consider testing whether the distance of one vector from an arbitrary center point is independent from the distance of the other vector from an arbitrary center point by a univariate test. We provide conditions under which it is enough to have a consistent univariate test of independence on the distances to guarantee that the power to detect dependence between the random vectors increases to one, as the sample size increases.</description>
    </item>
    
    <item>
      <title>Asymptotic behavior of binned kernel density estimators for locally non-stationary random fields</title>
      <link>/post/2016winter/2016-04-01/</link>
      <pubDate>Fri, 01 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-04-01/</guid>
      <description>Date: 2016-04-01 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, I will describe the finite- and large-sample behavior of binned kernel density estimators for dependent and locally non-stationary random fields converging to stationary random fields. In addition to looking at the bias and asymptotic normality of the estimators, I will present results from a simulation study which shows that the kernel density estimator and the binned kernel density estimator have the same behavior and both estimate accurately the true density when the number of fields increases.</description>
    </item>
    
    <item>
      <title>Robust minimax shrinkage estimation of location vectors under concave loss</title>
      <link>/post/2016winter/2016-03-18/</link>
      <pubDate>Fri, 18 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-03-18/</guid>
      <description>Date: 2016-03-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: We consider the problem of estimating the mean vector, q, of a multivariate spherically symmetric distribution under a loss function which is a concave function of squared error. In particular we find conditions on the shrinkage factor under which Stein-type shrinkage estimators dominate the usual minimax best equivariant estimator. In problems where the scale is known, minimax shrinkage factors which generally depend on both the loss and the sampling distribution are found.</description>
    </item>
    
    <item>
      <title>Nonparametric graphical models: Foundation and trends</title>
      <link>/post/2016winter/2016-03-11/</link>
      <pubDate>Fri, 11 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-03-11/</guid>
      <description>Date: 2016-03-11 Time: 15:30-16:30 Location: BURN 1205 Abstract: We consider the problem of learning the structure of a non-Gaussian graphical model. We introduce two strategies for constructing tractable nonparametric graphical model families. One approach is through semiparametric extension of the Gaussian or exponential family graphical models that allows arbitrary graphs. Another approach is to restrict the family of allowed graphs to be acyclic, enabling the use of fully nonparametric density estimation in high dimensions.</description>
    </item>
    
    <item>
      <title>Ridges and valleys in the high excursion sets of Gaussian random fields</title>
      <link>/post/2016winter/2016-03-10/</link>
      <pubDate>Thu, 10 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-03-10/</guid>
      <description>Date: 2016-03-10 Time: 15:30-16:30 Location: MAASS 217, McGill Abstract: It is well known that normal random variables do not like taking large values. Therefore, a continuous Gaussian random field on a compact set does not like exceeding a large level. If it does exceed a large level at some point, it tends to go back below the level a short distance away from that point. One, therefore, does not expect the excursion set above a high for such a field to possess any interesting structure.</description>
    </item>
    
    <item>
      <title>Aggregation methods for portfolios of dependent risks with Archimedean copulas</title>
      <link>/post/2016winter/2016-02-26/</link>
      <pubDate>Fri, 26 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-02-26/</guid>
      <description>Date: 2016-02-26 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we will consider a portfolio of dependent risks represented by a vector of dependent random variables whose joint cumulative distribution function (CDF) is defined with an Archimedean copula. Archimedean copulas are very popular and their extensions, nested Archimedean copulas, are well suited for vectors of random vectors in high dimension. I will describe a simple approach which makes it possible to compute the CDF of the sum or a variety of other functions of those random variables.</description>
    </item>
    
    <item>
      <title>An introduction to statistical lattice models and observables</title>
      <link>/post/2016winter/2016-02-19/</link>
      <pubDate>Fri, 19 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-02-19/</guid>
      <description>Date: 2016-02-19 Time: 15:30-16:30 Location: BURN 1205 Abstract: The study of convergence of random walks to well defined curves is founded in the fields of complex analysis, probability theory, physics and combinatorics. The foundations of this subject were motivated by physicists interested in the properties of one-dimensional models that represented some form of physical phenomenon. By taking physical models and generalizing them into abstract mathematical terms, macroscopic properties about the model could be determined from the microscopic level.</description>
    </item>
    
    <item>
      <title>Outlier detection for functional data using principal components</title>
      <link>/post/2016winter/2016-02-11/</link>
      <pubDate>Thu, 11 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-02-11/</guid>
      <description>Date: 2016-02-11 Time: 16:00-17:00 Location: CRM 6254 (U. de Montréal) Abstract: Principal components analysis is a widely used technique that provides an optimal lower-dimensional approximation to multivariate observations. In the functional case, a new characterization of elliptical distributions on separable Hilbert spaces allows us to obtain an equivalent stochastic optimality property for the principal component subspaces of random elements on separable Hilbert spaces. This property holds even when second moments do not exist.</description>
    </item>
    
    <item>
      <title>The Bayesian causal effect estimation algorithm</title>
      <link>/post/2016winter/2016-02-05/</link>
      <pubDate>Fri, 05 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-02-05/</guid>
      <description>Date: 2016-02-05 Time: 15:30-16:30 Location: BURN 1214 Abstract: Estimating causal exposure effects in observational studies ideally requires the analyst to have a vast knowledge of the domain of application. Investigators often bypass difficulties related to the identification and selection of confounders through the use of fully adjusted outcome regression models. However, since such models likely contain more covariates than required, the variance of the regression coefficient for exposure may be unnecessarily large.</description>
    </item>
    
    <item>
      <title>Estimating high-dimensional networks with hubs with an application to microbiome data</title>
      <link>/post/2016winter/2016-01-29/</link>
      <pubDate>Fri, 29 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-01-29/</guid>
      <description>Date: 2016-01-29 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we investigate the problem of estimating high-dimensional networks in which there are a few highly connected “hub&amp;rdquo; nodes. Methods based on L1-regularization have been widely used for performing sparse selection in the graphical modelling context. However, the L1 penalty penalizes each edge equally and independently of each other without taking into account any structural information. We introduce a new method for estimating undirected graphical models with hubs, called the hubs weighted graphical lasso (HWGL).</description>
    </item>
    
    <item>
      <title>Robust estimation in the presence of influential units in surveys</title>
      <link>/post/2016winter/2016-01-22/</link>
      <pubDate>Fri, 22 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>/post/2016winter/2016-01-22/</guid>
      <description>Date: 2016-01-22 Time: 15:30-16:30 Location: BURN 1205 Abstract: Influential units are those which make classical estimators (e.g., the Horvitz-Thompson estimator or calibration estimators) very unstable. The problem of influential units is particularly important in business surveys, which collect economic variables, whose distribution are highly skewed (heavy right tail). In this talk, we will attempt to answer the following questions:
(1) What is an influential value in surveys? (2) How measure the influence of unit?</description>
    </item>
    
    <item>
      <title>Causal discovery with confidence using invariance principles</title>
      <link>/post/2015fall/2015-12-10/</link>
      <pubDate>Thu, 10 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-12-10/</guid>
      <description>Date: 2015-12-10 Time: 15:30-16:30 Location: UdeM, Pav. Roger-Gaudry, salle S-116 Abstract: What is interesting about causal inference? One of the most compelling aspects is that any prediction under a causal model is valid in environments that are possibly very different to the environment used for inference. For example, variables can be actively changed and predictions will still be valid and useful. This invariance is very useful but still leaves open the difficult question of inference.</description>
    </item>
    
    <item>
      <title>Inference regarding within-family association in disease onset times under biased sampling schemes</title>
      <link>/post/2015fall/2015-11-26/</link>
      <pubDate>Thu, 26 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-11-26/</guid>
      <description>Date: 2015-11-26 Time: 15:30-16:30 Location: BURN 306 Abstract: In preliminary studies of the genetic basis for chronic conditions, interest routinely lies in the within-family dependence in disease status. When probands are selected from disease registries and their respective families are recruited, a variety of ascertainment bias-corrected methods of inference are available which are typically based on models for correlated binary data. This approach ignores the age that family members are at the time of assessment.</description>
    </item>
    
    <item>
      <title>Prevalent cohort studies: Length-biased sampling with right censoring</title>
      <link>/post/2015fall/2015-11-13/</link>
      <pubDate>Fri, 13 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-11-13/</guid>
      <description>Date: 2015-11-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: Logistic or other constraints often preclude the possibility of conducting incident cohort studies. A feasible alternative in such cases is to conduct a cross-sectional prevalent cohort study for which we recruit prevalent cases, i.e., subjects who have already experienced the initiating event, say the onset of a disease. When the interest lies in estimating the lifespan between the initiating event and a terminating event, say death for instance, such subjects may be followed prospectively until the terminating event or loss to follow-up, whichever happens first.</description>
    </item>
    
    <item>
      <title>Bayesian analysis of non-identifiable models, with an example from epidemiology and biostatistics</title>
      <link>/post/2015fall/2015-11-06/</link>
      <pubDate>Fri, 06 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-11-06/</guid>
      <description>Date: 2015-11-06 Time: 15:30-16:30 Location: BURN 1205 Abstract: Most regression models in biostatistics assume identifiability, which means that each point in the parameter space corresponds to a unique likelihood function for the observable data. Recently there has been interest in Bayesian inference for non-identifiable models, which can better represent uncertainty in some contexts. One example is in the field of epidemiology, where the investigator is concerned with bias due to unmeasured confounders (omitted variables).</description>
    </item>
    
    <item>
      <title>A knockoff filter for controlling the false discovery rate</title>
      <link>/post/2015fall/2015-10-30/</link>
      <pubDate>Fri, 30 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-10-30/</guid>
      <description>Date: 2015-10-30 Time: 16:00-17:00 Location: Salle 1360, Pavillon André-Aisenstadt, Université de Montréa Abstract: The big data era has created a new scientific paradigm: collect data first, ask questions later. Imagine that we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR) - the expected fraction of false discoveries among all discoveries - is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable.</description>
    </item>
    
    <item>
      <title>Robust mixture regression and outlier detection via penalized likelihood</title>
      <link>/post/2015fall/2015-10-23/</link>
      <pubDate>Fri, 23 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-10-23/</guid>
      <description>Date: 2015-10-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: Finite mixture regression models have been widely used for modeling mixed regression relationships arising from a clustered and thus heterogenous population. The classical normal mixture model, despite of its simplicity and wide applicability, may fail dramatically in the presence of severe outliers. We propose a robust mixture regression approach based on a sparse, case-specific, and scale-dependent mean-shift parameterization, for simultaneously conducting outlier detection and robust parameter estimation.</description>
    </item>
    
    <item>
      <title>Estimating high-dimensional multi-layered networks through penalized maximum likelihood</title>
      <link>/post/2015fall/2015-10-16/</link>
      <pubDate>Fri, 16 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-10-16/</guid>
      <description>Date: 2015-10-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: Gaussian graphical models represent a good tool for capturing interactions between nodes represent the underlying random variables. However, in many applications in biology one is interested in modeling associations both between, as well as within molecular compartments (e.g., interactions between genes and proteins/metabolites). To this end, inferring multi-layered network structures from high-dimensional data provides insight into understanding the conditional relationships among nodes within layers, after adjusting for and quantifying the effects of nodes from other layers.</description>
    </item>
    
    <item>
      <title>Parameter estimation of partial differential equations over irregular domains</title>
      <link>/post/2015fall/2015-10-09/</link>
      <pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-10-09/</guid>
      <description>Date: 2015-10-09 Time: 15:30-16:30 Location: BURN 1205 Abstract: Spatio-temporal data are abundant in many scientific fields; examples include daily satellite images of the earth, hourly temperature readings from multiple weather stations, and the spread of an infectious disease over a particular region. In many instances the spatio-temporal data are accompanied by mathematical models expressed in terms of partial differential equations (PDEs). These PDEs determine the theoretical aspects of the behavior of the physical, chemical or biological phenomena considered.</description>
    </item>
    
    <item>
      <title>Estimating covariance matrices of intermediate size</title>
      <link>/post/2015fall/2015-10-02/</link>
      <pubDate>Fri, 02 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-10-02/</guid>
      <description>Date: 2015-10-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: In finance, the covariance matrix of many assets is a key component of financial portfolio optimization and is usually estimated from historical data. Much research in the past decade has focused on improving estimation by studying the asymptotics of large covariance matrices in the so-called high-dimensional regime, where the dimension p grows at the same pace as the sample size n, and this approach has been very successful.</description>
    </item>
    
    <item>
      <title>Topics in statistical inference for the semiparametric elliptical copula model</title>
      <link>/post/2015fall/2015-09-25/</link>
      <pubDate>Fri, 25 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-09-25/</guid>
      <description>Date: 2015-09-25 Time: 15:30-16:30 Location: BURN 1205 Abstract: This talk addresses aspects of the statistical inference problem for the semiparametric elliptical copula model. The semiparametric elliptical copula model is the family of distributions whose dependence structures are specified by parametric elliptical copulas but whose marginal distributions are left unspecified. An elliptical copula is uniquely characterized by a characteristic generator and a copula correlation matrix Sigma. In the first part of this talk, I will consider the estimation of Sigma.</description>
    </item>
    
    <item>
      <title>A unified algorithm for fitting penalized models with high-dimensional data</title>
      <link>/post/2015fall/2015-09-18/</link>
      <pubDate>Fri, 18 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-09-18/</guid>
      <description>Date: 2015-09-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the light of high-dimensional problems, research on the penalized model has received much interest. Correspondingly, several algorithms have been developed for solving penalized high-dimensional models. I will describe fast and efficient unified algorithms for computing the solution path for a collection of penalized models. In particular, we will look at an algorithm for solving L1-penalized learning problems and an algorithm for solving group-lasso learning problems.</description>
    </item>
    
    <item>
      <title>Bias correction in multivariate extremes</title>
      <link>/post/2015fall/2015-09-11/</link>
      <pubDate>Fri, 11 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015fall/2015-09-11/</guid>
      <description>Date: 2015-09-11 Time: 15:30-16:30 Location: BURN 1205 Abstract: The estimation of the extremal dependence structure of a multivariate extreme-value distribution is spoiled by the impact of the bias, which increases with the number of observations used for the estimation. Already known in the univariate setting, the bias correction procedure is studied in this talk under the multivariate framework. New families of estimators of the stable tail dependence function are obtained.</description>
    </item>
    
    <item>
      <title>Some new classes of bivariate distributions based on conditional specification</title>
      <link>/post/2015winter/2015-05-14/</link>
      <pubDate>Thu, 14 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-05-14/</guid>
      <description>Date: 2015-05-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: A bivariate distribution can sometimes be characterized completely by properties of its conditional distributions. In this talk, we will discuss models of bivariate distributions whose conditionals are members of prescribed parametric families of distributions. Some relevant models with specified conditionals will be discussed, including the normal and lognormal cases, the skew-normal and other families of distributions. Finally, some conditionally specified densities will be shown to provide convenient flexible conjugate prior families in certain multiparameter Bayesian settings.</description>
    </item>
    
    <item>
      <title>A statistical view of some recent climate controversies</title>
      <link>/post/2015winter/2015-05-07/</link>
      <pubDate>Thu, 07 May 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-05-07/</guid>
      <description>Date: 2015-05-07 Time: 15:30-16:30 Location: Université de Sherbrooke Abstract: This talk looks at some recent climate controversies from a statistical standpoint. The issues are motivated via changepoints and their detection. Changepoints are ubiquitous features in climatic time series, occurring whenever stations relocate or gauges are changed. Ignoring changepoints can produce spurious trend conclusions. Changepoint tests involving cumulative sums, likelihood ratio, and maximums of F-statistics are introduced; the asymptotic distributions of these statistics are quantified under the changepoint-free null hypothesis.</description>
    </item>
    
    <item>
      <title>Testing for network community structure</title>
      <link>/post/2015winter/2015-03-20/</link>
      <pubDate>Fri, 20 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-03-20/</guid>
      <description>Date: 2015-03-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Networks provide a useful means to summarize sparse yet structured massive datasets, and so are an important aspect of the theory of big data. A key question in this setting is to test for the significance of community structure or what in social networks is termed homophily, the tendency of nodes to be connected based on similar characteristics. Network models where a single parameter per node governs the propensity of connection are popular in practice, because they are simple to understand and analyze.</description>
    </item>
    
    <item>
      <title>Bayesian approaches to causal inference: A lack-of-success story</title>
      <link>/post/2015winter/2015-03-13/</link>
      <pubDate>Fri, 13 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-03-13/</guid>
      <description>Date: 2015-03-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: Despite almost universal acceptance across most fields of statistics, Bayesian inferential methods have yet to breakthrough to widespread use in causal inference, despite Bayesian arguments being a core component of early developments in the field. Some quasi-Bayesian procedures have been proposed, but often these approaches rely on heuristic, sometimes flawed, arguments. In this talk I will discuss some formulations of classical causal inference problems from the perspective of standard Bayesian representations, and propose some inferential solutions.</description>
    </item>
    
    <item>
      <title>A novel statistical framework to characterize antigen-specific T-cell functional diversity in single-cell expression data</title>
      <link>/post/2015winter/2015-02-27/</link>
      <pubDate>Fri, 27 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-27/</guid>
      <description>Date: 2015-02-27 Time: 15:30-16:30 Location: BURN 1205 Abstract: I will talk about COMPASS, a new Bayesian hierarchical framework for characterizing functional differences in antigen-specific T cells by leveraging high-throughput, single-cell flow cytometry data. In particular, I will illustrate, using a variety of data sets, how COMPASS can reveal subtle and complex changes in antigen-specific T-cell activation profiles that correlate with biological endpoints. Applying COMPASS to data from the RV144 (“the Thai trial”) HIV clinical trial, it identified novel T-cell subsets that were inverse correlates of HIV infection risk.</description>
    </item>
    
    <item>
      <title>Comparison and assessment of particle diffusion models in biological fluids</title>
      <link>/post/2015winter/2015-02-20/</link>
      <pubDate>Fri, 20 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-20/</guid>
      <description>Date: 2015-02-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Rapidly progressing particle tracking techniques have revealed that foreign particles in biological fluids exhibit rich and at times unexpected behavior, with important consequences for disease diagnosis and drug delivery. Yet, there remains a frustrating lack of coherence in the description of these particles&amp;rsquo; motion. Largely this is due to a reliance on functional statistics (e.g., mean-squared displacement) to perform model selection and assess goodness-of-fit.</description>
    </item>
    
    <item>
      <title>Tuning parameters in high-dimensional statistics</title>
      <link>/post/2015winter/2015-02-13/</link>
      <pubDate>Fri, 13 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-13/</guid>
      <description>Date: 2015-02-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: High-dimensional statistics is the basis for analyzing large and complex data sets that are generated by cutting-edge technologies in genetics, neuroscience, astronomy, and many other fields. However, Lasso, Ridge Regression, Graphical Lasso, and other standard methods in high-dimensional statistics depend on tuning parameters that are difficult to calibrate in practice. In this talk, I present two novel approaches to overcome this difficulty.</description>
    </item>
    
    <item>
      <title>A fast unified algorithm for solving group Lasso penalized learning problems</title>
      <link>/post/2015winter/2015-02-05/</link>
      <pubDate>Thu, 05 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-05/</guid>
      <description>Date: 2015-02-05 Time: 15:30-16:30 Location: BURN 1B39 Abstract: We consider a class of group-lasso learning problems where the objective function is the sum of an empirical loss and the group-lasso penalty. For a class of loss function satisfying a quadratic majorization condition, we derive a unified algorithm called groupwise-majorization-descent (GMD) for efficiently computing the solution paths of the corresponding group-lasso penalized learning problem. GMD allows for general design matrices, without requiring the predictors to be group-wise orthonormal.</description>
    </item>
    
    <item>
      <title>Joint analysis of multiple multi-state processes via copulas</title>
      <link>/post/2015winter/2015-02-02/</link>
      <pubDate>Mon, 02 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-02-02/</guid>
      <description>Date: 2015-02-02 Time: 15:30-16:30 Location: BURN 1214 Abstract: A copula-based model is described which enables joint analysis of multiple progressive multi-state processes. Unlike intensity-based or frailty-based approaches to joint modeling, the copula formulation proposed herein ensures that a wide range of marginal multi-state processes can be specified and the joint model will retain these marginal features. The copula formulation also facilitates a variety of approaches to estimation and inference including composite likelihood and two-stage estimation procedures.</description>
    </item>
    
    <item>
      <title>Distributed estimation and inference for sparse regression</title>
      <link>/post/2015winter/2015-01-30/</link>
      <pubDate>Fri, 30 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-01-30/</guid>
      <description>Date: 2015-01-30 Time: 15:30-16:30 Location: BURN 1205 Abstract: We address two outstanding challenges in sparse regression: (i) computationally efficient estimation in distributed settings; (ii) valid inference for the selected coefficients. The main computational challenge in a distributed setting is harnessing the computational capabilities of all the machines while keeping communication costs low. We devise an approach that requires only a single round of communication among the machines. We show the approach recovers the convergence rate of the (centralized) lasso as long as each machine has access to an adequate number of samples.</description>
    </item>
    
    <item>
      <title>Simultaneous white noise models and shrinkage recovery of functional data</title>
      <link>/post/2015winter/2015-01-16/</link>
      <pubDate>Fri, 16 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-01-16/</guid>
      <description>Date: 2015-01-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: We consider the white noise representation of functional data taken as i.i.d. realizations of a Gaussian process. The main idea is to establish an asymptotic equivalence in Le Cam’s sense between an experiment which simultaneously describes these realizations and a collection of white noise models. In this context, we project onto an arbitrary basis and apply a novel variant of Stein-type estimation for optimal recovery of the realized trajectories.</description>
    </item>
    
    <item>
      <title>Functional data analysis and related topics</title>
      <link>/post/2015winter/2015-01-15/</link>
      <pubDate>Thu, 15 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-01-15/</guid>
      <description>Date: 2015-01-15 Time: 16:00-17:00 Location: CRM 1360 (U. de Montréal) Abstract: Functional data analysis (FDA) has received substantial attention, with applications arising from various disciplines, such as engineering, public health, finance etc. In general, the FDA approaches focus on nonparametric underlying models that assume the data are observed from realizations of stochastic processes satisfying some regularity conditions, e.g., smoothness constraints. The estimation and inference procedures usually do not depend on merely a finite number of parameters, which contrasts with parametric models, and exploit techniques, such as smoothing methods and dimension reduction, that allow data to speak for themselves.</description>
    </item>
    
    <item>
      <title>Mixtures of coalesced generalized hyperbolic distributions</title>
      <link>/post/2015winter/2015-01-13/</link>
      <pubDate>Tue, 13 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-01-13/</guid>
      <description>Date: 2015-01-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: A mixture of coalesced generalized hyperbolic distributions is developed by joining a finite mixture of generalized hyperbolic distributions with a mixture of multiple scaled generalized hyperbolic distributions. The result is a mixture of mixtures with shared model parameters and common mode. We begin by discussing the generalized hyperbolic distribution, which has the t, Gaussian and others as special cases. The generalized hyperbolic distribution can represented as a normal-variance mixture using a generalized inverse Gaussian distribution.</description>
    </item>
    
    <item>
      <title>Space-time data analysis: Out of the Hilbert box</title>
      <link>/post/2015winter/2015-01-9/</link>
      <pubDate>Fri, 09 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/post/2015winter/2015-01-9/</guid>
      <description>Date: 2015-01-09 Time: 15:30-16:30 Location: BURN 1205 Abstract: Given the discouraging state of current efforts to curb global warming, we can imagine that we will soon turn our attention to mitigation. On a global scale, distressed populations will turn to national and international organizations for solutions to dramatic problems caused by climate change. These institutions in turn will mandate the collection of data on a scale and resolution that will present extraordinary statistical and computational challenges to those of us viewed as having the appropriate expertise.</description>
    </item>
    
    <item>
      <title>Testing for structured Normal means</title>
      <link>/post/2014fall/2014-12-12/</link>
      <pubDate>Fri, 12 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-12-12/</guid>
      <description>Date: 2014-12-12 Time: 15:30-16:30 Location: BURN 1205 Abstract: We will discuss the detection of pattern in images and graphs from a high-dimensional Gaussian measurement. This problem is relevant to many applications including detecting anomalies in sensor and computer networks, large-scale surveillance, co-expressions in gene networks, disease outbreaks, etc. Beyond its wide applicability, structured Normal means detection serves as a case study in the difficulty of balancing computational complexity with statistical power.</description>
    </item>
    
    <item>
      <title>Copula model selection: A statistical approach</title>
      <link>/post/2014fall/2014-12-05/</link>
      <pubDate>Fri, 05 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-12-05/</guid>
      <description>Date: 2014-12-05 Time: 15:30-16:30 Location: BURN 1205 Abstract: Copula model selection is an important problem because similar but differing copula models can offer different conclusions surrounding the dependence structure of random variables. Chen &amp;amp; Fan (2005) proposed a model selection method involving a statistical hypothesis test. The hypothesis test attempts to take into account the randomness of the AIC and other likelihood-based model selection methods for finite samples. Performance of the test compared to the more common approach of AIC is illustrated in a series of simulations.</description>
    </item>
    
    <item>
      <title>Model-based methods of classification with applications</title>
      <link>/post/2014fall/2014-11-28/</link>
      <pubDate>Fri, 28 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-11-28/</guid>
      <description>Date: 2014-11-28 Time: 15:30-16:30 Location: BURN 1205 Abstract: Model-based clustering via finite mixture models is a popular clustering method for finding hidden structures in data. The model is often assumed to be a finite mixture of multivariate normal distributions; however, flexible extensions have been developed over recent years. This talk demonstrates some methods employed in unsupervised, semi-supervised, and supervised classification that include skew-normal and skew-t mixture models. Both real and simulated data sets are used to demonstrate the efficacy of these techniques.</description>
    </item>
    
    <item>
      <title>Estimating by solving nonconvex programs: Statistical and computational guarantees</title>
      <link>/post/2014fall/2014-11-21/</link>
      <pubDate>Fri, 21 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-11-21/</guid>
      <description>Date: 2014-11-21 Time: 15:30-16:30 Location: BURN 1205 Abstract: Many statistical estimators are based on solving nonconvex programs. Although the practical performance of such methods is often excellent, the associated theory is frequently incomplete, due to the potential gaps between global and local optima. In this talk, we present theoretical results that apply to all local optima of various regularized M-estimators, where both loss and penalty functions are allowed to be nonconvex.</description>
    </item>
    
    <item>
      <title>High-dimensional phenomena in mathematical statistics and convex analysis</title>
      <link>/post/2014fall/2014-11-20/</link>
      <pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-11-20/</guid>
      <description>Date: 2014-11-20 Time: 16:00-17:00 Location: CRM 1360 (U. de Montréal) Abstract: Statistical models in which the ambient dimension is of the same order or larger than the sample size arise frequently in different areas of science and engineering. Although high-dimensional models of this type date back to the work of Kolmogorov, they have been the subject of intensive study over the past decade, and have interesting connections to many branches of mathematics (including concentration of measure, random matrix theory, convex geometry, and information theory).</description>
    </item>
    
    <item>
      <title>Bridging the gap: A likelihood function approach for the analysis of ranking data</title>
      <link>/post/2014fall/2014-11-14/</link>
      <pubDate>Fri, 14 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-11-14/</guid>
      <description>Date: 2014-11-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the parametric setting, the notion of a likelihood function forms the basis for the development of tests of hypotheses and estimation of parameters. Tests in connection with the analysis of variance stem entirely from considerations of the likelihood function. On the other hand, non- parametric procedures have generally been derived without any formal mechanism and are often the result of clever intuition.</description>
    </item>
    
    <item>
      <title>Bayesian regression with B-splines under combinations of shape constraints and smoothness properties</title>
      <link>/post/2014fall/2014-11-07/</link>
      <pubDate>Fri, 07 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-11-07/</guid>
      <description>Date: 2014-11-07 Time: 15:30-16:30 Location: BURN 1205 Abstract: We approach the problem of shape constrained regression from a Bayesian perspective. A B-spline basis is used to model the regression function. The smoothness of the regression function is controlled by the order of the B-splines and the shape is controlled by the shape of an associated control polygon. Controlling the shape of the control polygon reduces to some inequality constraints on the spline coefficients.</description>
    </item>
    
    <item>
      <title>A copula-based model for risk aggregation</title>
      <link>/post/2014fall/2014-10-31/</link>
      <pubDate>Fri, 31 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-31/</guid>
      <description>Date: 2014-10-31 Time: 15:30-16:30 Location: BURN 1205 Abstract: A flexible approach is proposed for risk aggregation. The model consists of a tree structure, bivariate copulas, and marginal distributions. The construction relies on a conditional independence assumption whose implications are studied. Selection the tree structure, estimation and model validation are illustrated using data from a Canadian property and casualty insurance company.
Speaker Marie-Pier Côté is a PhD student in the Department of Mathematics and Statistics at McGill University.</description>
    </item>
    
    <item>
      <title>PREMIER: Probabilistic error-correction using Markov inference in error reads</title>
      <link>/post/2014fall/2014-10-24/</link>
      <pubDate>Fri, 24 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-24/</guid>
      <description>Date: 2014-10-24 Time: 15:30-16:30 Location: BURN 1205 Abstract: Next generation sequencing (NGS) is a technology revolutionizing genetics and biology. Compared with the old Sanger sequencing method, the throughput is astounding and has fostered a slew of innovative sequencing applications. Unfortunately, the error rates are also higher, complicating many downstream analyses. For example, de novo assembly of genomes is less accurate and slower when reads include many errors. We develop a probabilistic model for NGS reads that can detect and correct errors without a reference genome and while flexibly modeling and estimating the error properties of the sequencing machine.</description>
    </item>
    
    <item>
      <title>Patient privacy, big data, and specimen pooling: Using an old tool for new challenges</title>
      <link>/post/2014fall/2014-10-17/</link>
      <pubDate>Fri, 17 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-17/</guid>
      <description>Date: 2014-10-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the recent past, electronic health records and distributed data networks emerged as a viable resource for medical and scientific research. As the use of confidential patient information from such sources become more common, maintaining privacy of patients is of utmost importance. For a binary disease outcome of interest, we show that the techniques of specimen pooling could be applied for analysis of large and/or distributed data while respecting patient privacy.</description>
    </item>
    
    <item>
      <title>A margin-free clustering algorithm appropriate for dependent maxima in the domain of attraction of an extreme-value copula</title>
      <link>/post/2014fall/2014-10-10/</link>
      <pubDate>Fri, 10 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-10/</guid>
      <description>Date: 2014-10-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: Extracting relevant information in complex spatial-temporal data sets is of paramount importance in statistical climatology. This is especially true when identifying spatial dependencies between quantitative extremes like heavy rainfall. The paper of Bernard et al. (2013) develops a fast and simple clustering algorithm for finding spatial patterns appropriate for extremes. They develop their algorithm by adapting multivariate extreme-value theory to the context of spatial clustering.</description>
    </item>
    
    <item>
      <title>Statistical exploratory data analysis in the modern era</title>
      <link>/post/2014fall/2014-10-03/</link>
      <pubDate>Fri, 03 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-10-03/</guid>
      <description>Date: 2014-10-03 Time: 15:30-16:30 Location: BURN 1205 Abstract: Major challenges arising from today&amp;rsquo;s &amp;ldquo;data deluge&amp;rdquo; include how to handle the commonly occurring situation of different types of variables (say, continuous and categorical) being simultaneously measured, as well as how to assess the accompanying flood of questions. Based on information theory, a bias-corrected mutual information (BCMI) measure of association that is valid and estimable between all basic types of variables has been proposed.</description>
    </item>
    
    <item>
      <title>Analysis of palliative care studies with joint models for quality-of-life measures and survival</title>
      <link>/post/2014fall/2014-09-26/</link>
      <pubDate>Fri, 26 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-09-26/</guid>
      <description>Date: 2014-09-26 Time: 15:30-16:30 Location: BURN 1205 Abstract: In palliative care studies, the primary outcomes are often health related quality of life measures (HRLQ). Randomized trials and prospective cohorts typically recruit patients with advanced stage of disease and follow them until death or end of the study. An important feature of such studies is that, by design, some patients, but not all, are likely to die during the course of the study.</description>
    </item>
    
    <item>
      <title>Covariates missing by design</title>
      <link>/post/2014fall/2014-09-19/</link>
      <pubDate>Fri, 19 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-09-19/</guid>
      <description>Date: 2014-09-19 Time: 15:30-16:30 Location: BURN 1205 Abstract: Incomplete data can arise in many different situations for many different reasons. Sometimes the data may be incomplete for reasons beyond the control of the experimenter. However, it is also possible that this missingness is part of the study design. By using a two-phase sampling approach where only a small sub-sample gives complete information, it is possible to greatly reduce the cost of a study and still obtain precise estimates.</description>
    </item>
    
    <item>
      <title>Hydrological applications with the functional data analysis framework</title>
      <link>/post/2014fall/2014-09-12/</link>
      <pubDate>Fri, 12 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014fall/2014-09-12/</guid>
      <description>Date: 2014-09-12 Time: 15:30-16:30 Location: BURN 1205 Abstract: River flows records are an essential data source for a variety of hydrological applications including the prevention of flood risks and as well as the planning and management of water resources. A hydrograph is a graphical representation of the temporal variation of flow over a period of time (continuously measured, usually over a year). A flood hydrograph is commonly characterized by a number of features, mainly its peak, volume and duration.</description>
    </item>
    
    <item>
      <title>Adaptive piecewise polynomial estimation via trend filtering</title>
      <link>/post/2014winter/2014-04-11/</link>
      <pubDate>Fri, 11 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-04-11/</guid>
      <description>Date: 2014-04-11 Time: 15:30-16:30 Location: Salle KPMG, 1er étage HEC Montréal Abstract: We will discuss trend filtering, a recently proposed tool of Kim et al. (2009) for nonparametric regression. The trend filtering estimate is defined as the minimizer of a penalized least squares criterion, in which the penalty term sums the absolute kth order discrete derivatives over the input points. Perhaps not surprisingly, trend filtering estimates appear to have the structure of kth degree spline functions, with adaptively chosen knot points (we say “appear” here as trend filtering estimates are not really functions over continuous domains, and are only defined over the discrete set of inputs).</description>
    </item>
    
    <item>
      <title>Some aspects of data analysis under confidentiality protection</title>
      <link>/post/2014winter/2014-04-04/</link>
      <pubDate>Fri, 04 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-04-04/</guid>
      <description>Date: 2014-04-04 Time: 15:30-16:30 Location: BURN 1205 Abstract: Statisticians working in most federal agencies are often faced with two conflicting objectives: (1) collect and publish useful datasets for designing public policies and building scientific theories, and (2) protect confidentiality of data respondents which is essential to uphold public trust, leading to better response rates and data accuracy. In this talk I will provide a survey of two statistical methods currently used at the U.</description>
    </item>
    
    <item>
      <title>How much does the dependence structure matter?</title>
      <link>/post/2014winter/2014-03-28/</link>
      <pubDate>Fri, 28 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-03-28/</guid>
      <description>Date: 2014-03-28 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we will look at some classical problems from an anti-traditional perspective. We will consider two problems regarding a sequence of random variables with a given common marginal distribution. First, we will introduce the notion of extreme negative dependence (END), a new benchmark for negative dependence, which is comparable to comonotonicity and independence. Second, we will study the compatibility of the marginal distribution and the limiting distribution when the dependence structure in the sequence is allowed to vary among all possibilities.</description>
    </item>
    
    <item>
      <title>Insurance company operations and dependence modeling</title>
      <link>/post/2014winter/2014-03-21/</link>
      <pubDate>Fri, 21 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-03-21/</guid>
      <description>Date: 2014-03-21 Time: 15:30-16:30 Location: BURN 107 Abstract: Actuaries and other analysts have long had the responsibility in insurance company operations for various financial functions including (i) ratemaking, the process of setting premiums, (ii) loss reserving, the process of predicting obligations that arise from policies, and (iii) claims management, including fraud detection. With the advent of modern computing capabilities and detailed and novel data sources, new opportunities to make an impact on insurance company operations are extensive.</description>
    </item>
    
    <item>
      <title>Mixed effects trees and forests for clustered data</title>
      <link>/post/2014winter/2014-03-14/</link>
      <pubDate>Fri, 14 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-03-14/</guid>
      <description>Date: 2014-03-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, I will present extensions of tree-based and random forest methods for the case of clustered data. The proposed methods can handle unbalanced clusters, allows observations within clusters to be splitted, and can incorporate random effects and observation-level covariates. The basic tree-building algorithm for a continuous outcome is implemented using standard algorithms within the framework of the EM algorithm. The extension to other types of outcomes (e.</description>
    </item>
    
    <item>
      <title>ABC as the new empirical Bayes approach?</title>
      <link>/post/2014winter/2014-02-28/</link>
      <pubDate>Fri, 28 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-02-28/</guid>
      <description>Date: 2014-02-28 Time: 13:30-14:30 Location: UdM, Pav. Roger-Gaudry, Salle S-116 Abstract: Approximate Bayesian computation (ABC) has now become an essential tool for the analysis of complex stochastic models when the likelihood function is unavailable. The approximation is seen as a nuisance from a computational statistic point of view but we argue here it is also a blessing from an inferential perspective. We illustrate this paradoxical stand in the case of dynamic models and population genetics models.</description>
    </item>
    
    <item>
      <title>On the multivariate analysis of neural spike trains: Skellam process with resetting and its applications</title>
      <link>/post/2014winter/2014-02-21/</link>
      <pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-02-21/</guid>
      <description>Date: 2014-02-21 Time: 15:30-16:30 Location: BURN 1205 Abstract: Nerve cells (a.k.a. neurons) communicate via electrochemical waves (action potentials), which are usually called spikes as they are very localized in time. A sequence of consecutive spikes from one neuron is called a spike train. The exact mechanism of information coding in spike trains is still an open problem; however, one popular approach is to model spikes as realizations of an inhomogeneous Poisson process.</description>
    </item>
    
    <item>
      <title>Divergence based inference for general estimating equations</title>
      <link>/post/2014winter/2014-02-14/</link>
      <pubDate>Fri, 14 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-02-14/</guid>
      <description>Date: 2014-02-14 Time: 15:30-16:30 Location: BURN 1205 Abstract: Hellinger distance and its variants have long been used in the theory of robust statistics to develop inferential tools that are more robust than the maximum likelihood but as ecient as the MLE when the posited model holds. A key aspect of this alternative approach requires specication of a parametric family, which is usually not feasible in the context of problems involving complex data structures wherein estimating equations are typically used for inference.</description>
    </item>
    
    <item>
      <title>Statistical techniques for the normalization and segmentation of structural MRI</title>
      <link>/post/2014winter/2014-02-07/</link>
      <pubDate>Fri, 07 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-02-07/</guid>
      <description>Date: 2014-02-07 Time: 15:30-16:30 Location: BURN 1205 Abstract: While computed tomography and other imaging techniques are measured in absolute units with physical meaning, magnetic resonance images are expressed in arbitrary units that are difficult to interpret and differ between study visits and subjects. Much work in the image processing literature has centered on histogram matching and other histogram mapping techniques, but little focus has been on normalizing images to have biologically interpretable units.</description>
    </item>
    
    <item>
      <title>An exchangeable Kendall&#39;s tau for clustered data</title>
      <link>/post/2014winter/2014-01-31/</link>
      <pubDate>Fri, 31 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-01-31/</guid>
      <description>Date: 2014-01-31 Time: 15:30-16:30 Location: BURN 1205 Abstract: I&amp;rsquo;ll introduce the exchangeable Kendall&amp;rsquo;s tau as a nonparametric intra class association measure in a clustered data frame and provide an estimator for this measure. The asymptotic properties of this estimator are investigated under a multivariate exchangeable cdf. Two applications of the proposed statistic are considered. The first is an estimator of the intraclass correlation coefficient for data drawn from an elliptical distribution.</description>
    </item>
    
    <item>
      <title>Calibration of computer experiments with large data structures</title>
      <link>/post/2014winter/2014-01-24/</link>
      <pubDate>Fri, 24 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-01-24/</guid>
      <description>Date: 2014-01-24 Time: 15:30-16:30 Location: Salle 1355, pavillon André-Aisenstadt (CRM) Abstract: Statistical model calibration of computer models is commonly done in a wide variety of scientific endeavours. In the end, this exercise amounts to solving an inverse problem and a form of regression. Gaussian process model are very convenient in this setting as non-parametric regression estimators and provide sensible inference properties. However, when the data structures are large, fitting the model becomes difficult.</description>
    </item>
    
    <item>
      <title>An introduction to stochastic partial differential equations and intermittency</title>
      <link>/post/2014winter/2014-01-10/</link>
      <pubDate>Fri, 10 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/post/2014winter/2014-01-10/</guid>
      <description>Date: 2014-01-10 Time: 15:30-16:30 Location: BURN 1205 Abstract: In a seminal article in 1944, Itô introduced the stochastic integral with respect to the Brownian motion, which turned out to be one of the most fruitful ideas in mathematics in the 20th century. This lead to the development of stochastic analysis, a field which includes the study of stochastic partial differential equations (SPDEs). One of the approaches for the study of SPDEs was initiated by Walsh (1986) and relies on the concept of random-field solution for equations perturbed by a space-time white noise (or Brownian sheet).</description>
    </item>
    
    <item>
      <title>Great probabilists publish posthumously</title>
      <link>/post/2013fall/2013-12-06/</link>
      <pubDate>Fri, 06 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-12-06/</guid>
      <description>Date: 2013-12-06 Time: 15:30-16:30 Location: UQAM Salle SH-3420 Abstract: Jacob Bernoulli died in 1705. His great book Ars Conjectandi was published in 1713, 300 years ago. Thomas Bayes died in 1761. His great paper was read to the Royal Society of London in December 1763, 250 years ago, and published in 1764. These anniversaries are noted by discussing new evidence regarding the circumstances of publication, which in turn can lead to a better understanding of the works themselves.</description>
    </item>
    
    <item>
      <title>Signal detection in high dimension: Testing sphericity against spiked alternatives</title>
      <link>/post/2013fall/2013-11-29/</link>
      <pubDate>Fri, 29 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-11-29/</guid>
      <description>Date: 2013-11-29 Time: 15:30-16:30 Location: Concordia MB-2.270 Abstract: We consider the problem of testing the null hypothesis of sphericity for a high-dimensional covariance matrix against the alternative of a finite (unspecified) number of symmetry-breaking directions (multispiked alternatives) from the point of view of the asymptotic theory of statistical experiments. The region lying below the so-called phase transition or impossibility threshold is shown to be a contiguity region. Simple analytical expressions are derived for the asymptotic power envelope and the asymptotic powers of existing tests.</description>
    </item>
    
    <item>
      <title>Tail order and its applications</title>
      <link>/post/2013fall/2013-11-22/</link>
      <pubDate>Fri, 22 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-11-22/</guid>
      <description>Date: 2013-11-22 Time: 15:30-16:30 Location: BURN 1205 Abstract: Tail order is a notion for quantifying the strength of dependence in the tail of a joint distribution. It can account for a wide range of dependence, ranging from tail positive dependence to tail negative dependence. We will introduce theory and applications of tail order. Conditions for tail orders of copula families will be discussed, and they are helpful in guiding us to find suitable copula families for statistical inference.</description>
    </item>
    
    <item>
      <title>Submodel selection and post estimation: Making sense or folly</title>
      <link>/post/2013fall/2013-11-15/</link>
      <pubDate>Fri, 15 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-11-15/</guid>
      <description>Date: 2013-11-15 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we consider estimation in generalized linear models when there are many potential predictors and some of them may not have influence on the response of interest. In the context of two competing models where one model includes all predictors and the other restricts variable coefficients to a candidate linear subspace based on subject matter or prior knowledge, we investigate the relative performances of Stein type shrinkage, pretest, and penalty estimators (L1GLM, adaptive L1GLM, and SCAD) with respect to the full model estimator.</description>
    </item>
    
    <item>
      <title>The inadequacy of the summed score (and how you can fix it!)</title>
      <link>/post/2013fall/2013-11-08/</link>
      <pubDate>Fri, 08 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-11-08/</guid>
      <description>Date: 2013-11-08 Time: 15:30-16:30 Location: BURN 1205 Abstract: Health researchers often use patient and physician questionnaires to assess certain aspects of health status. Item Response Theory (IRT) provides a set of tools for examining the properties of the instrument and for estimation of the latent trait for each individual. In my research, I critically examine the usefulness of the summed score over items and an alternative weighted summed score (using weights computed from the IRT model) as an alternative to both the empirical Bayes estimator and maximum likelihood estimator for the Generalized Partial Credit Model.</description>
    </item>
    
    <item>
      <title>Bayesian latent variable modelling of longitudinal family data for genetic pleiotropy studies</title>
      <link>/post/2013fall/2013-11-01/</link>
      <pubDate>Fri, 01 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-11-01/</guid>
      <description>Date: 2013-11-01 Time: 15:30-16:30 Location: BURN 1205 Abstract: Motivated by genetic association studies of pleiotropy, we propose a Bayesian latent variable approach to jointly study multiple outcomes or phenotypes. The proposed method models both continuous and binary phenotypes, and it accounts for serial and familial correlations when longitudinal and pedigree data have been collected. We present a Bayesian estimation method for the model parameters and we discuss some of the model misspecification effects.</description>
    </item>
    
    <item>
      <title>XY - Basketball meets Big Data</title>
      <link>/post/2013fall/2013-10-25/</link>
      <pubDate>Fri, 25 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-10-25/</guid>
      <description>Date: 2013-10-25 Time: 15:30-16:30 Location: HEC Montréal Salle CIBC 1er étage Abstract: In this talk, I will explore the state of the art in the analysis and modeling of player tracking data in the NBA. In the past, player tracking data has been used primarily for visualization, such as understanding the spatial distribution of a player’s shooting characteristics, or to extract summary statistics, such as the distance traveled by a player in a given game.</description>
    </item>
    
    <item>
      <title>Whole genome 3D architecture of chromatin and regulation</title>
      <link>/post/2013fall/2013-10-18/</link>
      <pubDate>Fri, 18 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-10-18/</guid>
      <description>Date: 2013-10-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: The expression of a gene is usually controlled by the regulatory elements in its promoter region. However, it has long been hypothesized that, in complex genomes, such as the human genome, a gene may be controlled by distant enhancers and repressors. A recent molecular technique, 3C (chromosome conformation capture), that uses formaldehyde cross-linking and locus-specific PCR, was able to detect physical contacts between distant genomic loci.</description>
    </item>
    
    <item>
      <title>Some recent developments in likelihood-based small area estimation</title>
      <link>/post/2013fall/2013-10-04/</link>
      <pubDate>Fri, 04 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-10-04/</guid>
      <description>Date: 2013-10-04 Time: 15:30-16:30 Location: BURN 1205 Abstract: Mixed models are commonly used for the analysis data in small area estimation. In particular, small area estimation has been extensively studied under linear mixed models. However, in practice there are many situations that we have counts or proportions in small area estimation; for example a (monthly) dataset on the number of incidences in small areas. Recently, small area estimation under the linear mixed model with penalized spline model, for xed part of the model, was studied.</description>
    </item>
    
    <item>
      <title>Measurement error and variable selection in parametric and nonparametric models</title>
      <link>/post/2013fall/2013-09-27/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-09-27/</guid>
      <description>Date: 2013-09-27 Time: 15:30-16:30 Location: RPHYS 114 Abstract: This talk will start with a discussion of the relationships between LASSO estimation, ridge regression, and attenuation due to measurement error as motivation for, and introduction to, a new generalizable approach to variable selection in parametric and nonparametric regression and discriminant analysis. The approach transcends the boundaries of parametric/nonparametric models. It will first be described in the familiar context of linear regression where its relationship to the LASSO will be described in detail.</description>
    </item>
    
    <item>
      <title>Tests of independence for sparse contingency tables and beyond</title>
      <link>/post/2013fall/2013-09-20/</link>
      <pubDate>Fri, 20 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-09-20/</guid>
      <description>Date: 2013-09-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, a new and consistent statistic is proposed to test whether two discrete random variables are independent. The test is based on a statistic of the Cramér–von Mises type constructed from the so-called empirical checkerboard copula. The test can be used even for sparse contingency tables or tables whose dimension changes with the sample size. Because the limiting distribution of the test statistic is not tractable, a valid bootstrap procedure for the computation of p-values will be discussed.</description>
    </item>
    
    <item>
      <title>Bayesian nonparametric density estimation under length bias sampling</title>
      <link>/post/2013fall/2013-09-13/</link>
      <pubDate>Fri, 13 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013fall/2013-09-13/</guid>
      <description>Date: 2013-09-13 Time: 15:30-16:30 Location: BURN 1205 Abstract: A new density estimation method in a Bayesian nonparametric framework is presented when recorded data are not coming directly from the distribution of interest, but from a length biased version. From a Bayesian perspective, efforts to computationally evaluate posterior quantities conditionally on length biased data were hindered by the inability to circumvent the problem of a normalizing constant. In this talk a novel Bayesian nonparametric approach to the length bias sampling problem is presented which circumvents the issue of the normalizing constant.</description>
    </item>
    
    <item>
      <title>Arup Bose: Consistency of large dimensional sample covariance matrix under weak dependence</title>
      <link>/post/2013winter/2013-04-12/</link>
      <pubDate>Fri, 12 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-04-12/</guid>
      <description>Date: 2013-04-12 Time: 14:30-15:30 Location: Concordia Abstract: Estimation of large dimensional covariance matrix has been of interest recently. One model assumes that there are $p$ dimensional independent identically distributed Gaussian observations $X_1, \ldots , X_n$ with dispersion matrix $\Sigma_p$ and $p$ grows much faster than $n$. Appropriate convergence rate results have been established in the literature for tapered and banded estimators of $\Sigma_p$ which are based on the sample variance covariance matrix of $n$ observations.</description>
    </item>
    
    <item>
      <title>Éric Marchand: On improved predictive density estimation with parametric constraints</title>
      <link>/post/2013winter/2013-04-05/</link>
      <pubDate>Fri, 05 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-04-05/</guid>
      <description>Date: 2013-04-05 Time: 14:30-15:30 Location: BURN 1205 Abstract: We consider the problem of predictive density estimation under Kullback-Leibler loss when the parameter space is restricted to a convex subset. The principal situation analyzed relates to the estimation of an unknown predictive p-variate normal density based on an observation generated by another p-variate normal density. The means of the densities are assumed to coincide, the covariance matrices are a known multiple of the identity matrix.</description>
    </item>
    
    <item>
      <title>Hélène Massam: The hyper Dirichlet revisited: a characterization</title>
      <link>/post/2013winter/2013-03-22/</link>
      <pubDate>Fri, 22 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-03-22/</guid>
      <description>Date: 2013-03-22 Time: 14:30-15:30 Location: BURN 107 Abstract: We give a characterization of the hyper Dirichlet distribution hyper Markov with respect to a decomposable graph $G$ (or equivalently a moral directed acyclic graph). For $X=(X_1,\ldots,X_d)$ following the hyper Dirichlet distribution, our characterization is through the so-called &amp;ldquo;local and global independence properties&amp;rdquo; for a carefully designed family of orders of the variables $X_1,\ldots,X_d$.
The hyper Dirichlet for general directed acyclic graphs was derived from a characterization of the Dirichlet distribution given by Geiger and Heckerman (1997).</description>
    </item>
    
    <item>
      <title>Jiahua Chen: Quantile and quantile function estimations under density ratio model</title>
      <link>/post/2013winter/2013-03-15/</link>
      <pubDate>Fri, 15 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-03-15/</guid>
      <description>Date: 2013-03-15 Time: 14:30-15:30 Location: BURN 1205 Abstract: Join work with Yukun Liu (East China Normal University)
Population quantiles and their functions are important parameters in many applications. For example, the lower level quantiles often serve as crucial quality indices of forestry products and others. In the presence of several independent samples from populations satisfying density ratio model, we investigate the properties of the empirical likelihood (EL) based inferences of quantiles and their functions.</description>
    </item>
    
    <item>
      <title>Natalia Stepanova: On asymptotic efficiency of some nonparametric tests for testing multivariate independence</title>
      <link>/post/2013winter/2013-03-01/</link>
      <pubDate>Fri, 01 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-03-01/</guid>
      <description>Date: 2013-03-01 Time: 14:30-15:30 Location: BURN 1205 Abstract: Some problems of statistics can be reduced to extremal problems of minimizing functionals of smooth functions defined on the cube $[0,1]^m$, $m\geq 2$. In this talk, we consider a class of extremal problems that is closely connected to the problem of testing multivariate independence. By solving the extremal problem, we provide a unified approach to establishing weak convergence for a wide class of empirical processes which emerge in connection with testing multivariate independence.</description>
    </item>
    
    <item>
      <title>Changbao Wu: Analysis of complex survey data with missing observations</title>
      <link>/post/2013winter/2013-02-22/</link>
      <pubDate>Fri, 22 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-02-22/</guid>
      <description>Date: 2013-02-22 Time: 14:30-15:30 Location: CRM, Université de Montréal, Pav. André-Ainsenstadt, salle 1360 Abstract: In this talk, we first provide an overview of issues arising from and methods dealing with complex survey data in the presence of missing observations, with a major focus on the estimating equation approach for analysis and imputation methods for missing data. We then propose a semiparametric fractional imputation method for handling item nonresponses, assuming certain baseline auxiliary variables can be observed for all units in the sample.</description>
    </item>
    
    <item>
      <title>Eric Cormier: Data Driven Nonparametric Inference for Bivariate Extreme-Value Copulas</title>
      <link>/post/2013winter/2013-02-15/</link>
      <pubDate>Fri, 15 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-02-15/</guid>
      <description>Date: 2013-02-15 Time: 14:30-15:30 Location: BURN 1205 Abstract: It is often crucial to know whether the dependence structure of a bivariate distribution belongs to the class of extreme-­‐value copulas. In this talk, I will describe a graphical tool that allows judgment regarding the existence of extreme-­‐value dependence. I will also present a data-­‐ driven nonparametric estimator of the Pickands dependence function. This estimator, which is constructed from constrained b-­‐splines, is intrinsic and differentiable, thereby enabling sampling from the fitted model.</description>
    </item>
    
    <item>
      <title>Celia Greenwood: Multiple testing and region-based tests of rare genetic variation</title>
      <link>/post/2013winter/2013-02-08/</link>
      <pubDate>Fri, 08 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-02-08/</guid>
      <description>Date: 2013-02-08 Time: 14:30-15:30 Location: BURN 1205 Abstract: In the context of univariate association tests between a trait of interest and common genetic variants (SNPs) across the whole genome, corrections for multiple testing have been well-studied. Due to the patterns of correlation (i.e. linkage disequilibrium), the number of independent tests remains close to 1 million, even when many more common genetic markers are available. With the advent of the DNA sequencing era, however, newly-identified genetic variants tend to be rare or even unique, and consequently single-variant tests of association have little power.</description>
    </item>
    
    <item>
      <title>Daniela Witten: Structured learning of multiple Gaussian graphical models</title>
      <link>/post/2013winter/2013-02-01/</link>
      <pubDate>Fri, 01 Feb 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-02-01/</guid>
      <description>Date: 2013-02-01 Time: 14:30-15:30 Location: BURN 1205 Abstract: I will consider the task of estimating high-dimensional Gaussian graphical models (or networks) corresponding to a single set of features under several distinct conditions. In other words, I wish to estimate several distinct but related networks. I assume that most aspects of the networks are shared, but that there are some structured differences between them. The goal is to exploit the similarity among the networks in order to obtain more accurate estimates of each individual network, as well as to identify the differences between the networks.</description>
    </item>
    
    <item>
      <title>Mylène Bédard: On the empirical efficiency of local MCMC algorithms with pools of proposals</title>
      <link>/post/2013winter/2013-01-25/</link>
      <pubDate>Fri, 25 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-01-25/</guid>
      <description>Date: 2013-01-25 Time: 14:30-15:30 Location: BURN 1205 Abstract: In an attempt to improve on the Metropolis algorithm, various MCMC methods with auxiliary variables, such as the multiple-try and delayed rejection Metropolis algorithms, have been proposed. These methods generate several candidates in a single iteration; accordingly they are computationally more intensive than the Metropolis algorithm. It is usually difficult to provide a general estimate for the computational cost of a method without being overly conservative; potentially efficient methods could thus be overlooked by relying on such estimates.</description>
    </item>
    
    <item>
      <title>Victor Chernozhukov: Inference on treatment effects after selection amongst high-dimensional controls</title>
      <link>/post/2013winter/2013-01-18/</link>
      <pubDate>Fri, 18 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-01-18/</guid>
      <description>Date: 2013-01-18 Time: 14:30-15:30 Location: BURN 306 Abstract: We propose robust methods for inference on the effect of a treatment variable on a scalar outcome in the presence of very many controls. Our setting is a partially linear model with possibly non-Gaussian and heteroscedastic disturbances. Our analysis allows the number of controls to be much larger than the sample size. To make informative inference feasible, we require the model to be approximately sparse; that is, we require that the effect of confounding factors can be controlled for up to a small approximation error by conditioning on a relatively small number of controls whose identities are unknown.</description>
    </item>
    
    <item>
      <title>Ana Best: Risk-set sampling, left truncation, and Bayesian methods in survival analysis</title>
      <link>/post/2013winter/2013-01-11/</link>
      <pubDate>Fri, 11 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>/post/2013winter/2013-01-11/</guid>
      <description>Date: 2013-01-11 Time: 14:30-15:30 Location: BURN 1205 Abstract: Statisticians are often faced with budget concerns when conducting studies. The collection of some covariates, such as genetic data, is very expensive. Other covariates, such as detailed histories, might be difficult or time-consuming to measure. This helped bring about the invention of the nested case-control study, and its more generalized version, risk-set sampled survival analysis. The literature has a good discussion of the properties of risk-set sampling in standard right-censored survival data.</description>
    </item>
    
    <item>
      <title>What percentage of children in the U.S. are eating a healthy diet? A statistical approach</title>
      <link>/post/2012fall/2012-12-14/</link>
      <pubDate>Fri, 14 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-12-14/</guid>
      <description>Date: 2012-12-14 Time: 14:30-15:30 Location: Concordia, Room LB 921-04 Abstract: In the United States the preferred method of obtaining dietary intake data is the 24-hour dietary recall, yet the measure of most interest is usual or long-term average daily intake, which is impossible to measure. Thus, usual dietary intake is assessed with considerable measurement error. Also, diet represents numerous foods, nutrients and other components, each of which have distinctive attributes.</description>
    </item>
    
    <item>
      <title>Sample size and power determination for multiple comparison procedures aiming at rejecting at least r among m false hypotheses</title>
      <link>/post/2012fall/2012-12-07/</link>
      <pubDate>Fri, 07 Dec 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-12-07/</guid>
      <description>Date: 2012-12-07 Time: 14:30-15:30 Location: BURN 1205 Abstract: Multiple testing problems arise in a variety of situations, notably in clinical trials with multiple endpoints. In such cases, it is often of interest to reject either all hypotheses or at least one of them. More generally, the question arises as to whether one can reject at least r out of m hypotheses. Statistical tools addressing this issue are rare in the literature.</description>
    </item>
    
    <item>
      <title>Sharing confidential datasets using differential privacy</title>
      <link>/post/2012fall/2012-11-30/</link>
      <pubDate>Fri, 30 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-11-30/</guid>
      <description>Date: 2012-11-30 Time: 14:30-15:30 Location: BURN 1205 Abstract: While statistical agencies would like to share their data with researchers, they must also protect the confidentiality of the data provided by their respondents. To satisfy these two conflicting objectives, agencies use various techniques to restrict and modify the data before publication. Most of these techniques however share a common flaw: their confidentiality protection can not be rigorously measured. In this talk, I will present the criterion of differential privacy, a rigorous measure of the protection offered by such methods.</description>
    </item>
    
    <item>
      <title>A nonparametric Bayesian model for local clustering</title>
      <link>/post/2012fall/2012-11-23/</link>
      <pubDate>Fri, 23 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-11-23/</guid>
      <description>Date: 2012-11-23 Time: 14:30-15:30 Location: BURN 107 Abstract: We propose a nonparametric Bayesian local clustering (NoB-LoC) approach for heterogeneous data. Using genomics data as an example, the NoB-LoC clusters genes into gene sets and simultaneously creates multiple partitions of samples, one for each gene set. In other words, the sample partitions are nested within the gene sets. Inference is guided by a joint probability model on all random elements. Biologically, the model formalizes the notion that biological samples cluster differently with respect to different genetic processes, and that each process is related to only a small subset of genes.</description>
    </item>
    
    <item>
      <title>Copula-based regression estimation and Inference</title>
      <link>/post/2012fall/2012-11-16/</link>
      <pubDate>Fri, 16 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-11-16/</guid>
      <description>Date: 2012-11-16 Time: 14:30-15:30 Location: BURN 1205 Abstract: In this paper we investigate a new approach of estimating a regression function based on copulas. The main idea behind this approach is to write the regression function in terms of a copula and marginal distributions. Once the copula and the marginal distributions are estimated we use the plug-in method to construct the new estimator. Because various methods are available in the literature for estimating both a copula and a distribution, this idea provides a rich and flexible alternative to many existing regression estimators.</description>
    </item>
    
    <item>
      <title>The multidimensional edge: Seeking hidden risks</title>
      <link>/post/2012fall/2012-11-09/</link>
      <pubDate>Fri, 09 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-11-09/</guid>
      <description>Date: 2012-11-09 Time: 14:30-15:30 Location: BURN 1205 Abstract: Assessing tail risks using the asymptotic models provided by multivariate extreme value theory has the danger that when asymptotic independence is present (as with the Gaussian copula model), the asymptotic model provides estimates of probabilities of joint tail regions that are zero. In diverse applications such as finance, telecommunications, insurance and environmental science, it may be difficult to believe in the absence of risk contagion.</description>
    </item>
    
    <item>
      <title>Multivariate extremal dependence: Estimation with bias correction</title>
      <link>/post/2012fall/2012-11-02/</link>
      <pubDate>Fri, 02 Nov 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-11-02/</guid>
      <description>Date: 2012-11-02 Time: 14:30-15:30 Location: BURN 1205 Abstract: Estimating extreme risks in a multivariate framework is highly connected with the estimation of the extremal dependence structure. This structure can be described via the stable tail dependence function L, for which several estimators have been introduced. Asymptotic normality is available for empirical estimates of L, with rate of convergence k^1&amp;frasl;2, where k denotes the number of high order statistics used in the estimation.</description>
    </item>
    
    <item>
      <title>Simulation model calibration and prediction using outputs from multi-fidelity simulators</title>
      <link>/post/2012fall/2012-10-26/</link>
      <pubDate>Fri, 26 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-10-26/</guid>
      <description>Date: 2012-10-26 Time: 14:30-15:30 Location: BURN 1205 Abstract: Computer simulators are used widely to describe physical processes in lieu of physical observations. In some cases, more than one computer code can be used to explore the same physical system - each with different degrees of fidelity. In this work, we combine field observations and model runs from deterministic multi-fidelity computer simulators to build a predictive model for the real process.</description>
    </item>
    
    <item>
      <title>Observational studies in healthcare: are they any good?</title>
      <link>/post/2012fall/2012-10-19/</link>
      <pubDate>Fri, 19 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-10-19/</guid>
      <description>Date: 2012-10-19 Time: 14:30-15:30 Location: UdeM Abstract: Observational healthcare data, such as administrative claims and electronic health records, play an increasingly prominent role in healthcare. Pharmacoepidemiologic studies in particular routinely estimate temporal associations between medical product exposure and subsequent health outcomes of interest, and such studies influence prescribing patterns and healthcare policy more generally. Some authors have questioned the reliability and accuracy of such studies, but few previous efforts have attempted to measure their performance.</description>
    </item>
    
    <item>
      <title>	Modeling operational risk using a Bayesian approach to EVT</title>
      <link>/post/2012fall/2012-10-12/</link>
      <pubDate>Fri, 12 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-10-12/</guid>
      <description>Date: 2012-10-12 Time: 14:30-15:30 Location: BURN 1205 Abstract: Extreme Value Theory has been widely used for assessing risk for highly unusual events, either by using block maxima or peaks over the threshold (POT) methods. However, one of the main drawbacks of the POT method is the choice of a threshold, which plays an important role in the estimation since the parameter estimates strongly depend on this value. Bayesian inference is an alternative to handle these difficulties; the threshold can be treated as another parameter in the estimation, avoiding the classical empirical approach.</description>
    </item>
    
    <item>
      <title>Markov switching regular vine copulas</title>
      <link>/post/2012fall/2012-10-05/</link>
      <pubDate>Fri, 05 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-10-05/</guid>
      <description>Date: 2012-10-05 Time: 14:30-15:30 Location: BURN 1205 Abstract: Using only bivariate copulas as building blocks, regular vines(R-vines) constitute a flexible class of high-dimensional dependence models. In this talk we introduce a Markov switching R-vine copula model, combining the flexibility of general R-vine copulas with the possibility for dependence structures to change over time. Frequentist as well as Bayesian parameter estimation is discussed. Further, we apply the newly proposed model to examine the dependence of exchange rates as well as stock and stock index returns.</description>
    </item>
    
    <item>
      <title>The current state of Q-learning for personalized medicine</title>
      <link>/post/2012fall/2012-09-28/</link>
      <pubDate>Fri, 28 Sep 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-09-28/</guid>
      <description>Date: 2012-09-28 Time: 14:30-15:30 Location: BURN 1205 Abstract: In this talk, I will provide an introduction to DTRs and an overview the state of the art (and science) of Q-learning, a popular tool in reinforcement learning. The use of Q-learning and its variance in randomized and non-randomized studies will be discussed, as well as issues concerning inference as the resulting estimators are not always regular. Current and future directions of interest will also be considered.</description>
    </item>
    
    <item>
      <title>Regularized semiparametric functional linear regression</title>
      <link>/post/2012fall/2012-09-21/</link>
      <pubDate>Fri, 21 Sep 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012fall/2012-09-21/</guid>
      <description>Date: 2012-09-21 Time: 14:30-15:30 Location: McGill, Burnside Hall 1214 Abstract: In many scientific experiments we need to face analysis with functional data, where the observations are sampled from random process, together with a potentially large number of non-functional covariates. The complex nature of functional data makes it difficult to directly apply existing methods to model selection and estimation. We propose and study a new class of penalized semiparametric functional linear regression to characterize the regression relation between a scalar response and multiple covariates, including both functional covariates and scalar covariates.</description>
    </item>
    
    <item>
      <title>Li: High-dimensional feature selection using hierarchical Bayesian logistic regression with heavy-tailed priors | Rao: Best predictive estimation for linear mixed models with applications to small area estimation</title>
      <link>/post/2012winter/2012-04-13/</link>
      <pubDate>Fri, 13 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-04-13/</guid>
      <description>Date: 2012-04-13 Time: 14:00-16:30 Location: MAASS 217 Abstract: Li: The problem of selecting the most useful features from a great many (eg, thousands) of candidates arises in many areas of modern sciences. An interesting problem from genomic research is that, from thousands of genes that are active (expressed) in certain tissue cells, we want to ﬁnd the genes that can be used to separate tissues of diﬀerent classes (eg. cancer and normal).</description>
    </item>
    
    <item>
      <title>Hypothesis testing in finite mixture models: from the likelihood ratio test to EM-test</title>
      <link>/post/2012winter/2012-04-05/</link>
      <pubDate>Thu, 05 Apr 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-04-05/</guid>
      <description>Date: 2012-04-05 Time: 15:30-16:30 Location: BURN 1205 Abstract: In the presence of heterogeneity, a mixture model is most natural to characterize the random behavior of the samples taken from such populations. Such strategy has been widely employed in applications ranging from genetics, information technology, marketing, to finance. Studying the mixing structure behind a random sample from the population allows us to infer the degree of heterogeneity with important implications in applications such as the presence of disease subgroups in genetics.</description>
    </item>
    
    <item>
      <title>A matching-based approach to assessing the surrogate value of a biomarker</title>
      <link>/post/2012winter/2012-03-30/</link>
      <pubDate>Fri, 30 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-30/</guid>
      <description>Date: 2012-03-30 Time: 15:30-16:30 Location: BURN 1205 Abstract: Statisticians have developed a number of frameworks which can be used to assess the surrogate value of a biomarker, i.e. establish whether treatment effects on a biological quantity measured shortly after administration of treatment predict treatment effects on the clinical endpoint of interest. The most commonly applied of these frameworks is due to Prentice (1989), who proposed a set of criteria which a surrogate marker should satisfy.</description>
    </item>
    
    <item>
      <title>Model selection principles in misspecified models</title>
      <link>/post/2012winter/2012-03-23/</link>
      <pubDate>Fri, 23 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-23/</guid>
      <description>Date: 2012-03-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: Model selection is of fundamental importance to high-dimensional modeling featured in many contemporary applications. Classical principles of model selection include the Bayesian principle and the Kullback-Leibler divergence principle, which lead to the Bayesian information criterion and Akaike information criterion, respectively, when models are correctly specified. Yet model misspecification is unavoidable in practice. We derive novel asymptotic expansions of the two well-known principles in misspecified generalized linear models, which give the generalized BIC (GBIC) and generalized AIC.</description>
    </item>
    
    <item>
      <title>Variable selection in longitudinal data with a change-point</title>
      <link>/post/2012winter/2012-03-16/</link>
      <pubDate>Fri, 16 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-16/</guid>
      <description>Date: 2012-03-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: Follow-up studies are frequently carried out to investigate the evolution of measurements through time, taken on a set of subjects. These measurements (responses) are bound to be influenced by subject specific covariates and if a regression model is used the data analyst is faced with the problem of selecting those covariates that “best explain” the data. For example, in a clinical trial, subjects may be monitored for a response following the administration of a treatment with a view of selecting the covariates that are best predictive of a treatment response.</description>
    </item>
    
    <item>
      <title>Using tests of homoscedasticity to test missing completely at random | Hugh Chipman: Sequential optimization of a computer model and other Active Learning problems</title>
      <link>/post/2012winter/2012-03-09/</link>
      <pubDate>Fri, 09 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-09/</guid>
      <description>Date: 2012-03-09 Time: 14:00-16:30 Location: UQAM, 201 ave. du Président-Kennedy, salle 5115 Abstract: Li: The problem of selecting the most useful features from a great many (eg, thousands) of candidates arises in many areas of modern sciences. An interesting problem from genomic research is that, from thousands of genes that are active (expressed) in certain tissue cells, we want to ﬁnd the genes that can be used to separate tissues of diﬀerent classes (eg.</description>
    </item>
    
    <item>
      <title>Estimating a variance-covariance surface for functional and longitudinal data</title>
      <link>/post/2012winter/2012-03-02/</link>
      <pubDate>Fri, 02 Mar 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-03-02/</guid>
      <description>Date: 2012-03-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: In functional data analysis, as in its multivariate counterpart, estimates of the bivariate covariance kernel σ(s,t ) and its inverse are useful for many things, and we need the inverse of a covariance matrix or kernel especially often. However, the dimensionality of functional observations often exceeds the sample size available to estimate σ(s,t, and then the analogue S of the multivariate sample estimate is singular and non-invertible.</description>
    </item>
    
    <item>
      <title>McGillivray: A penalized quasi-likelihood approach for estimating the number of states in a hidden Markov model | Best: Risk-set sampling and left truncation in survival analysis</title>
      <link>/post/2012winter/2012-02-17/</link>
      <pubDate>Fri, 17 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-02-17/</guid>
      <description>Date: 2012-02-17 Time: 15:30-16:30 Location: BURN 1205 Abstract: McGillivray: In statistical applications of hidden Markov models (HMMs), one may have no knowledge of the number of hidden states (or order) of the model needed to be able to accurately represent the underlying process of the data. The problem of estimating the number of hidden states of the HMM is thus brought to the forefront. In this talk, we present a penalized quasi-likelihood approach for order estimation in HMMs which makes use of the fact that the marginal distribution of the observations from a HMM is a finite mixture model.</description>
    </item>
    
    <item>
      <title>Stute: Principal component analysis of the Poisson Process | Blath: Longterm properties of the symbiotic branching model</title>
      <link>/post/2012winter/2012-02-10/</link>
      <pubDate>Fri, 10 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-02-10/</guid>
      <description>Date: 2012-02-10 Time: 14:00-16:30 Location: Concordia Abstract: Stute: The Poisson Process constitutes a well-known model for describing random events over time. It has many applications in marketing research, insurance mathematics and finance. Though it has been studied for decades not much is known how to check (in a non-asymptotic way) the validity of the Poisson Process. In this talk we present the principal component decomposition of the Poisson Process which enables us to derive finite sample properties of associated goodness-of-fit tests.</description>
    </item>
    
    <item>
      <title>Du: Simultaneous fixed and random effects selection in finite mixtures of linear mixed-effects models | Harel: Measuring fatigue in systemic sclerosis: a comparison of the SF-36 vitality subscale and FACIT fatigue scale using item response theory</title>
      <link>/post/2012winter/2012-02-03/</link>
      <pubDate>Fri, 03 Feb 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-02-03/</guid>
      <description>Date: 2012-02-03 Time: 15:30-16:30 Location: BURN 1205 Abstract: Du: Linear mixed-effects (LME) models are frequently used for modeling longitudinal data. One complicating factor in the analysis of such data is that samples are sometimes obtained from a population with significant underlying heterogeneity, which would be hard to capture by a single LME model. Such problems may be addressed by a finite mixture of linear mixed-effects (FMLME) models, which segments the population into subpopulations and models each subpopulation by a distinct LME model.</description>
    </item>
    
    <item>
      <title>Applying Kalman filtering to problems in causal inference</title>
      <link>/post/2012winter/2012-01-27/</link>
      <pubDate>Fri, 27 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-01-27/</guid>
      <description>Date: 2012-01-27 Time: 15:30-16:30 Location: BURN 1205 Abstract: A common problem in observational studies is estimating the causal effect of time-varying treatment in the presence of a time varying confounder. When random assignment of subjects to comparison groups is not possible, time-varying confounders can cause bias in estimating causal effects even after standard regression adjustment if past treatment history is a predictor of future confounders. To eliminate the bias of standard methods for estimating the causal effect of time varying treatment, Robins developed a number of innovative methods for discrete treatment levels, including G-computation, G-estimation, and marginal structural models (MSMs).</description>
    </item>
    
    <item>
      <title>A concave regularization technique for sparse mixture models</title>
      <link>/post/2012winter/2012-01-20/</link>
      <pubDate>Fri, 20 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-01-20/</guid>
      <description>Date: 2012-01-20 Time: 15:30-16:30 Location: BURN 1205 Abstract: Latent variable mixture models are a powerful tool for exploring the structure in large datasets. A common challenge for interpreting such models is a desire to impose sparsity, the natural assumption that each data point only contains few latent features. Since mixture distributions are constrained in their L1 norm, typical sparsity techniques based on L1 regularization become toothless, and concave regularization becomes necessary.</description>
    </item>
    
    <item>
      <title>Bayesian approaches to evidence synthesis in clinical practice guideline development</title>
      <link>/post/2012winter/2012-01-13/</link>
      <pubDate>Fri, 13 Jan 2012 00:00:00 +0000</pubDate>
      
      <guid>/post/2012winter/2012-01-13/</guid>
      <description>Date: 2012-01-13 Time: 15:30-16:30 Location: Concordia, Library Building LB-921.04 Abstract: The American College of Cardiology Foundation (ACCF) and the American Heart Association (AHA) have jointly engaged in the production of guideline in the area of cardiovascular disease since 1980. The developed guidelines are intended to assist health care providers in clinical decision making by describing a range of generally acceptable approaches for the diagnosis, management, or prevention of specific diseases or conditions.</description>
    </item>
    
    <item>
      <title>Detecting evolution in experimental ecology: Diagnostics for missing state variables</title>
      <link>/post/2011fall/2011-12-09/</link>
      <pubDate>Fri, 09 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-12-09/</guid>
      <description>Date: 2011-12-09 Time: 15:30-16:30 Location: UQAM Salle 5115 Abstract: This talk considers goodness of fit diagnostics for time-series data from processes approximately modeled by systems of nonlinear ordinary differential equations. In particular, we seek to determine three nested causes of lack of fit: (i) unmodeled stochastic forcing, (ii) mis-specified functional forms and (iii) mis-specified state variables. Testing lack of fit in differential equations is challenging since the model is expressed in terms of rates of change of the measured variables.</description>
    </item>
    
    <item>
      <title>Path-dependent estimation of a distribution under generalized censoring</title>
      <link>/post/2011fall/2011-12-02/</link>
      <pubDate>Fri, 02 Dec 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-12-02/</guid>
      <description>Date: 2011-12-02 Time: 15:30-16:30 Location: BURN 1205 Abstract: This talk focuses on the problem of the estimation of a distribution on an arbitrary complete separable metric space when the data points are subject to censoring by a general class of random sets. A path-dependent estimator for the distribution is proposed; among other properties, the estimator is sequential in the sense that it only uses data preceding any fixed point at which it is evaluated.</description>
    </item>
    
    <item>
      <title>Estimation of the risk of a collision when using a cell phone while driving</title>
      <link>/post/2011fall/2011-11-25/</link>
      <pubDate>Fri, 25 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-11-25/</guid>
      <description>Date: 2011-11-25 Time: 15:30-16:30 Location: BURN 1205 Abstract: The use of cell phone while driving raises the question of whether it is associated with an increased collision risk and if so, what is its magnitude. For policy decision making, it is important to rely on an accurate estimate of the real crash risk of cell phone use while driving. Three important epidemiological studies were published on the subject, two using the case-crossover approach and one using a more conventional longitudinal cohort design.</description>
    </item>
    
    <item>
      <title>Construction of bivariate distributions via principal components</title>
      <link>/post/2011fall/2011-11-18/</link>
      <pubDate>Fri, 18 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-11-18/</guid>
      <description>Date: 2011-11-18 Time: 15:30-16:30 Location: BURN 1205 Abstract: The diagonal expansion of a bivariate distribution (Lancaster, 1958) has been used as a tool to construct bivariate distributions; this method has been generalized using principal dimensions of random variables (Cuadras 2002). Sufficient and necessary conditions are given for uniform, exponential, logistic and Pareto marginals in the one and two-dimensional case. The corresponding copulas are obtained.
Speaker Amparo Casanova is an Assistant Professor at the Dalla Lana School of Public Health, Division of Biostatistics, University of Toronto.</description>
    </item>
    
    <item>
      <title>Guérin: An ergodic variant of the telegraph process for a toy model of bacterial chemotaxis | Staicu: Skewed functional processes and their applications</title>
      <link>/post/2011fall/2011-11-11/</link>
      <pubDate>Fri, 11 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-11-11/</guid>
      <description>Date: 2011-11-11 Time: 14:00-16:30 Location: UdeM Abstract: Guérin: I will study the long time behavior of a variant of the classic telegraph process, with non-constant jump rates that induce a drift towards the origin. This process can be seen as a toy model for velocity-jump processes recently proposed as mathematical models of bacterial chemotaxis. I will give its invariant law and construct an explicit coupling for velocity and position, providing exponential ergodicity with moreover a quantitative control of the total variation distance to equilibrium at each time instant.</description>
    </item>
    
    <item>
      <title>A Bayesian method of parametric inference for diffusion processes</title>
      <link>/post/2011fall/2011-11-04/</link>
      <pubDate>Fri, 04 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-11-04/</guid>
      <description>Date: 2011-11-04 Time: 15:30-16:30 Location: BURN 1205 Abstract: Diffusion processes have been used to model a multitude of continuous-time phenomena in Engineering and the Natural Sciences, and as in this case, the volatility of financial assets. However, parametric inference has long been complicated by an intractable likelihood function. For many models the most effective solution involves a large amount of missing data for which the typical Gibbs sampler can be arbitrarily slow.</description>
    </item>
    
    <item>
      <title>Maximum likelihood estimation in network models</title>
      <link>/post/2011fall/2011-11-03/</link>
      <pubDate>Thu, 03 Nov 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-11-03/</guid>
      <description>Date: 2011-11-03 Time: 16:00-17:00 Location: BURN 1205 Abstract: This talk is concerned with maximum likelihood estimation (MLE) in exponential statistical models for networks (random graphs) and, in particular, with the beta model, a simple model for undirected graphs in which the degree sequence is the minimal sufficient statistic. The speaker will present necessary and sufficient conditions for the existence of the MLE of the beta model parameters that are based on a geometric object known as the polytope of degree sequences.</description>
    </item>
    
    <item>
      <title>Simulated method of moments estimation for copula-based multivariate models</title>
      <link>/post/2011fall/2011-10-28/</link>
      <pubDate>Fri, 28 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-10-28/</guid>
      <description>Date: 2011-10-28 Time: 15:00-16:00 Location: BURN 1205 Abstract: This paper considers the estimation of the parameters of a copula via a simulated method of moments type approach. This approach is attractive when the likelihood of the copula model is not known in closed form, or when the researcher has a set of dependence measures or other functionals of the copula, such as pricing errors, that are of particular interest. The proposed approach naturally also nests method of moments and generalized method of moments estimators.</description>
    </item>
    
    <item>
      <title>Bayesian modelling of GWAS data using linear mixed models</title>
      <link>/post/2011fall/2011-10-21/</link>
      <pubDate>Fri, 21 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-10-21/</guid>
      <description>Date: 2011-10-21 Time: 15:30-16:30 Location: BURN 1205 Abstract: Genome-wide association studies (GWAS) are used to identify physical positions (loci) on the genome where genetic variation is causally associated with a phenotype of interest at the population level. Typical studies are based on the measurement of several hundred thousand single nucleotide polymorphism (SNP) variants spread across the genome, in a few thousand individuals. The resulting datasets are large and require computationally efficient methods of statistical analysis.</description>
    </item>
    
    <item>
      <title>Dupuis: Modeling non-stationary extremes: The case of heat waves | Davis: Estimating extremal dependence in time series via the extremogram</title>
      <link>/post/2011fall/2011-10-14/</link>
      <pubDate>Fri, 14 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-10-14/</guid>
      <description>Date: 2011-10-14 Time: 14:00-16:30 Location: TROTTIER 1080 Abstract: Dupuis: Environmental processes are often non-stationary since climate patterns cause systematic seasonal effects and long-term climate changes cause trends. The usual limit models are not applicable for non-stationary processes, but models from standard extreme value theory can be used along with statistical modeling to provide useful inference. Traditional approaches include letting model parameters be a function of covariates or using time-varying thresholds.</description>
    </item>
    
    <item>
      <title>Nonexchangeability and radial asymmetry identification via bivariate quantiles, with financial applications</title>
      <link>/post/2011fall/2011-10-07/</link>
      <pubDate>Fri, 07 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-10-07/</guid>
      <description>Date: 2011-10-07 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, the following topics will be discussed: A class of bivariate probability integral transforms and Kendall distribution; bivariate quantile curves, central and lateral regions; non-exchangeability and radial asymmetry identification; new measures of nonexchangeability and radial asymmetry; financial applications and a few open problems (joint work with Flavio Ferreira).
Speaker Nikolai Kolev is a Professor of Statistics at the University of Sao Paulo, Brazil.</description>
    </item>
    
    <item>
      <title>Data sketching for cardinality and entropy estimation?</title>
      <link>/post/2011fall/2011-09-30/</link>
      <pubDate>Fri, 30 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-09-30/</guid>
      <description>Date: 2011-09-30 Time: 15:30-16:30 Location: BURN 1205 Abstract: Streaming data is ubiquitous in a wide range of areas from engineering and information technology, finance, and commerce, to atmospheric physics, and earth sciences. The online approximation of properties of data streams is of great interest, but this approximation process is hindered by the sheer size of the data and the speed at which it is generated. Data stream algorithms typically allow only one pass over the data, and maintain sub-linear representations of the data from which target properties can be inferred with high efficiency.</description>
    </item>
    
    <item>
      <title>What is singular learning theory?</title>
      <link>/post/2011fall/2011-09-23/</link>
      <pubDate>Fri, 23 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-09-23/</guid>
      <description>Date: 2011-09-23 Time: 15:30-16:30 Location: BURN 1205 Abstract: In this talk, we give a basic introduction to Sumio Watanabe&amp;rsquo;s Singular Learning Theory, as outlined in his book &amp;ldquo;Algebraic Geometry and Statistical Learning Theory&amp;rdquo;. Watanabe&amp;rsquo;s key insight to studying singular models was to use a deep result in algebraic geometry known as Hironaka&amp;rsquo;s Resolution of Singularities. This result allows him to reparametrize the model in a normal form so that central limit theorems can be applied.</description>
    </item>
    
    <item>
      <title>Inference and model selection for pair-copula constructions</title>
      <link>/post/2011fall/2011-09-16/</link>
      <pubDate>Fri, 16 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-09-16/</guid>
      <description>Date: 2011-09-16 Time: 15:30-16:30 Location: BURN 1205 Abstract: Pair-copula constructions (PCCs) provide an elegant way to construct highly flexible multivariate distributions. However, for convenience of inference, pair-copulas are often assumed to depend on the conditioning variables only indirectly. In this talk, I will show how nonparametric smoothing techniques can be used to avoid this assumption. Model selection for PCCs will also be addressed within the proposed method.
Speaker Elif F.</description>
    </item>
    
    <item>
      <title>Susko: Properties of Bayesian posteriors and bootstrap support in phylogenetic inference | Labbe: An integrated hierarchical Bayesian model for multivariate eQTL genetic mapping</title>
      <link>/post/2011fall/2011-09-09/</link>
      <pubDate>Fri, 09 Sep 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-09-09/</guid>
      <description>Date: 2011-09-09 Time: 14:00-16:30 Location: UdeM, Pav. André-Aisenstadt, SALLE 1360 Abstract: Susko: The data generated by large scale sequencing projects is complex, high-dimensional, multivariate discrete data. In studies of evolutionary biology, the parameter space of evolutionary trees is an unusual additional complication from a statistical perspective. In this talk I will briefly introduce the general approaches to utilizing sequence data in phylogenetic inference. A particular issue of interest in phylogenetic inference is assessments of uncertainty about the true tree or structures that might be present in it.</description>
    </item>
    
    <item>
      <title>Precision estimation for stereological volumes</title>
      <link>/post/2011fall/2011-08-31/</link>
      <pubDate>Wed, 31 Aug 2011 00:00:00 +0000</pubDate>
      
      <guid>/post/2011fall/2011-08-31/</guid>
      <description>Date: 2011-08-31 Time: 15:30-16:30 Location: BURN 1205 Abstract: Volume estimators based on Cavalieri’s principle are widely used in the bio- sciences. For example in neuroscience, where volumetric measurements of brain structures are of interest, systematic samples of serial sections are obtained by magnetic resonance imaging or by a physical cutting procedure. The volume v is then estimated by ˆv, which is the sum over the areas of the structure of interest in the section planes multiplied by the width of the sections, t &amp;gt; 0.</description>
    </item>
    
  </channel>
</rss>