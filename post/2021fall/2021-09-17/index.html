<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="pinterest" content="nopin">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta name="generator" content="Hugo 0.136.5">



<link rel="canonical" href="https://mcgillstat.github.io/post/2021fall/2021-09-17/">


    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/solarized_dark.min.css">
    <title>On the Minimal Error of Empirical Risk Minimization - McGill Statistics Seminars</title>
    
<meta name="description" content="&lt;h4 id=&#34;date-2021-09-17&#34;&gt;Date: 2021-09-17&lt;/h4&gt;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&lt;p&gt;In recent years, highly expressive machine learning models, i.e. models that can express rich classes of functions, are becoming more andmore commonly used due their success both in regression and classification tasks, such models are deep neural nets, kernel machines and more.From the classical theory statistics point of view (the minimax theory),rich models tend to have a higher minimax rate, i.e. any estimator musthave a high risk (a “worst case scenario” error). Therefore, it seems thatfor modern models the classical theory may be too conservative and strict.In this talk, we consider the most popular procedure for regressiontask, that is Empirical Risk Minimization with squared loss (ERM) andwe shall analyze its minimal squared error both in the random and thefixed design settings, under the assumption of a convex family of functions.Namely, the minimal squared error that the ERM attains on estimatingany function in our class in both settings.In the fixed design setting, we show that the error is governed bythe global complexity of the entire class. In contrast, in random design,the ERM may only adapt to simpler models if the local neighborhoodsaround the regression function are nearly as complex as the class itself,a somewhat counter-intuitive conclusion. We provide sharp lower boundsfor performance of ERM for both Donsker and non-Donsker classes. Thistalk is based on joint work with Alexander Rakhlin.&lt;/p&gt;">

<meta property="og:title" content="On the Minimal Error of Empirical Risk Minimization - McGill Statistics Seminars">
<meta property="og:type" content="article">
<meta property="og:url" content="https://mcgillstat.github.io/post/2021fall/2021-09-17/">
<meta property="og:image" content="https://mcgillstat.github.io/images/default.png">
<meta property="og:site_name" content="McGill Statistics Seminars">
<meta property="og:description" content="&lt;h4 id=&#34;date-2021-09-17&#34;&gt;Date: 2021-09-17&lt;/h4&gt;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&lt;p&gt;In recent years, highly expressive machine learning models, i.e. models that can express rich classes of functions, are becoming more andmore commonly used due their success both in regression and classification tasks, such models are deep neural nets, kernel machines and more.From the classical theory statistics point of view (the minimax theory),rich models tend to have a higher minimax rate, i.e. any estimator musthave a high risk (a “worst case scenario” error). Therefore, it seems thatfor modern models the classical theory may be too conservative and strict.In this talk, we consider the most popular procedure for regressiontask, that is Empirical Risk Minimization with squared loss (ERM) andwe shall analyze its minimal squared error both in the random and thefixed design settings, under the assumption of a convex family of functions.Namely, the minimal squared error that the ERM attains on estimatingany function in our class in both settings.In the fixed design setting, we show that the error is governed bythe global complexity of the entire class. In contrast, in random design,the ERM may only adapt to simpler models if the local neighborhoodsaround the regression function are nearly as complex as the class itself,a somewhat counter-intuitive conclusion. We provide sharp lower boundsfor performance of ERM for both Donsker and non-Donsker classes. Thistalk is based on joint work with Alexander Rakhlin.&lt;/p&gt;">
<meta property="og:locale" content="ja_JP">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="McGill Statistics Seminars">
<meta name="twitter:url" content="https://mcgillstat.github.io/post/2021fall/2021-09-17/">
<meta name="twitter:title" content="On the Minimal Error of Empirical Risk Minimization - McGill Statistics Seminars">
<meta name="twitter:description" content="&lt;h4 id=&#34;date-2021-09-17&#34;&gt;Date: 2021-09-17&lt;/h4&gt;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&lt;h4 id=&#34;httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&#34;&gt;https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09&lt;/a&gt;&lt;/h4&gt;&lt;h4 id=&#34;meeting-id-834-3668-6293&#34;&gt;Meeting ID: 834 3668 6293&lt;/h4&gt;&lt;h4 id=&#34;passcode-12345&#34;&gt;Passcode: 12345&lt;/h4&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&lt;p&gt;In recent years, highly expressive machine learning models, i.e. models that can express rich classes of functions, are becoming more andmore commonly used due their success both in regression and classification tasks, such models are deep neural nets, kernel machines and more.From the classical theory statistics point of view (the minimax theory),rich models tend to have a higher minimax rate, i.e. any estimator musthave a high risk (a “worst case scenario” error). Therefore, it seems thatfor modern models the classical theory may be too conservative and strict.In this talk, we consider the most popular procedure for regressiontask, that is Empirical Risk Minimization with squared loss (ERM) andwe shall analyze its minimal squared error both in the random and thefixed design settings, under the assumption of a convex family of functions.Namely, the minimal squared error that the ERM attains on estimatingany function in our class in both settings.In the fixed design setting, we show that the error is governed bythe global complexity of the entire class. In contrast, in random design,the ERM may only adapt to simpler models if the local neighborhoodsaround the regression function are nearly as complex as the class itself,a somewhat counter-intuitive conclusion. We provide sharp lower boundsfor performance of ERM for both Donsker and non-Donsker classes. Thistalk is based on joint work with Alexander Rakhlin.&lt;/p&gt;">
<meta name="twitter:image" content="https://mcgillstat.github.io/images/default.png">


<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "NewsArticle",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id":"https:\/\/mcgillstat.github.io\/"
    },
    "headline": "On the Minimal Error of Empirical Risk Minimization - McGill Statistics Seminars",
    "image": {
      "@type": "ImageObject",
      "url": "https:\/\/mcgillstat.github.io\/images\/default.png",
      "height": 800,
      "width": 800
    },
    "datePublished": "2021-09-17T00:00:00JST",
    "dateModified": "2021-09-17T00:00:00JST",
    "author": {
      "@type": "Person",
      "name": "McGill Statistics Seminars"
    },
    "publisher": {
      "@type": "Organization",
      "name": "McGill Statistics Seminars",
      "logo": {
        "@type": "ImageObject",
        "url": "https:\/\/mcgillstat.github.io\/images/logo.png",
        "width": 600,
        "height": 60
      }
    },
    "description": "\u003ch4 id=\u0022date-2021-09-17\u0022\u003eDate: 2021-09-17\u003c\/h4\u003e\n\u003ch4 id=\u0022time-1530-1630-montreal-time\u0022\u003eTime: 15:30-16:30 (Montreal time)\u003c\/h4\u003e\n\u003ch4 id=\u0022httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09\u0022\u003e\u003ca href=\u0022https:\/\/mcgill.zoom.us\/j\/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09\u0022\u003ehttps:\/\/mcgill.zoom.us\/j\/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09\u003c\/a\u003e\u003c\/h4\u003e\n\u003ch4 id=\u0022meeting-id-834-3668-6293\u0022\u003eMeeting ID: 834 3668 6293\u003c\/h4\u003e\n\u003ch4 id=\u0022passcode-12345\u0022\u003ePasscode: 12345\u003c\/h4\u003e\n\u003ch2 id=\u0022abstract\u0022\u003eAbstract:\u003c\/h2\u003e\n\u003cp\u003eIn recent years, highly expressive machine learning models, i.e. models that can express rich classes of functions, are becoming more and\nmore commonly used due their success both in regression and classification tasks, such models are deep neural nets, kernel machines and more.\nFrom the classical theory statistics point of view (the minimax theory),\nrich models tend to have a higher minimax rate, i.e. any estimator must\nhave a high risk (a “worst case scenario” error). Therefore, it seems that\nfor modern models the classical theory may be too conservative and strict.\nIn this talk, we consider the most popular procedure for regression\ntask, that is Empirical Risk Minimization with squared loss (ERM) and\nwe shall analyze its minimal squared error both in the random and the\nfixed design settings, under the assumption of a convex family of functions.\nNamely, the minimal squared error that the ERM attains on estimating\nany function in our class in both settings.\nIn the fixed design setting, we show that the error is governed by\nthe global complexity of the entire class. In contrast, in random design,\nthe ERM may only adapt to simpler models if the local neighborhoods\naround the regression function are nearly as complex as the class itself,\na somewhat counter-intuitive conclusion. We provide sharp lower bounds\nfor performance of ERM for both Donsker and non-Donsker classes. This\ntalk is based on joint work with Alexander Rakhlin.\u003c\/p\u003e"
  }
</script>


    <link href="https://mcgillstat.github.io/css/styles.css" rel="stylesheet">
    

  </head>

  <body>
    
    
    

    <header class="l-header">
      <nav class="navbar navbar-default">
        <div class="container">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://mcgillstat.github.io/">McGill Statistics Seminars</a>
          </div>

          
          <div id="navbar" class="collapse navbar-collapse">
            
            <ul class="nav navbar-nav navbar-right">
              
              
              <li><a href="/">Current Seminar Series</a></li>
              
              
              
              <li><a href="/post/">Past Seminar Series</a></li>
              
              
            </ul>
            
          </div>
          

        </div>
      </nav>
    </header>

    <main>
      <div class="container">
        
<div class="row">
  <div class="col-md-8">

    <nav class="p-crumb">
      <ol class="breadcrumb">
        <li><a href="https://mcgillstat.github.io/"><i class="fa fa-home" aria-hidden="true"></i></a></li>
        
        <li itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb"><a href="https://mcgillstat.github.io/post/" itemprop="url"><span itemprop="title">post</span></a></li>
        
        <li class="active">Gil Kur</li>
      </ol>
    </nav>

    <article class="single">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2021-09-17T00:00:00JST">Sep 17, 2021</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="https://mcgillstat.github.io/post/">post</a></li>
      
    </ul>

    <h2 class="title">On the Minimal Error of Empirical Risk Minimization</h1>
    <h3 class="post-meta">Gil Kur </h3>
    
  </header>

  

  <div class="article-body"><h4 id="date-2021-09-17">Date: 2021-09-17</h4>
<h4 id="time-1530-1630-montreal-time">Time: 15:30-16:30 (Montreal time)</h4>
<h4 id="httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09httpsmcgillzoomusj83436686293pwdb0rmwmlxrxe3owr6nlnicwf5d0djqt09"><a href="https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09">https://mcgill.zoom.us/j/83436686293?pwd=b0RmWmlXRXE3OWR6NlNIcWF5d0dJQT09</a></h4>
<h4 id="meeting-id-834-3668-6293">Meeting ID: 834 3668 6293</h4>
<h4 id="passcode-12345">Passcode: 12345</h4>
<h2 id="abstract">Abstract:</h2>
<p>In recent years, highly expressive machine learning models, i.e. models that can express rich classes of functions, are becoming more and
more commonly used due their success both in regression and classification tasks, such models are deep neural nets, kernel machines and more.
From the classical theory statistics point of view (the minimax theory),
rich models tend to have a higher minimax rate, i.e. any estimator must
have a high risk (a “worst case scenario” error). Therefore, it seems that
for modern models the classical theory may be too conservative and strict.
In this talk, we consider the most popular procedure for regression
task, that is Empirical Risk Minimization with squared loss (ERM) and
we shall analyze its minimal squared error both in the random and the
fixed design settings, under the assumption of a convex family of functions.
Namely, the minimal squared error that the ERM attains on estimating
any function in our class in both settings.
In the fixed design setting, we show that the error is governed by
the global complexity of the entire class. In contrast, in random design,
the ERM may only adapt to simpler models if the local neighborhoods
around the regression function are nearly as complex as the class itself,
a somewhat counter-intuitive conclusion. We provide sharp lower bounds
for performance of ERM for both Donsker and non-Donsker classes. This
talk is based on joint work with Alexander Rakhlin.</p>
<h2 id="speaker">Speaker</h2>
<p>Gil Kur is a PhD student MIT. He is currently a visiting scholar working with Professor Elliot Paquette.  His research focuses on nonparametric statistics, high dimensional statistics and convex geometry.</p>
</div>

  <footer class="article-footer">
    
    
    
    <section class="bordered">
      <header>
        <div class="panel-title">CATEGORIES</div>
      </header>
      <div>
        <ul class="p-terms">
          
          <li><a href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar/">McGill Statistics Seminar</a></li>
          
        </ul>
      </div>
    </section>
    
    
    
    <section class="bordered">
      <header>
        <div class="panel-title">TAGS</div>
      </header>
      <div>
        <ul class="p-terms">
          
          <li><a href="https://mcgillstat.github.io/tags/2021-fall/">2021 fall</a></li>
          
        </ul>
      </div>
    </section>
    
    
  </footer>

</article>


    
  </div>

  <div class="col-md-4">
    
<aside class="l-sidebar">

  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">Recent Talks</div>
    </div>
    <div class="list-group">
      
      <a href="https://mcgillstat.github.io/tags/2024-fall/" class="list-group-item"> · Nov 8, 2024</a>
      
      <a href="https://mcgillstat.github.io/post/2024fall/2024-11-08/" class="list-group-item">Christian Genest · Nov 8, 2024</a>
      
      <a href="https://mcgillstat.github.io/categories/" class="list-group-item"> · Nov 8, 2024</a>
      
      <a href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar/" class="list-group-item"> · Nov 8, 2024</a>
      
      <a href="https://mcgillstat.github.io/tags/" class="list-group-item"> · Nov 8, 2024</a>
      
    </div>
  </section>

  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">CATEGORY</div>
    </div>
    <div class="list-group">
      
      <a href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar" class="list-group-item">mcgill statistics seminar</a>
      
      <a href="https://mcgillstat.github.io/categories/crm-ssc-prize-address" class="list-group-item">crm-ssc prize address</a>
      
      <a href="https://mcgillstat.github.io/categories/crm-colloquium" class="list-group-item">crm-colloquium</a>
      
    </div>
  </section>
  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">TAG</div>
    </div>
    <div class="list-group">
      
      <a href="https://mcgillstat.github.io/tags/2024-winter" class="list-group-item">2024 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2024-fall" class="list-group-item">2024 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2023-winter" class="list-group-item">2023 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2023-summer" class="list-group-item">2023 summer</a>
      
      <a href="https://mcgillstat.github.io/tags/2023-fall" class="list-group-item">2023 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2022-winter" class="list-group-item">2022 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2022-fall" class="list-group-item">2022 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2021-winter" class="list-group-item">2021 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2021-fall" class="list-group-item">2021 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2020-winter" class="list-group-item">2020 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2020-fall" class="list-group-item">2020 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2019-winter" class="list-group-item">2019 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2019-fall" class="list-group-item">2019 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2018-winter" class="list-group-item">2018 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2018-fall" class="list-group-item">2018 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2017-winter" class="list-group-item">2017 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2017-fall" class="list-group-item">2017 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2016-winter" class="list-group-item">2016 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2016-fall" class="list-group-item">2016 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2015-winter" class="list-group-item">2015 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2015-fall" class="list-group-item">2015 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2014-winter" class="list-group-item">2014 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2014-fall" class="list-group-item">2014 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2013-winter" class="list-group-item">2013 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2013-fall" class="list-group-item">2013 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2012-winter" class="list-group-item">2012 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2012-fall" class="list-group-item">2012 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2011-fall" class="list-group-item">2011 fall</a>
      
    </div>
  </section>
  

</aside>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    
  </div>
</div>

      </div>
    </main>

    <footer class="l-footer">
      <div class="container">
        <p><span class="h-logo">&copy; McGill Statistics Seminars</span></p>
        <aside>
          <p><a href="http://www.mcgill.ca/mathstat/">Department of Mathematics and Statistics</a>.</p>
          <p><a href="https://www.mcgill.ca/">McGill University</a></p>
        </aside>
      </div>
    </footer>

    <script src="//code.jquery.com/jquery-3.1.1.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

