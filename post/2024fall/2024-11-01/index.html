<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="pinterest" content="nopin">
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">
<meta name="generator" content="Hugo 0.136.5">



<link rel="canonical" href="https://mcgillstat.github.io/post/2024fall/2024-11-01/">


    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/solarized_dark.min.css">
    <title>On Mixture of Experts in Large-Scale Statistical Machine Learning Applications - McGill Statistics Seminars</title>
    
<meta name="description" content="&lt;h4 id=&#34;date-2024-11-01&#34;&gt;Date: 2024-11-01&lt;/h4&gt;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&lt;h4 id=&#34;httpsmcgillzoomusj81284191962httpsmcgillzoomusj81284191962&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81284191962&#34;&gt;https://mcgill.zoom.us/j/81284191962&lt;/a&gt;&lt;/h4&gt;&lt;h4 id=&#34;meeting-id-812-8419-1962&#34;&gt;Meeting ID: 812 8419 1962&lt;/h4&gt;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&lt;p&gt;Mixtures of experts (MoEs), a class of statistical machine learning models that combine multiple models, known as experts, to form more complex and accurate models, have been combinedinto deep learning architectures to improve the ability of these architectures and AI models to capture the heterogeneity of the data and to scale up these architectures without increasing the computationalcost. In mixtures of experts, each expert specializes in a different aspect of the data, which is then combined with a gating function to produce the final output. Therefore, parameter and expert estimatesplay a crucial role by enabling statisticians and data scientists to articulate and make sense of the diverse patterns present in the data. However, the statistical behaviors of parameters and experts in a mixtureof experts have remained unsolved, which is due to the complex interaction between gating function and expert parameters.&lt;/p&gt;">

<meta property="og:title" content="On Mixture of Experts in Large-Scale Statistical Machine Learning Applications - McGill Statistics Seminars">
<meta property="og:type" content="article">
<meta property="og:url" content="https://mcgillstat.github.io/post/2024fall/2024-11-01/">
<meta property="og:image" content="https://mcgillstat.github.io/images/default.png">
<meta property="og:site_name" content="McGill Statistics Seminars">
<meta property="og:description" content="&lt;h4 id=&#34;date-2024-11-01&#34;&gt;Date: 2024-11-01&lt;/h4&gt;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&lt;h4 id=&#34;httpsmcgillzoomusj81284191962httpsmcgillzoomusj81284191962&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81284191962&#34;&gt;https://mcgill.zoom.us/j/81284191962&lt;/a&gt;&lt;/h4&gt;&lt;h4 id=&#34;meeting-id-812-8419-1962&#34;&gt;Meeting ID: 812 8419 1962&lt;/h4&gt;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&lt;p&gt;Mixtures of experts (MoEs), a class of statistical machine learning models that combine multiple models, known as experts, to form more complex and accurate models, have been combinedinto deep learning architectures to improve the ability of these architectures and AI models to capture the heterogeneity of the data and to scale up these architectures without increasing the computationalcost. In mixtures of experts, each expert specializes in a different aspect of the data, which is then combined with a gating function to produce the final output. Therefore, parameter and expert estimatesplay a crucial role by enabling statisticians and data scientists to articulate and make sense of the diverse patterns present in the data. However, the statistical behaviors of parameters and experts in a mixtureof experts have remained unsolved, which is due to the complex interaction between gating function and expert parameters.&lt;/p&gt;">
<meta property="og:locale" content="ja_JP">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="McGill Statistics Seminars">
<meta name="twitter:url" content="https://mcgillstat.github.io/post/2024fall/2024-11-01/">
<meta name="twitter:title" content="On Mixture of Experts in Large-Scale Statistical Machine Learning Applications - McGill Statistics Seminars">
<meta name="twitter:description" content="&lt;h4 id=&#34;date-2024-11-01&#34;&gt;Date: 2024-11-01&lt;/h4&gt;&lt;h4 id=&#34;time-1530-1630-montreal-time&#34;&gt;Time: 15:30-16:30 (Montreal time)&lt;/h4&gt;&lt;h4 id=&#34;location-in-person-burnside-1104&#34;&gt;Location: In person, Burnside 1104&lt;/h4&gt;&lt;h4 id=&#34;httpsmcgillzoomusj81284191962httpsmcgillzoomusj81284191962&#34;&gt;&lt;a href=&#34;https://mcgill.zoom.us/j/81284191962&#34;&gt;https://mcgill.zoom.us/j/81284191962&lt;/a&gt;&lt;/h4&gt;&lt;h4 id=&#34;meeting-id-812-8419-1962&#34;&gt;Meeting ID: 812 8419 1962&lt;/h4&gt;&lt;h4 id=&#34;passcode-none&#34;&gt;Passcode: None&lt;/h4&gt;&lt;h2 id=&#34;abstract&#34;&gt;Abstract:&lt;/h2&gt;&lt;p&gt;Mixtures of experts (MoEs), a class of statistical machine learning models that combine multiple models, known as experts, to form more complex and accurate models, have been combinedinto deep learning architectures to improve the ability of these architectures and AI models to capture the heterogeneity of the data and to scale up these architectures without increasing the computationalcost. In mixtures of experts, each expert specializes in a different aspect of the data, which is then combined with a gating function to produce the final output. Therefore, parameter and expert estimatesplay a crucial role by enabling statisticians and data scientists to articulate and make sense of the diverse patterns present in the data. However, the statistical behaviors of parameters and experts in a mixtureof experts have remained unsolved, which is due to the complex interaction between gating function and expert parameters.&lt;/p&gt;">
<meta name="twitter:image" content="https://mcgillstat.github.io/images/default.png">


<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "NewsArticle",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id":"https:\/\/mcgillstat.github.io\/"
    },
    "headline": "On Mixture of Experts in Large-Scale Statistical Machine Learning Applications - McGill Statistics Seminars",
    "image": {
      "@type": "ImageObject",
      "url": "https:\/\/mcgillstat.github.io\/images\/default.png",
      "height": 800,
      "width": 800
    },
    "datePublished": "2024-11-01T00:00:00JST",
    "dateModified": "2024-11-01T00:00:00JST",
    "author": {
      "@type": "Person",
      "name": "McGill Statistics Seminars"
    },
    "publisher": {
      "@type": "Organization",
      "name": "McGill Statistics Seminars",
      "logo": {
        "@type": "ImageObject",
        "url": "https:\/\/mcgillstat.github.io\/images/logo.png",
        "width": 600,
        "height": 60
      }
    },
    "description": "\u003ch4 id=\u0022date-2024-11-01\u0022\u003eDate: 2024-11-01\u003c\/h4\u003e\n\u003ch4 id=\u0022time-1530-1630-montreal-time\u0022\u003eTime: 15:30-16:30 (Montreal time)\u003c\/h4\u003e\n\u003ch4 id=\u0022location-in-person-burnside-1104\u0022\u003eLocation: In person, Burnside 1104\u003c\/h4\u003e\n\u003ch4 id=\u0022httpsmcgillzoomusj81284191962httpsmcgillzoomusj81284191962\u0022\u003e\u003ca href=\u0022https:\/\/mcgill.zoom.us\/j\/81284191962\u0022\u003ehttps:\/\/mcgill.zoom.us\/j\/81284191962\u003c\/a\u003e\u003c\/h4\u003e\n\u003ch4 id=\u0022meeting-id-812-8419-1962\u0022\u003eMeeting ID: 812 8419 1962\u003c\/h4\u003e\n\u003ch4 id=\u0022passcode-none\u0022\u003ePasscode: None\u003c\/h4\u003e\n\u003ch2 id=\u0022abstract\u0022\u003eAbstract:\u003c\/h2\u003e\n\u003cp\u003eMixtures of experts (MoEs), a class of statistical machine learning models that combine multiple models, known as experts, to form more complex and accurate models, have been combined\ninto deep learning architectures to improve the ability of these architectures and AI models to capture the heterogeneity of the data and to scale up these architectures without increasing the computational\ncost. In mixtures of experts, each expert specializes in a different aspect of the data, which is then combined with a gating function to produce the final output. Therefore, parameter and expert estimates\nplay a crucial role by enabling statisticians and data scientists to articulate and make sense of the diverse patterns present in the data. However, the statistical behaviors of parameters and experts in a mixture\nof experts have remained unsolved, which is due to the complex interaction between gating function and expert parameters.\u003c\/p\u003e"
  }
</script>


    <link href="https://mcgillstat.github.io/css/styles.css" rel="stylesheet">
    

  </head>

  <body>
    
    
    

    <header class="l-header">
      <nav class="navbar navbar-default">
        <div class="container">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://mcgillstat.github.io/">McGill Statistics Seminars</a>
          </div>

          
          <div id="navbar" class="collapse navbar-collapse">
            
            <ul class="nav navbar-nav navbar-right">
              
              
              <li><a href="/">Current Seminar Series</a></li>
              
              
              
              <li><a href="/post/">Past Seminar Series</a></li>
              
              
            </ul>
            
          </div>
          

        </div>
      </nav>
    </header>

    <main>
      <div class="container">
        
<div class="row">
  <div class="col-md-8">

    <nav class="p-crumb">
      <ol class="breadcrumb">
        <li><a href="https://mcgillstat.github.io/"><i class="fa fa-home" aria-hidden="true"></i></a></li>
        
        <li itemscope="" itemtype="http://data-vocabulary.org/Breadcrumb"><a href="https://mcgillstat.github.io/post/" itemprop="url"><span itemprop="title">post</span></a></li>
        
        <li class="active">Nhat Ho</li>
      </ol>
    </nav>

    <article class="single">
  <header>
    <ul class="p-facts">
      <li><i class="fa fa-calendar" aria-hidden="true"></i><time datetime="2024-11-01T00:00:00JST">Nov 1, 2024</time></li>
      <li><i class="fa fa-bookmark" aria-hidden="true"></i><a href="https://mcgillstat.github.io/post/">post</a></li>
      
    </ul>

    <h2 class="title">On Mixture of Experts in Large-Scale Statistical Machine Learning Applications</h1>
    <h3 class="post-meta">Nhat Ho </h3>
    
  </header>

  

  <div class="article-body"><h4 id="date-2024-11-01">Date: 2024-11-01</h4>
<h4 id="time-1530-1630-montreal-time">Time: 15:30-16:30 (Montreal time)</h4>
<h4 id="location-in-person-burnside-1104">Location: In person, Burnside 1104</h4>
<h4 id="httpsmcgillzoomusj81284191962httpsmcgillzoomusj81284191962"><a href="https://mcgill.zoom.us/j/81284191962">https://mcgill.zoom.us/j/81284191962</a></h4>
<h4 id="meeting-id-812-8419-1962">Meeting ID: 812 8419 1962</h4>
<h4 id="passcode-none">Passcode: None</h4>
<h2 id="abstract">Abstract:</h2>
<p>Mixtures of experts (MoEs), a class of statistical machine learning models that combine multiple models, known as experts, to form more complex and accurate models, have been combined
into deep learning architectures to improve the ability of these architectures and AI models to capture the heterogeneity of the data and to scale up these architectures without increasing the computational
cost. In mixtures of experts, each expert specializes in a different aspect of the data, which is then combined with a gating function to produce the final output. Therefore, parameter and expert estimates
play a crucial role by enabling statisticians and data scientists to articulate and make sense of the diverse patterns present in the data. However, the statistical behaviors of parameters and experts in a mixture
of experts have remained unsolved, which is due to the complex interaction between gating function and expert parameters.</p>
<p>In the first part of the talk, we investigate the performance of the least squares estimators (LSE) under a deterministic MoEs model where the data are sampled according to a regression model, a
setting that has remained largely unexplored. We establish a condition called strong identifiability to characterize the convergence behavior of various types of expert functions. We demonstrate that the
rates for estimating strongly identifiable experts, namely the widely used feed-forward networks with activation functions sigmoid(·) and tanh(·), are substantially faster than those of polynomial experts,
which we show to exhibit a surprising slow estimation rate.</p>
<p>In the second part of the talk, we show that the insights from theories shed light into understanding and improving important practical applications in machine learning and artificial intelligence (AI), in-
cluding effectively scaling up massive AI models with several billion parameters, efficiently finetuning large-scale AI models for downstream tasks, and enhancing the performance of Transformer model,
state-of-the-art deep learning architecture, with a novel self-attention mechanism.</p>
<h2 id="speaker">Speaker</h2>
<p>Nhat Ho is currently an Assistant Professor of Statistics and Data Science at the University of Texas at Austin. He is a core member of the University of Texas, Austin Machine Learning Laboratory
and senior personnel of the Institute for Foundations of Machine Learning. He is currently associate editor of Electronic Journal of Statistics and area chair of ICML, ICLR, AISTATS, etc. His current
research focuses on the interplay of four principles of statistics and data science: (1) Heterogeneity of complex data, including mixture and hierarchical models and Bayesian nonparametrics; (2) Stability
and optimality of optimization and sampling algorithms for solving statistical machine learning models; (3) Scalability and efficiency of optimal transport for machine learning and deep learning applications;
(4) Interpretability, efficiency, and robustness of massive and complex machine learning models.</p>
</div>

  <footer class="article-footer">
    
    
    
    <section class="bordered">
      <header>
        <div class="panel-title">CATEGORIES</div>
      </header>
      <div>
        <ul class="p-terms">
          
          <li><a href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar/">McGill Statistics Seminar</a></li>
          
        </ul>
      </div>
    </section>
    
    
    
    <section class="bordered">
      <header>
        <div class="panel-title">TAGS</div>
      </header>
      <div>
        <ul class="p-terms">
          
          <li><a href="https://mcgillstat.github.io/tags/2024-fall/">2024 Fall</a></li>
          
        </ul>
      </div>
    </section>
    
    
  </footer>

</article>


    
  </div>

  <div class="col-md-4">
    
<aside class="l-sidebar">

  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">Recent Talks</div>
    </div>
    <div class="list-group">
      
      <a href="https://mcgillstat.github.io/tags/2024-fall/" class="list-group-item"> · Nov 22, 2024</a>
      
      <a href="https://mcgillstat.github.io/post/2024fall/2024-11-22/" class="list-group-item">Benjamin Bobbia · Nov 22, 2024</a>
      
      <a href="https://mcgillstat.github.io/categories/" class="list-group-item"> · Nov 22, 2024</a>
      
      <a href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar/" class="list-group-item"> · Nov 22, 2024</a>
      
      <a href="https://mcgillstat.github.io/tags/" class="list-group-item"> · Nov 22, 2024</a>
      
    </div>
  </section>

  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">CATEGORY</div>
    </div>
    <div class="list-group">
      
      <a href="https://mcgillstat.github.io/categories/mcgill-statistics-seminar" class="list-group-item">mcgill statistics seminar</a>
      
      <a href="https://mcgillstat.github.io/categories/crm-ssc-prize-address" class="list-group-item">crm-ssc prize address</a>
      
      <a href="https://mcgillstat.github.io/categories/crm-colloquium" class="list-group-item">crm-colloquium</a>
      
    </div>
  </section>
  
  <section class="panel panel-default">
    <div class="panel-heading">
      <div class="panel-title">TAG</div>
    </div>
    <div class="list-group">
      
      <a href="https://mcgillstat.github.io/tags/2024-winter" class="list-group-item">2024 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2024-fall" class="list-group-item">2024 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2023-winter" class="list-group-item">2023 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2023-summer" class="list-group-item">2023 summer</a>
      
      <a href="https://mcgillstat.github.io/tags/2023-fall" class="list-group-item">2023 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2022-winter" class="list-group-item">2022 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2022-fall" class="list-group-item">2022 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2021-winter" class="list-group-item">2021 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2021-fall" class="list-group-item">2021 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2020-winter" class="list-group-item">2020 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2020-fall" class="list-group-item">2020 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2019-winter" class="list-group-item">2019 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2019-fall" class="list-group-item">2019 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2018-winter" class="list-group-item">2018 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2018-fall" class="list-group-item">2018 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2017-winter" class="list-group-item">2017 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2017-fall" class="list-group-item">2017 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2016-winter" class="list-group-item">2016 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2016-fall" class="list-group-item">2016 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2015-winter" class="list-group-item">2015 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2015-fall" class="list-group-item">2015 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2014-winter" class="list-group-item">2014 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2014-fall" class="list-group-item">2014 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2013-winter" class="list-group-item">2013 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2013-fall" class="list-group-item">2013 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2012-winter" class="list-group-item">2012 winter</a>
      
      <a href="https://mcgillstat.github.io/tags/2012-fall" class="list-group-item">2012 fall</a>
      
      <a href="https://mcgillstat.github.io/tags/2011-fall" class="list-group-item">2011 fall</a>
      
    </div>
  </section>
  

</aside>



    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    
  </div>
</div>

      </div>
    </main>

    <footer class="l-footer">
      <div class="container">
        <p><span class="h-logo">&copy; McGill Statistics Seminars</span></p>
        <aside>
          <p><a href="http://www.mcgill.ca/mathstat/">Department of Mathematics and Statistics</a>.</p>
          <p><a href="https://www.mcgill.ca/">McGill University</a></p>
        </aside>
      </div>
    </footer>

    <script src="//code.jquery.com/jquery-3.1.1.min.js"></script>
    <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
  </body>
</html>

